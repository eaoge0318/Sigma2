# 02. åç«¯æ¨¡å—è¯¦ç»†å®ç°

> æœ¬æ–‡æ¡£åŒ…å«æ‰€æœ‰åç«¯æ¨¡å—çš„å®Œæ•´ä»£ç æ¡†æ¶ï¼ŒåŸºäºæ¨¡å—åŒ–æ¶æ„è®¾è®¡

## ğŸ—ï¸ v2.0 é‡æ§‹æ ¸å¿ƒæ–¹é‡

ç‚ºäº†æå‡åˆ†æé‚è¼¯æ·±åº¦èˆ‡ç³»çµ±ç©©å®šåº¦ï¼Œæœ¬ç‰ˆæœ¬éµå¾ªä»¥ä¸‹ä¸‰å¤§ä¿®æ”¹åŸå‰‡ï¼š

1.  **æ¨¡çµ„åŒ–æ¥µé™æ‹†åˆ†**ï¼š
    *   `agent.py` æŒ‡æ§é‚è¼¯èˆ‡å·¥ä½œæµå®šç¾©åˆ†é–‹ï¼Œå–®ä¸€æª”æ¡ˆä¸Šé™ **350 è¡Œ**ã€‚
    *   æ‰€æœ‰ 18 å€‹åˆ†æå·¥å…·ä¾ç…§é¡åˆ¥å­˜æ”¾åœ¨ `tools/` è³‡æ–™å¤¾ä¸­ã€‚
2.  **è‡ªæˆ‘åµéŒ¯èˆ‡åˆ†æå¾ªç’° (Self-Correction & Analysis Loop)**ï¼š
    *   **æ ¸å¿ƒç«™é»**ï¼šæ–°å¢ `ExpandConcept` å·¥ä½œç«™ã€‚
    *   **å¾ªç’°é‚è¼¯**ï¼šç•¶è³‡æ–™æœå°‹çµæœä¸æ»¿æ„æ™‚ï¼ŒAgent æœƒè‡ªå‹•è½‰å‘ LLM é€²è¡Œé ˜åŸŸè¡“èªè¯æƒ³ï¼Œä¸¦åŸ·è¡Œã€Œå†åˆ†æã€å¾ªç’°ï¼Œç›´åˆ°ç²å¾—æœ‰æ„ç¾©çš„æ•¸æ“šæˆ–é”åˆ°é‡è©¦ä¸Šé™ã€‚
3.  **18 å€‹å…¨è¦ç¯„åˆ†æå·¥å…·åº«**ï¼š
    *   å®Œæ•´å¯¦ä½œæŸ¥è©¢ (Query)ã€çµ±è¨ˆ (Stats)ã€æ¨¡å¼ (Patterns) èˆ‡è¼”åŠ© (Helpers) å››å¤§é¡å·¥å…·åŒ…ã€‚

---

## ç›®å½•ç»“æ„

```
backend/services/analysis/
â”œâ”€â”€ __init__.py                 (  20è¡Œ)
â”œâ”€â”€ analysis_service.py         ( 350è¡Œ)
â”œâ”€â”€ agent.py                    ( 350è¡Œ) [LlamaIndex åŸºæ–¼ Workflow æ¨¡å¼]
â””â”€â”€ tools/
    â”œâ”€â”€ __init__.py             (  30è¡Œ)
    â”œâ”€â”€ base.py                 (  50è¡Œ)
    â”œâ”€â”€ data_query.py           ( 200è¡Œ)
    â”œâ”€â”€ statistics.py           ( 250è¡Œ)
    â”œâ”€â”€ patterns.py             ( 200è¡Œ)
    â”œâ”€â”€ helpers.py              ( 100è¡Œ)
    â””â”€â”€ executor.py             ( 100è¡Œ)
```

---

## 1. åŒ…åˆå§‹åŒ– (`__init__.py`)

### `backend/services/analysis/__init__.py`

```python
"""
æ™ºèƒ½åˆ†ææœåŠ¡åŒ…
æä¾›åŸºäº LLM çš„æ•°æ®åˆ†æåŠŸèƒ½
"""

from .analysis_service import AnalysisService
from .agent import LLMAnalysisAgent
from .tools.executor import ToolExecutor

__all__ = [
    'AnalysisService',
    'LLMAnalysisAgent',
    'ToolExecutor',
]
```

---

## 2. æ ¸å¿ƒç´¢å¼•æœåŠ¡ (`analysis_service.py`, ~350è¡Œ)

```python
# backend/services/analysis/analysis_service.py

import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
import hashlib
import logging

logger = logging.getLogger(__name__)

class AnalysisService:
    """
    æ•°æ®åˆ†ææ ¸å¿ƒæœåŠ¡
    è´Ÿè´£ï¼šCSV ç´¢å¼•å»ºç«‹ã€æ•°æ®æ‘˜è¦ã€è¯­ä¹‰æœç´¢
    """
    
    def __init__(self, base_dir: str = "workspace"):
        self.base_dir = Path(base_dir)
        self.stop_events = {}  # session_id -> bool

    def stop_generation(self, session_id: str):
        """è¨­å®šåœæ­¢æ¨™èªŒ"""
        self.stop_events[session_id] = True
        logger.info(f"Stop signal set for session: {session_id}")

    def clear_stop_signal(self, session_id: str):
        """æ¸…é™¤åœæ­¢æ¨™èªŒ"""
        if session_id in self.stop_events:
            del self.stop_events[session_id]

    def is_generation_stopped(self, session_id: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ”¶åˆ°åœæ­¢ä¿¡è™Ÿ"""
        return self.stop_events.get(session_id, False)
    
    def get_file_id(self, filename: str) -> str:
        """ç”Ÿæˆæ–‡ä»¶ IDï¼ˆåŸºäºæ–‡ä»¶åçš„ hashï¼‰"""
        return hashlib.md5(filename.encode()).hexdigest()[:12]
    
    def get_analysis_path(self, session_id: str, file_id: str) -> Path:
        """è·å–åˆ†ææ–‡ä»¶å­˜å‚¨è·¯å¾„"""
        analysis_dir = self.base_dir / session_id / "analysis" / file_id
        analysis_dir.mkdir(parents=True, exist_ok=True)
        return analysis_dir
    
    async def build_analysis_index(
        self, 
        csv_path: str, 
        session_id: str, 
        filename: str
    ) -> Dict:
        """
        ä¸º CSV æ–‡ä»¶å»ºç«‹åˆ†æç´¢å¼•
        è¿™æ˜¯ä¸€æ¬¡æ€§æ“ä½œï¼Œç»“æœä¼šç¼“å­˜
        
        ç”Ÿæˆæ–‡ä»¶ï¼š
        - summary.json: åŸºæœ¬æ‘˜è¦
        - statistics.json: ç»Ÿè®¡ä¿¡æ¯
        - correlations.json: ç›¸å…³æ€§çŸ©é˜µ
        - semantic_index.json: è¯­ä¹‰ç´¢å¼•
        """
        file_id = self.get_file_id(filename)
        analysis_path = self.get_analysis_path(session_id, file_id)
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç´¢å¼•
        summary_file = analysis_path / "summary.json"
        if summary_file.exists():
            logger.info(f"Index already exists for {filename}")
            with open(summary_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        logger.info(f"Building index for {filename}")
        
        # è¯»å– CSV
        df = pd.read_csv(csv_path)
        
        # 1. ç”ŸæˆåŸºæœ¬æ‘˜è¦
        summary = {
            "file_id": file_id,
            "filename": filename,
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "parameters": list(df.columns),
            "created_at": pd.Timestamp.now().isoformat()
        }
        
        # 2. å‚æ•°åˆ†ç±»
        categories = self._categorize_parameters(df.columns)
        summary["categories"] = categories
        
        # 3. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = self._calculate_statistics(df)
        with open(analysis_path / "statistics.json", 'w', encoding='utf-8') as f:
            json.dump(statistics, f, ensure_ascii=False, indent=2)
        
        # 4. è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlations = self._calculate_correlations(df)
        with open(analysis_path / "correlations.json", 'w', encoding='utf-8') as f:
            json.dump(correlations, f, ensure_ascii=False, indent=2)
        
        # 5. æ„å»ºè¯­ä¹‰ç´¢å¼•
        semantic_index = self._build_semantic_index(df.columns)
        with open(analysis_path / "semantic_index.json", 'w', encoding='utf-8') as f:
            json.dump(semantic_index, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Index built successfully for {filename}")
        return summary
    
    def _categorize_parameters(self, columns: List[str]) -> Dict[str, List[str]]:
        """æ ¹æ®å‚æ•°åå‰ç¼€è¿›è¡Œåˆ†ç±»"""
        categories = {}
        for col in columns:
            # æå–å‰ç¼€ï¼ˆå¦‚ TENSION-A101 -> TENSIONï¼‰
            parts = col.split('-')
            if len(parts) > 1:
                category = parts[0]
            else:
                parts = col.split('_')
                category = parts[0] if len(parts) > 1 else "OTHER"
            
            if category not in categories:
                categories[category] = []
            categories[category].append(col)
        
        return categories
    
    def _calculate_statistics(self, df: pd.DataFrame) -> Dict:
        """è®¡ç®—æ‰€æœ‰æ•°å€¼å‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯"""
        statistics = {}
        
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                try:
                    statistics[col] = {
                        "count": int(df[col].count()),
                        "mean": float(df[col].mean()),
                        "std": float(df[col].std()),
                        "min": float(df[col].min()),
                        "max": float(df[col].max()),
                        "median": float(df[col].median()),
                        "q1": float(df[col].quantile(0.25)),
                        "q3": float(df[col].quantile(0.75)),
                        "missing_count": int(df[col].isna().sum())
                    }
                except Exception as e:
                    logger.warning(f"Failed to calculate stats for {col}: {e}")
        
        return statistics
    
    def _calculate_correlations(self, df: pd.DataFrame) -> Dict:
        """è®¡ç®—æ•°å€¼å‚æ•°é—´çš„ç›¸å…³æ€§çŸ©é˜µ"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) < 2:
            return {}
        
        corr_matrix = df[numeric_cols].corr()
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        correlations = {}
        for col1 in numeric_cols:
            correlations[col1] = {}
            for col2 in numeric_cols:
                correlations[col1][col2] = float(corr_matrix.loc[col1, col2])
        
        return correlations
    
    def _build_semantic_index(self, columns: List[str]) -> Dict[str, List[str]]:
        """
        æ„å»ºè¯­ä¹‰ç´¢å¼•ï¼šæ¦‚å¿µ -> å‚æ•°åˆ—è¡¨
        æ”¯æŒä¸­è‹±æ–‡å…³é”®è¯æœç´¢
        """
        # å…³é”®è¯æ˜ å°„è¡¨
        keyword_map = {
            "æ¸©åº¦": ["TEMP", "HEAT", "BCDRY", "ACDRY", "æ¸©", "çƒ­"],
            "å¼ åŠ›": ["TENSION", "PULL", "STRESS", "å¼ ", "æ‹‰"],
            "æ¹¿åº¦": ["MOISTURE", "HUMIDITY", "WET", "æ¹¿", "æ°´"],
            "é€Ÿåº¦": ["SPEED", "VELOCITY", "RPM", "é€Ÿ"],
            "å‹åŠ›": ["PRESSURE", "PRESS", "å‹"],
            "å“è´¨": ["QUALITY", "GRADE", "METROLOGY", "å“", "è´¨"],
            "æ–­çº¸": ["BREAK", "BREAKAGE", "æ–­", "è£‚"],
            "æµé‡": ["FLOW", "RATE", "æµ"],
            "æµ“åº¦": ["CONCENTRATION", "CONSISTENCY", "æµ“"],
        }
        
        semantic_index = {}
        for concept, keywords in keyword_map.items():
            matched = []
            for col in columns:
                col_upper = col.upper()
                for kw in keywords:
                    if kw.upper() in col_upper:
                        matched.append(col)
                        break
            if matched:
                semantic_index[concept] = matched
        
        return semantic_index
    
    def load_summary(self, session_id: str, file_id: str) -> Dict:
        """åŠ è½½æ–‡ä»¶æ‘˜è¦"""
        analysis_path = self.get_analysis_path(session_id, file_id)
        summary_file = analysis_path / "summary.json"
        
        if not summary_file.exists():
            raise FileNotFoundError(f"Summary not found for file_id: {file_id}")
        
        with open(summary_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_mapping_table(
        self, session_id: str, file_id: Optional[str] = None
    ) -> Dict[str, str]:
        """
        åŠ è½½æœ¯è¯­å¯¹åº”è¡¨
        ä¼˜å…ˆçº§ï¼š
        1. ç»‘å®šæ–‡ä»¶ (analysis/{file_id}/mapping.csv)
        2. å…¨å±€æ–‡ä»¶ (uploads/(å‚æ•°å¯¹åº”è¡¨)_*.csv)
        """
        # ... (Implementation details: check bound first, then global)
        pass
    
    def load_correlations(self, session_id: str, file_id: str) -> Dict:
        """åŠ è½½ç›¸å…³æ€§çŸ©é˜µ"""
        analysis_path = self.get_analysis_path(session_id, file_id)
        corr_file = analysis_path / "correlations.json"
        
        if not corr_file.exists():
            raise FileNotFoundError(f"Correlations not found for file_id: {file_id}")
        
        with open(corr_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def load_semantic_index(self, session_id: str, file_id: str) -> Dict:
        """åŠ è½½è¯­ä¹‰ç´¢å¼•"""
        analysis_path = self.get_analysis_path(session_id, file_id)
        index_file = analysis_path / "semantic_index.json"
        
        if not index_file.exists():
            raise FileNotFoundError(f"Semantic index not found for file_id: {file_id}")
        
        with open(index_file, 'r', encoding='utf-8') as f:
            return json.load(f)
```

---

## 3. å·¥å…·åŸºç±» (`tools/base.py`, ~50è¡Œ)

```python
# backend/services/analysis/tools/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, List

class AnalysisTool(ABC):
    """åˆ†æå·¥å…·æŠ½è±¡åŸºç±»"""
    
    def __init__(self, analysis_service):
        self.analysis_service = analysis_service
    
    @property
    @abstractmethod
    def name(self) -> str:
        """å·¥å…·åç§°"""
        pass
    
    @property
    @abstractmethod
    def description(self) -> str:
        """å·¥å…·æè¿°"""
        pass
    
    @property
    def required_params(self) -> List[str]:
        """å¿…éœ€å‚æ•°åˆ—è¡¨ï¼ˆå­ç±»å¯è¦†ç›–ï¼‰"""
        return ['file_id']
    
    @abstractmethod
    def execute(self, params: Dict, session_id: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥å…·
        
        Args:
            params: å·¥å…·å‚æ•°
            session_id: ç”¨æˆ·ä¼šè¯ ID
        
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        pass
    
    def validate_params(self, params: Dict) -> bool:
        """éªŒè¯å‚æ•°å®Œæ•´æ€§"""
        return all(p in params for p in self.required_params)
```

---

## 4. å·¥å…·åŒ…åˆå§‹åŒ– (`tools/__init__.py`, ~30è¡Œ)

```python
# backend/services/analysis/tools/__init__.py

from .base import AnalysisTool
from .data_query import (
    GetParameterListTool,
    GetParameterStatisticsTool,
    SearchParametersByConceptTool,
)
from .statistics import (
    CalculateCorrelationTool,
    GetTopCorrelationsTool,
    CompareGroupsTool,
)
from .executor import ToolExecutor

__all__ = [
    'AnalysisTool',
    'ToolExecutor',
    # æŸ¥è¯¢å·¥å…·
    'GetParameterListTool',
    'GetParameterStatisticsTool',
    'SearchParametersByConceptTool',
    # ç»Ÿè®¡å·¥å…·
    'CalculateCorrelationTool',
    'GetTopCorrelationsTool',
    'CompareGroupsTool',
]
```

---

## 5. æŸ¥è¯¢å·¥å…· (`tools/data_query.py`, ~200è¡Œ)

```python
# backend/services/analysis/tools/data_query.py

from .base import AnalysisTool
from typing import Dict, Any

class GetParameterListTool(AnalysisTool):
    """è·å–æ•°æ®é›†çš„æ‰€æœ‰å­—æ®µåˆ—è¡¨"""
    name = "get_parameter_list"
    description = "è·å–CSVæ–‡ä»¶çš„æ‰€æœ‰å­—æ®µåç§°ï¼Œæ”¯æŒå…³é”®å­—è¿‡æ»¤"
    required_params = ["file_id"]
    
    def execute(self, params: Dict, session_id: str) -> Dict[str, Any]:
        file_id = params.get('file_id')
        keyword = params.get('keyword', '').lower()
        
        summary = self.analysis_service.load_summary(session_id, file_id)
        all_params = summary['parameters']
        
        # å…³é”®å­—è¿‡æ»¤
        if keyword:
            matched = [p for p in all_params if keyword in p.lower()]
        else:
            matched = all_params
        
        return {
            "parameters": matched,
            "total_count": len(all_params),
            "matched_count": len(matched),
            "categories": summary.get('categories', {})
        }


class GetParameterStatisticsTool(AnalysisTool):
    """è·å–å­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯"""
    name = "get_parameter_statistics"
    description = "è¿”å›å­—æ®µçš„å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ç­‰"
    required_params = ["file_id", "parameter"]
    
    def execute(self, params: Dict, session_id: str) -> Dict[str, Any]:
        file_id = params.get('file_id')
        parameter = params.get('parameter')
        
        statistics = self.analysis_service.load_statistics(session_id, file_id)
        
        if parameter not in statistics:
            return {"error": f"Parameter {parameter} not found or not numeric"}
        
        result = statistics[parameter].copy()
        result["parameter"] = parameter
        
        return result


class SearchParametersByConceptTool(AnalysisTool):
    """æ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³å­—æ®µ"""
    name = "search_parameters_by_concept"
    description = "ä¾‹å¦‚è¾“å…¥'ä»·æ ¼'ï¼Œèƒ½æ‰¾åˆ°'å•ä»·'ã€'æ€»ä»·'ã€'å”®ä»·'ç­‰ç›¸å…³å­—æ®µ"
    required_params = ["file_id", "concept"]
    
    def execute(self, params: Dict, session_id: str) -> Dict[str, Any]:
        file_id = params.get('file_id')
        concept = params.get('concept', '')
        
        semantic_index = self.analysis_service.load_semantic_index(session_id, file_id)
        summary = self.analysis_service.load_summary(session_id, file_id)
        
        matched_parameters = []
        
        # è¯­ä¹‰ç´¢å¼•åŒ¹é…
        if concept in semantic_index:
            for param in semantic_index[concept]:
                matched_parameters.append({
                    "name": param,
                    "confidence": 0.9,
                    "reason": f"è¯­ä¹‰æ˜ å°„: {concept}"
                })
        
        # æ¨¡ç³ŠåŒ¹é…
        for param in summary['parameters']:
            if concept.lower() in param.lower():
                if not any(m['name'] == param for m in matched_parameters):
                    matched_parameters.append({
                        "name": param,
                        "confidence": 0.7,
                        "reason": "å…³é”®å­—åŒ¹é…"
                    })
        
        return {
            "matched_parameters": matched_parameters,
            "total_matches": len(matched_parameters)
        }


# å…¶ä»–æŸ¥è¯¢å·¥å…·å¯åœ¨æ­¤ç»§ç»­æ·»åŠ ...
# class GetDataOverviewTool(AnalysisTool): ...
# class GetTimeSeriesDataTool(AnalysisTool): ...
```

---

## 6. ç»Ÿè®¡å·¥å…· (`tools/statistics.py`, ~250è¡Œ)

```python
# backend/services/analysis/tools/statistics.py

from .base import AnalysisTool
from typing import Dict, Any
import pandas as pd
from scipy.stats import pearsonr, spearmanr, ttest_ind
from pathlib import Path

class CalculateCorrelationTool(AnalysisTool):
    """è®¡ç®—ç›¸å…³æ€§å·¥å…·"""
    
    name = "calculate_correlation"
    description = "è®¡ç®—å‚æ•°é—´çš„ç›¸å…³ç³»æ•°"
    required_params = ['file_id', 'parameters']
    
    def execute(self, params: Dict, session_id: str) -> Dict[str, Any]:
        file_id = params.get('file_id')
        parameters = params.get('parameters', [])
        method = params.get('method', 'pearson')
        target = params.get('target')
        
        # åŠ è½½ CSV æ•°æ®
        csv_path = self._get_csv_path(session_id, file_id)
        df = pd.read_csv(csv_path)
        
        results = []
        
        if target:
            # è®¡ç®—æ‰€æœ‰å‚æ•°ä¸ target çš„ç›¸å…³æ€§
            for param in parameters:
                if param in df.columns and target in df.columns:
                    try:
                        corr, p_val = self._calc_corr(df[param], df[target], method)
                        results.append({
                            "param1": param,
                            "param2": target,
                            "correlation": float(corr),
                            "p_value": float(p_val),
                            "interpretation": self._interpret_corr(corr, p_val)
                        })
                    except Exception as e:
                        continue
        else:
            # ä¸¤ä¸¤è®¡ç®—
            for i in range(len(parameters)):
                for j in range(i + 1, len(parameters)):
                    p1, p2 = parameters[i], parameters[j]
                    if p1 in df.columns and p2 in df.columns:
                        try:
                            corr, p_val = self._calc_corr(df[p1], df[p2], method)
                            results.append({
                                "param1": p1,
                                "param2": p2,
                                "correlation": float(corr),
                                "p_value": float(p_val),
                                "interpretation": self._interpret_corr(corr, p_val)
                            })
                        except:
                            continue
        
        return {
            "method": method,
            "results": results
        }
    
    def _calc_corr(self, x, y, method):
        if method == 'pearson':
            return pearsonr(x, y)
        elif method == 'spearman':
            return spearmanr(x, y)
        else:
            return pearsonr(x, y)
    
    def _interpret_corr(self, corr: float, p_value: float) -> str:
        if p_value >= 0.05:
            return "æ— ç»Ÿè®¡æ˜¾è‘—æ€§"
        
        abs_corr = abs(corr)
        if abs_corr >= 0.7:
            strength = "å¼º"
        elif abs_corr >= 0.4:
            strength = "ä¸­ç­‰"
        else:
            strength = "å¼±"
        
        direction = "æ­£" if corr > 0 else "è´Ÿ"
        return f"{strength}{direction}ç›¸å…³ï¼Œç»Ÿè®¡æ˜¾è‘—"
    
    def _get_csv_path(self, session_id: str, file_id: str) -> str:
        """è·å– CSV æ–‡ä»¶è·¯å¾„"""
        analysis_path = self.analysis_service.get_analysis_path(session_id, file_id)
        summary = self.analysis_service.load_summary(session_id, file_id)
        filename = summary['filename']
        return str(Path(self.analysis_service.base_dir) / session_id / "uploads" / filename)


class GetTopCorrelationsTool(AnalysisTool):
    """è·å– Top ç›¸å…³æ€§å·¥å…·"""
    
    name = "get_top_correlations"
    description = "å¿«é€Ÿè·å–ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§æœ€å¼ºçš„å‚æ•°"
    required_params = ['file_id', 'target']
    
    def execute(self, params: Dict, session_id: str) -> Dict[str, Any]:
        file_id = params.get('file_id')
        target = params.get('target')
        top_n = params.get('top_n', 10)
        min_corr = params.get('min_correlation', 0.3)
        
        # è¯»å–ç›¸å…³æ€§çŸ©é˜µ
        correlations = self.analysis_service.load_correlations(session_id, file_id)
        
        if target not in correlations:
            return {"error": f"Target {target} not found"}
        
        # æå–ä¸ target çš„ç›¸å…³æ€§
        results = []
        for param, corr_value in correlations[target].items():
            if param != target and abs(corr_value) >= min_corr:
                results.append({
                    "parameter": param,
                    "correlation": corr_value,
                    "p_value": 0.001  # ç®€åŒ–ï¼Œå®é™…åº”é‡æ–°è®¡ç®—
                })
        
        # æ’åº
        results.sort(key=lambda x: abs(x["correlation"]), reverse=True)
        
        return {
            "target": target,
            "top_correlations": results[:top_n]
        }


class CompareGroupsTool(AnalysisTool):
    """ç»„é—´æ¯”è¾ƒå·¥å…·"""
    
    name = "compare_groups"
    description = "æ¯”è¾ƒä¸åŒæ¡ä»¶ä¸‹å‚æ•°çš„å·®å¼‚ï¼ˆt-testï¼‰"
    required_params = ['file_id', 'parameter', 'group_by']
    
    def execute(self, params: Dict, session_id: str) -> Dict[str, Any]:
        file_id = params.get('file_id')
        parameter = params.get('parameter')
        group_by = params.get('group_by')
        
        # åŠ è½½æ•°æ®
        csv_path = self._get_csv_path(session_id, file_id)
        df = pd.read_csv(csv_path)
        
        if parameter not in df.columns or group_by not in df.columns:
            return {"error": "Parameter or group_by not found"}
        
        # åˆ†ç»„ç»Ÿè®¡
        groups = df.groupby(group_by)[parameter]
        group_stats = {}
        group_data = {}
        
        for name, group in groups:
            group_stats[f"group_{name}"] = {
                "mean": float(group.mean()),
                "std": float(group.std()),
                "count": len(group)
            }
            group_data[name] = group.values
        
        # t-test
        if len(group_data) == 2:
            g1, g2 = list(group_data.values())
            stat, p_val = ttest_ind(g1, g2)
            test_result = {
                "statistic": float(stat),
                "p_value": float(p_val),
                "interpretation": "ä¸¤ç»„å‡å€¼å­˜åœ¨æ˜¾è‘—å·®å¼‚" if p_val < 0.05 else "ä¸¤ç»„å‡å€¼æ— æ˜¾è‘—å·®å¼‚"
            }
        else:
            test_result = {"error": "Only supports 2 groups for t-test"}
        
        return {
            "parameter": parameter,
            "groups": group_stats,
            "test_result": test_result
        }
    
    def _get_csv_path(self, session_id: str, file_id: str) -> str:
        analysis_path = self.analysis_service.get_analysis_path(session_id, file_id)
        summary = self.analysis_service.load_summary(session_id, file_id)
        filename = summary['filename']
        return str(Path(self.analysis_service.base_dir) / session_id / "uploads" / filename)


# å…¶ä»–ç»Ÿè®¡å·¥å…·...
# class DetectOutliersTool(AnalysisTool): ...
# class AnalyzeDistributionTool(AnalysisTool): ...
# class PerformRegressionTool(AnalysisTool): ...
```

---

## 7. æ™ºèƒ½åˆ†æ Agent (`agent.py`, ~300è¡Œ)

```python
# backend/services/analysis/agent.py

from typing import Dict, Any, List, Optional
from llama_index.core.agent import ReActAgent
from llama_index.llms.ollama import Ollama
from llama_index.core.tools import FunctionTool
from .tools.executor import ToolExecutor

### 2. SigmaAnalysisWorkflow (æ™ºæ…§åˆ†æå·¥ä½œæµ)

**æ–‡ä»¶**: `backend/services/analysis/agent.py`

é€™æ˜¯ç³»çµ±çš„æ ¸å¿ƒå¤§è…¦ï¼ŒåŸºæ–¼ LlamaIndex `Workflow` å¯¦ä½œã€‚å®ƒç®¡è½„äº†å¾å•é¡Œé€²å…¥åˆ°æœ€çµ‚ç­”æ¡ˆè¼¸å‡ºçš„æ‰€æœ‰é‚è¼¯ç¯€é»ã€‚

#### 2.1 æ ¸å¿ƒå®šç¾©
```python
class SigmaAnalysisWorkflow(Workflow):
    """
    Sigma2 æ™ºæ…§åˆ†æå·¥ä½œæµ (äº‹ä»¶é©…å‹•ç‹€æ…‹æ©Ÿ)
    """
    def __init__(
        self, 
        tool_executor: ToolExecutor,
        analysis_service: AnalysisService,
        timeout: int = 200,
        verbose: bool = True
    ):
        super().__init__(timeout=timeout, verbose=verbose)
        self.tool_executor = tool_executor
        self.analysis_service = analysis_service
        self.llm = Ollama(model="llama3", request_timeout=120.0)
```

#### 2.2 é—œéµå·¥ä½œç«™ (Steps)
- **ExecuteAnalysis** (Local): åŸ·è¡Œ Pandas é‹ç®—ã€‚
    - **æ™ºæ…§åˆ†æµ (Smart Skip)**: ç³»çµ±æœƒæª¢æŸ¥çµæœæ˜¯å¦åŒ…å«æ•¸å€¼æ•¸æ“šã€‚
        - è‹¥åŒ…å«æ•¸æ“šï¼šæ‹‹å‡º `VisualizingEvent` é€²å…¥ç¹ªåœ–ç«™ã€‚
        - è‹¥åƒ…ç‚ºæ¸…å–®æˆ–æ–‡æœ¬ï¼šè·³éç¹ªåœ–ç«™ï¼Œç›´æ¥æ‹‹å‡º `SummarizeEvent` é€²å…¥ç¸½çµç«™ã€‚
- **Visualizer** (Local): **[å¯¦ä½œè¦é»]** æ™ºæ…§åœ–è¡¨å·¥å» ã€‚
    - **ç›´æ–¹åœ– (Histogram)**: è‡ªå‹•è¨ˆç®— Min/Max ä¸¦é€²è¡Œ 15-bins åˆ†ç®±çµ±è¨ˆã€‚
    - **æ•£ä½ˆåœ– (Scatter)**: è‡ªå‹•é…å°æ•¸å€¼æ¬„ä½ç‚º X-Y åº§æ¨™ã€‚
    - **é›™è»¸ (Dual-Axis)**: åµæ¸¬æ•¸æ“šåˆ»åº¦è½å·®ï¼ˆ>10å€ï¼‰è‡ªå‹•é–‹å•Ÿå³å´ y1 è»¸ã€‚
- **ExpandConcept** (LLM): **[æ ¸å¿ƒå¾ªç’°]** ç•¶åœ°ç«¯æœå°‹ç„¡çµæœæ™‚ï¼Œå•Ÿå‹•èªç¾©è¯æƒ³ã€‚
- **Humanizer** (LLM): ç”Ÿæˆç¹é«”ä¸­æ–‡å ±å‘Šã€‚æ•´åˆ Visualizer ç”¢ç”Ÿçš„åœ–è¡¨èˆ‡æ•¸æ“šè§£è®€ã€‚

#### 2.3 ExpandConcept å¯¦ä½œé‚è¼¯ (è‡ªæˆ‘åµéŒ¯å¾ªç’°)

æ­¤å·¥ä½œç«™å°ˆè²¬è™•ç†ã€Œæœå°‹å¤±æ•—ã€çš„å¾©åŸå·¥ä½œï¼š

```python
@step
async def expand_concept(self, ev: ConceptExpansionEvent) -> AnalysisEvent:
    # 1. å–å¾—åŸå§‹ query èˆ‡æœå°‹å¤±æ•—çš„è¡“èª
    query = ev.query
    
    # 2. å‘¼å« LLM é€²è¡Œèªç¾©æ“´å±• (Semantic Expansion)
    # Prompt: "ç”¨æˆ¶æœå°‹äº† '{query}' ä½†æ²’çµæœã€‚è«‹æ ¹æ“šé€ ç´™å·¥æ¥­çŸ¥è­˜ï¼Œ
    # è¯æƒ³ 3-5 å€‹ç›¸é—œçš„è‹±æ–‡æ¬„ä½é—œéµå­—ã€‚"
    suggestions = await self.llm.acomplete(...)
    
    # 3. è¨˜éŒ„å˜—è©¦æ¬¡æ•¸ (Context ç®¡ç†)
    # 4. æ‹‹å‡º AnalysisEvent å›æµé‡è©¦
    return AnalysisEvent(query=suggestions[0], ...) 
```

---

#### 2.3 äº‹ä»¶å‚³é€å¸¶ (Events)
ç³»çµ±å®šç¾©äº†æ˜ç¢ºçš„äº‹ä»¶å…§å®¹ï¼š
- `IntentEvent`: æ”œå¸¶ query, intent, history ç­‰ã€‚
- `AnalysisEvent`: è§¸ç™¼åœ°ç«¯åˆ†æã€‚
- `ConceptExpansionEvent`: è§¸ç™¼èªç¾©è¯æƒ³å¾ªç’°ã€‚
- `VisualizingEvent`: **[New]** æ”œå¸¶åœ°ç«¯åŸå§‹æ•¸æ“šï¼Œäº¤çµ¦ç¹ªåœ–ç«™è™•ç†ã€‚
- `SummarizeEvent`: æ”œå¸¶åœ°ç«¯æ•¸æ“šèˆ‡ï¼ˆé¸é…çš„ï¼‰Chart JSON å›å‚³çµ¦ç¸½çµç«™ã€‚
- `StopEvent`: çµæŸä¿¡è™Ÿã€‚

        # æ³¨å…¥ file_id åˆ° context ä¸­ï¼Œæˆ–ç›´æ¥å‚³é
        # LlamaIndex æœƒè‡ªå‹•ç®¡ç†å°è©±ç‹€æ…‹
        response = await self.agent.achat(user_question)
        
        # æå–å·¥å…·å‘¼å«ç´€éŒ„ï¼ˆç”¨æ–¼å‰ç«¯å±•ç¤ºï¼‰
        tool_log = []
        if response.sources:
            for source in response.sources:
                tool_log.append({
                    "tool": source.tool_name,
                    "params": source.raw_input,
                    "result": source.content
                })

        return {
            "response": response.response,
            "tool_used": tool_log[0]["tool"] if tool_log else None,
            "tool_result": tool_log[0]["result"] if tool_log else None,
            "all_tool_calls": tool_log
        }

    def _wrap_tools(self) -> List[FunctionTool]:
        """å°è£ç¾æœ‰å·¥å…·"""
        li_tools = []
        for name, tool in self.tool_executor.tools.items():
            # ä½¿ç”¨ LlamaIndex çš„åŒ…è£å™¨å°‡è‡ªå®šç¾©å·¥å…·è½‰ç‚º Agent å¯è­˜åˆ¥æ ¼å¼
            wrapped = FunctionTool.from_defaults(
                fn=tool.execute,
                name=tool.name,
                description=tool.description
            )
            li_tools.append(wrapped)
        return li_tools
```

---

**èªªæ˜**: ä½¿ç”¨ LlamaIndex æ¡†æ¶å¾Œï¼Œå‚³çµ±çš„ã€Œè¦å‰‡åŒ¹é…ã€æ„åœ–è­˜åˆ¥å·²è¢«å…¶å…§å»ºçš„æ¨ç†å¼•æ“å–ä»£ï¼Œé€™æä¾›äº†æ›´å¼·çš„å¥å£¯æ€§èˆ‡å¤šè¼ªå°è©±èƒ½åŠ›ã€‚


---

## ä¸‹ä¸€æ­¥

æŸ¥çœ‹ **[03_api_design.md](./03_api_design.md)** äº†è§£ API æ¥å£è®¾è®¡ã€‚
