import json
import logging
import httpx
import requests
import re
from typing import Any, Optional, Union
from llama_index.core.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
    ChatMessage,
)
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.llms.callbacks import llm_completion_callback
from llama_index.core.workflow import (
    StartEvent,
    StopEvent,
    Workflow,
    step,
    Context,
)
import config
from .analysis_types import (
    IntentEvent,
    AnalysisEvent,
    TranslationEvent,
    VisualizingEvent,
    SummarizeEvent,
    ProgressEvent,
    TextChunkEvent,
    ErrorEvent,
)
from .tools.executor import ToolExecutor
from .analysis_service import AnalysisService

logger = logging.getLogger(__name__)


class CustomOllamaLLM(CustomLLM):
    """
    è‡ªå®šç¾©é«˜æ•ˆ Ollama å°è£ï¼Œæ”¯æŒ httpx ç•°æ­¥è«‹æ±‚
    """

    model_name: str
    api_url: str
    timeout: float = 120.0

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=32768,
            num_output=4096,
            model_name=self.model_name,
        )

    @llm_completion_callback()
    async def acomplete(
        self, prompt: str, json_mode: bool = False, **kwargs: Any
    ) -> CompletionResponse:
        """æ ¸å¿ƒéä¸²æµå›å‚³ï¼Œæ”¯æŒ JSON æ¨¡å¼"""
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        if json_mode:
            payload["format"] = "json"

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(self.api_url, json=payload)
                response.raise_for_status()
                result = response.json()
                content = result.get("message", {}).get("content", "")
                return CompletionResponse(text=content)
        except Exception as e:
            logger.error(f"Ollama Async é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(f"ç„¡æ³•éåŒæ­¥é€£ç·šè‡³ Ollama: {str(e)}")

    async def astream_complete(
        self, prompt: str, **kwargs: Any
    ) -> CompletionResponseGen:
        """æ ¸å¿ƒä¸²æµå›å‚³ï¼Œç”¨æ–¼å³æ™‚æ‰“å­—æ©Ÿæ•ˆæœ"""
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": True,
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST", self.api_url, json=payload
                ) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if not line:
                            continue
                        chunk = json.loads(line)
                        if "message" in chunk:
                            content = chunk["message"].get("content", "")
                            yield CompletionResponse(text=content, delta=content)
                        if chunk.get("done"):
                            break
        except Exception as e:
            logger.error(f"Ollama Stream é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(f"ç„¡æ³•ä¸²æµé€£ç·šè‡³ Ollama: {str(e)}")

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        response = requests.post(self.api_url, json=payload, timeout=self.timeout)
        result = response.json()
        return CompletionResponse(text=result.get("message", {}).get("content", ""))

    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        yield self.complete(prompt, **kwargs)


class SigmaAnalysisWorkflow(Workflow):
    """
    Sigma2 æ™ºèƒ½åˆ†æå·¥ä½œæµ (é«˜æ€§èƒ½ä¿®æ­£ç‰ˆ)
    """

    def __init__(
        self,
        tool_executor: ToolExecutor,
        analysis_service: AnalysisService,
        model_name: str = config.LLM_MODEL,
        ollama_api_url: str = config.LLM_API_URL,
        timeout: int = 180,
    ):
        super().__init__(timeout=timeout, verbose=True)
        self.tool_executor = tool_executor
        self.analysis_service = analysis_service
        # ä½¿ç”¨è‡ªå®šç¾©æ¥µé€Ÿå¼•æ“ï¼Œä¸¦å…±äº«å¯¦ä¾‹ä»¥é™ä½é–‹éŠ·
        self.llm = CustomOllamaLLM(model_name=model_name, api_url=ollama_api_url)
        self.llm_json = self.llm  # æ•…æ„ä¸é–‹ JSON æ¨¡å¼ä»¥æå‡ä¸²æµéˆæ´»æ€§

    @step
    async def route_intent(
        self, ctx: Context, ev: StartEvent
    ) -> IntentEvent | ErrorEvent:
        # å‘å‰ç«¯ç™¼é€åˆæ­¥åé¥‹
        ctx.write_event_to_stream(ProgressEvent(msg="â”€ æ­£åœ¨å¿«é€ŸåŒ¹é…æŒ‡ä»¤è·¯å¾‘..."))
        query = getattr(ev, "query", "").strip()
        file_id = getattr(ev, "file_id", None)
        session_id = getattr(ev, "session_id", None)
        history = getattr(ev, "history", "")

        if not query:
            return ErrorEvent(error="æœªæä¾›å•é¡Œ", session_id=session_id)

        # --- æ¥µé€Ÿç¡¬é«”æ±ºç­– (Heuristic Logic) ---
        # 1. å¦‚æœæœ‰ File_ID ä¸”å•é¡Œæ˜ç¢ºåŒ…å«"åˆ†æ"ã€"è¨ºæ–·"ç­‰å¼·çƒˆæ„åœ–ï¼Œæ‰æ­¸é¡ç‚º analysis
        strong_analysis_keywords = [
            "åˆ†æ",
            "è¨ºæ–·",
            "ç•°å¸¸",
            "åŸå› ",
            "ç‚ºä»€éº¼",
            "å½±éŸ¿",
            "é—œè¯",
            "è¶¨å‹¢",
            "é æ¸¬",
            "åœ–",
            "ç•«",
            "åˆ†ä½ˆ",
            "hotelling",
            "pca",
        ]

        query_lower = query.lower()
        if file_id and any(kw in query_lower for kw in strong_analysis_keywords):
            intent = "analysis"
        # 2. å¦‚æœæ˜¯å•åƒæ•¸åç¨±ã€æ¬„ä½ç­‰ï¼Œä¹Ÿæ­¸é¡ç‚º analysis ä½†å¾ŒçºŒæœƒèµ°å¿«è»Šé“ (Metadata Fast-Track)
        elif file_id and any(
            kw in query_lower
            for kw in ["æ¬„ä½", "åƒæ•¸", "column", "parameter", "å¹¾ç­†", "è¡Œæ•¸", "æ‘˜è¦"]
        ):
            intent = "analysis"
        else:
            # 3. å…¶ä»–æƒ…æ³ (ä¾‹å¦‚èŠå¤©ã€é–’èŠã€æˆ–ä¸æ˜ç¢ºæŒ‡ä»¤)ï¼Œå‹•ç”¨ LLM åˆ¤æ–·
            # é€™è£¡æˆ‘å€‘ç¨å¾®ä¿å®ˆä¸€é»ï¼Œå¦‚æœ LLM åˆ¤æ–·æ˜¯ chat å°±èµ° chat
            try:
                # ç°¡å–®çš„åˆ†é¡ Prompt
                prompt = (
                    f"Classify user query into 'analysis' (needs data tools) or 'chat' (general QA/coding).\n"
                    f"Query: {query}\n"
                    f"Answer (analysis/chat):"
                )
                response = await self.llm.acomplete(prompt)
                intent = str(response.text).strip().lower()
                # é˜²å‘†
                if "analysis" in intent:
                    intent = "analysis"
                else:
                    intent = "chat"
            except Exception:
                # é»˜èª fallback
                intent = "chat"

        return IntentEvent(
            query=query,
            intent=intent,
            file_id=file_id,
            session_id=session_id,
            history=history,
            mode=ev.mode,
            suspect_pool=getattr(ev, "suspect_pool", []),
        )

    @step
    async def handle_error(self, ctx: Context, ev: ErrorEvent) -> StopEvent:
        """
        [Local Step] éŒ¯èª¤è™•ç†ç«™
        """
        logger.error(f"[Error Station] Workflow Error: {ev.error}")
        return StopEvent(
            result={
                "response": "æŠ±æ­‰ï¼Œç³»çµ±é‹ä½œå‡ºç¾éŒ¯èª¤ï¼š" + str(ev.error),
                "data": None,
            }
        )

    @step
    async def dispatch_work(
        self, ctx: Context, ev: IntentEvent
    ) -> Union[AnalysisEvent, TranslationEvent, SummarizeEvent, VisualizingEvent]:
        intent = (ev.intent or "").strip().lower()
        query_lower = ev.query.lower()

        # --- è¦–è¦ºåŒ–å¿«è»Šé“ (Visualization Fast-Track) ---
        # å¦‚æœç”¨æˆ¶åªæ˜¯æƒ³ç•«åœ–ï¼Œç›´æ¥èª¿ç”¨ get_time_series_data å¾Œå‡ºåœ–ï¼Œå®Œå…¨è·³é LLM åˆ†æå¾ªç’°
        viz_keywords = ["ç•«", "ç¹ªè£½", "é¡¯ç¤º", "show", "plot", "draw"]
        chart_keywords = ["åœ–", "è¶¨å‹¢", "chart", "trend", "graph", "æŠ˜ç·š", "æ›²ç·š"]
        # æ’é™¤å«æœ‰æ·±åº¦åˆ†ææ„åœ–çš„èªå¥
        deep_analysis_keywords = [
            "åˆ†æ",
            "è¨ºæ–·",
            "ç•°å¸¸",
            "åŸå› ",
            "ç‚ºä»€éº¼",
            "å½±éŸ¿",
            "é—œè¯",
            "åµæ¸¬",
            "æ¯”è¼ƒ",
            "å°æ¯”",
        ]

        has_viz_intent = any(kw in query_lower for kw in viz_keywords)
        has_chart_intent = any(kw in query_lower for kw in chart_keywords)
        has_deep_intent = any(kw in query_lower for kw in deep_analysis_keywords)

        if (
            "analysis" in intent
            and (has_viz_intent or has_chart_intent)
            and not has_deep_intent
            and ev.file_id
        ):
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            if summary:
                params_list = summary.get("parameters", [])
                mappings = summary.get("mappings", {}) if summary else {}
                total_rows = summary.get("total_rows", 0)

                # å¾ query ä¸­æå–åƒæ•¸åç¨± (ç²¾ç¢ºåŒ¹é…å·²çŸ¥æ¬„ä½å)
                extracted_params = []
                query_upper = ev.query.upper()
                for p in params_list:
                    if p.upper() in query_upper:
                        extracted_params.append(p)

                # å¦‚æœæ²’æœ‰ç²¾ç¢ºåŒ¹é…åˆ°ï¼Œå˜—è©¦ç”¨ regex æå–å·¥æ¥­æ„Ÿæ¸¬å™¨ä»£ç¢¼æ¨¡å¼
                if not extracted_params:
                    sensor_matches = re.findall(
                        r"[A-Z][A-Z0-9]*[-_][A-Z0-9]+[-_][A-Z0-9]+", ev.query
                    )
                    for sm in sensor_matches:
                        # å¤§å°å¯«ä¸æ•æ„ŸåŒ¹é…
                        for p in params_list:
                            if p.upper() == sm.upper():
                                extracted_params.append(p)
                                break

                if extracted_params:
                    ctx.write_event_to_stream(
                        ProgressEvent(
                            msg=f"-- [å¿«è»Šé“] åµæ¸¬åˆ°ç¹ªåœ–æŒ‡ä»¤ï¼Œç›´æ¥æ“·å– {', '.join(extracted_params)} çš„æ™‚é–“åºåˆ—æ•¸æ“š..."
                        )
                    )

                    # ç›´æ¥å‘¼å« get_time_series_data å·¥å…·
                    tool_params = {
                        "file_id": ev.file_id,
                        "parameters": extracted_params,
                    }

                    # å¦‚æœ query ä¸­æœ‰æŒ‡å®šç¯„åœï¼Œä¹ŸåŠ ä¸Š
                    range_patterns = [
                        (
                            r"ç¬¬?\s*(\d+)\s*(?:ç­†)?\s*(?:åˆ°|è‡³|~|ï½|to|-|èˆ‡)\s*ç¬¬?\s*(\d+)\s*(?:ç­†)?",
                            "range",
                        ),
                        (r"(\d+)\s*[~-]\s*(\d+)", "range"),
                    ]
                    for rp, rtype in range_patterns:
                        rm = re.search(rp, ev.query)
                        if rm and rtype == "range":
                            tool_params["target_segments"] = (
                                f"{rm.group(1)}-{rm.group(2)}"
                            )
                            break

                    try:
                        chart_data = await self.tool_executor.execute_tool(
                            "get_time_series_data", tool_params, ev.session_id
                        )

                        if (
                            isinstance(chart_data, dict)
                            and "data" in chart_data
                            and chart_data["data"]
                        ):
                            ctx.write_event_to_stream(
                                ProgressEvent(
                                    msg=f"-- [å¿«è»Šé“] æ•¸æ“šæ“·å–å®Œæˆ ({chart_data.get('total_points', 0)} ç­†)ï¼Œæ­£åœ¨ç¹ªè£½åœ–è¡¨..."
                                )
                            )
                            return VisualizingEvent(
                                data=chart_data,
                                query=ev.query,
                                file_id=ev.file_id,
                                session_id=ev.session_id,
                                history=ev.history,
                                mode=ev.mode,
                                row_count=chart_data.get("total_points", total_rows),
                                col_count=len(chart_data.get("data", {}).keys()),
                                mappings=mappings,
                                suspect_pool=ev.suspect_pool,
                            )
                        else:
                            # æ•¸æ“šç²å–å¤±æ•—ï¼Œå›é€€åˆ°æ­£å¸¸åˆ†ææµç¨‹
                            ctx.write_event_to_stream(
                                ProgressEvent(
                                    msg="-- [å¿«è»Šé“] æ•¸æ“šæ“·å–å¤±æ•—ï¼Œæ”¹èµ°æ¨™æº–åˆ†ææµç¨‹..."
                                )
                            )
                    except Exception as e:
                        logger.warning(
                            f"Visualization Fast-Track failed: {e}, falling back to analysis"
                        )
                        ctx.write_event_to_stream(
                            ProgressEvent(
                                msg="-- [å¿«è»Šé“] å¿«é€Ÿç¹ªåœ–å¤±æ•—ï¼Œæ”¹èµ°æ¨™æº–åˆ†ææµç¨‹..."
                            )
                        )
                else:
                    # [CLARIFICATION] ç”¨æˆ¶æƒ³ç•«åœ–ä½†æ²’èªªç•«ä»€éº¼
                    # åœæ­¢çŒœæ¸¬ï¼Œç›´æ¥åå•ç”¨æˆ¶
                    ctx.write_event_to_stream(
                        ProgressEvent(msg="-- [å¿«è»Šé“] åµæ¸¬åˆ°ç¹ªåœ–æ„åœ–ï¼Œä½†æœªæŒ‡å®šåƒæ•¸...")
                    )
                    return SummarizeEvent(
                        data={
                            "direct_reply": (
                                "è«‹å•æ‚¨æƒ³è¦ç¹ªè£½å“ªä¸€å€‹åƒæ•¸çš„è¶¨å‹¢åœ–ï¼Ÿ\n\n"
                                "è«‹æ˜ç¢ºæŒ‡å®šåƒæ•¸åç¨±ï¼ˆä¾‹å¦‚ï¼šã€Œç•« **(åƒæ•¸å)** çš„è¶¨å‹¢åœ–ã€ï¼‰ã€‚\n"
                                "ç›®å‰ç³»çµ±ç„¡æ³•å¾—çŸ¥æ‚¨çš„ç¹ªåœ–ç›®æ¨™ï¼Œè«‹è£œå……èª¬æ˜ã€‚"
                            )
                        },
                        query=ev.query,
                        file_id=ev.file_id,
                        session_id=ev.session_id,
                        history=ev.history,
                        mode=ev.mode,
                    )

        # --- é›¶å»¶é²å¿«è»Šé“ (Metadata Fast-Track) ---
        # å¦‚æœåªæ˜¯æƒ³çŸ¥é“æ¬„ä½æ¸…å–®ã€è¡Œæ•¸æˆ–æª”æ¡ˆæ‘˜è¦ï¼Œæ²’å¿…è¦å‹•ç”¨ AI å¤§è…¦
        summary_keywords = [
            "æœ‰å“ªäº›æ¬„ä½",
            "æ¬„ä½æ¸…å–®",
            "æ‰€æœ‰åƒæ•¸",
            "å¹¾ç­†è³‡æ–™",
            "ç¸½è¡Œæ•¸",
            "å¹¾è¡Œ",
            "æ‘˜è¦",
            "æ¦‚æ³",
            "é€™ä»½æª”æ¡ˆ",
            "ç°¡ä»‹",
        ]
        if "analysis" in intent and (any(kw in query_lower for kw in summary_keywords)):
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            if summary:
                ctx.write_event_to_stream(
                    ProgressEvent(
                        msg="â”€ æª¢æ¸¬åˆ°åŸºç¤è³‡è¨Š/æ‘˜è¦æŸ¥è©¢ï¼Œæ­£åœ¨å¾å¿«å–ç›´æ¥æå–ç­”æ¡ˆ..."
                    )
                )
                params_list = summary.get("parameters", [])
                total_rows = summary.get("total_rows", 0)
                categories = summary.get("categories", {})

                # æ§‹å»ºçµæ§‹åŒ–æ‘˜è¦
                cat_info = ", ".join(
                    [f"{k} ({len(v)}å€‹)" for k, v in categories.items()]
                )
                quality_stats = summary.get("quality_stats", {})

                content = (
                    f"### æª”æ¡ˆæ¦‚æ³æ‘˜è¦\n\n"
                    f"æ­¤æª”æ¡ˆå…±æœ‰ **{len(params_list)}** å€‹æ¬„ä½ï¼Œç¸½è¨ˆ **{total_rows}** ç­†æ•¸æ“šã€‚\n"
                    f"**æ•¸æ“šåˆ†é¡**: {cat_info}\n\n"
                )

                # æ–°å¢ï¼šå“è³ªæè¿° (æ›´è©³ç´°ç‰ˆ)
                quality_msg = []
                null_cols = quality_stats.get("null_columns_preview", [])
                const_cols = quality_stats.get("constant_columns_preview", [])
                sparse_cols = quality_stats.get("sparse_columns_preview", [])

                if quality_stats.get("null_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {len(null_cols)} å€‹é«˜ç¼ºå¤±ç‡æ¬„ä½ ({', '.join(null_cols[:3])}...)"
                    )

                if quality_stats.get("sparse_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {quality_stats['sparse_column_count']} å€‹ç¨€ç–æ¬„ä½ (çœŸå€¼æ¯”ä¾‹ < 80%ï¼Œå¦‚ {', '.join(sparse_cols[:3])})"
                    )

                if quality_stats.get("constant_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {quality_stats['constant_column_count']} å€‹å®šå€¼/å…¨é›¶æ¬„ä½ (å¦‚ {', '.join(const_cols[:3])})"
                    )

                if quality_msg:
                    content += (
                        "**æ•¸æ“šå“è³ªè­¦è¨Š**: \n- " + "\n- ".join(quality_msg) + "\n\n"
                    )
                else:
                    content += "**æ•¸æ“šå“è³ª**: æ•¸æ“šå®Œæ•´ï¼Œç„¡æ˜é¡¯ç¼ºå¤±æˆ–ç¨€ç–æ¬„ä½ã€‚\n\n"

                content += "æ‚¨å¯ä»¥å•æˆ‘é—œæ–¼é€™äº›åƒæ•¸çš„è¶¨å‹¢ã€ç•°å¸¸åµæ¸¬æˆ–ç›¸é—œæ€§åˆ†æã€‚"

                if any(
                    kw in query_lower
                    for kw in [
                        "æ¬„ä½",
                        "åƒæ•¸",
                        "æ¸…å–®",
                        "å“ªäº›",
                        "å“ªå…©å€‹",
                        "å“ªå¹¾å€‹",
                        "é‚£å…©å€‹",
                        "é‚£å¹¾å€‹",
                    ]
                ):
                    content += (
                        f"\n\nå…¨éƒ¨æ¬„ä½æ¸…å–®å¦‚ä¸‹ (å…± {len(params_list)} å€‹):\n"
                        f"{', '.join(params_list)}"
                    )
                    # å¼·åˆ¶æ””æˆªï¼šç”¨æˆ¶è¿½å•ç©ºå€¼æ¬„ä½
                    if (
                        "ç©ºå€¼" in query_lower
                        or "ç¼ºå¤±" in query_lower
                        or "å“ªå…©å€‹" in query_lower
                    ):
                        null_cols = quality_stats.get("null_columns_preview", [])
                        if null_cols:
                            content += f"\n\n### ğŸ”´ é«˜ç¼ºå¤±ç‡æ¬„ä½è©³ç´°æ¸…å–®:\n**{', '.join(null_cols)}**\n(é€™äº›æ¬„ä½å¹¾ä¹ç‚ºç©ºï¼Œå»ºè­°å¿½ç•¥æˆ–æª¢æŸ¥ä¾†æº)"
                return SummarizeEvent(
                    data={"final_decision": content, "all_steps_results": []},
                    query=ev.query,
                    file_id=ev.file_id,
                    session_id=ev.session_id,
                    history=ev.history,
                    mode=ev.mode,
                    row_count=total_rows,
                    col_count=len(params_list),
                    mappings=summary.get("mappings", {}),
                    suspect_pool=ev.suspect_pool,
                )

        if "analysis" in intent:
            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                suspect_pool=ev.suspect_pool,
            )
        return TranslationEvent(
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            mode=ev.mode,
            suspect_pool=ev.suspect_pool,
        )

    @step
    async def execute_analysis(
        self, ctx: Context, ev: AnalysisEvent
    ) -> Union[AnalysisEvent, VisualizingEvent, SummarizeEvent]:
        """
        [Local Step] åŸ·è¡Œæ™ºæ…§åˆ†ææ±ºç­– (æ”¯æŒæœ€å¤š 3 æ­¥çš„å¾ªç’°è¨ºæ–·)
        """
        # [INTERRUPT CHECK] æª¢æŸ¥æ˜¯å¦æ”¶åˆ°ç«‹å³å›ç­”æŒ‡ä»¤
        if self.tool_executor.analysis_service.is_generation_stopped(ev.session_id):
            ctx.write_event_to_stream(
                ProgressEvent(msg="âš¡ æ”¶åˆ°ç«‹å³å›ç­”æŒ‡ä»¤ï¼Œä¸­æ­¢åˆ†æä¸¦ç”Ÿæˆçµè«–...")
            )
            # æ¸…é™¤ä¿¡è™Ÿä»¥å…å½±éŸ¿ä¸‹æ¬¡
            self.tool_executor.analysis_service.clear_stop_signal(ev.session_id)

            # ä½¿ç”¨ç›®å‰ç´¯ç©çš„çµæœç›´æ¥ç¸½çµ
            return SummarizeEvent(
                data={
                    "all_steps_results": ev.prev_results,
                    "reason": "user_interruption",
                },
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,  # å‚³éç›®å‰ç‚ºæ­¢çš„å°è©±æ­·å²
                mode=ev.mode,
                suspect_pool=ev.suspect_pool,
            )

        summary = self.tool_executor.analysis_service.load_summary(
            ev.session_id, ev.file_id
        )
        params_list = summary.get("parameters", []) if summary else []
        total_cols = len(params_list)
        total_rows = summary.get("total_rows", 0) if summary else 0

        # åªæœ‰åœ¨ç¬¬ä¸€æ­¥é¡¯ç¤ºè©³ç´°æª¢ç´¢è¨Šæ¯ï¼Œå¾ŒçºŒæ­¥æ•¸é¡¯ç¤ºç°¡æ½”é€²åº¦
        if ev.step_count == 1:
            ctx.write_event_to_stream(
                ProgressEvent(
                    msg=f"â”€ æ­£åœ¨åˆå§‹åŒ–åˆ†æç’°å¢ƒï¼Œé–å®š {total_cols} å€‹åŸå§‹æ¬„ä½..."
                )
            )

        mappings = summary.get("mappings", {}) if summary else {}

        # --- [NEW] èªæ„ç¯„åœé è™•ç†å™¨ (Range Pre-processor) ---
        # æ•æ‰å„ç¨®å¯«æ³•ä¸¦æ¨™æº–åŒ–
        range_patterns = [
            (
                r"ç¬¬?\s*(\d+)\s*(?:ç­†)?\s*(?:åˆ°|è‡³|~|ï½|to|-|èˆ‡)\s*ç¬¬?\s*(\d+)\s*(?:ç­†)?",
                "range",
            ),
            (r"ç¬¬?\s*(\d+)\s*(?:ç­†)?\s*(?:ä¹‹å¾Œ|ä»¥å¾Œ|èµ·|onwards|\+)", "after"),
            (r"ç¬¬?\s*(\d+)\s*(?:ç­†)?\s*(?:ä»¥å‰|ä¹‹å‰|æ­¢|before|up to)", "before"),
            (r"(?:ç¬¬)\s*(\d+)\s*(?:ç­†)", "single"),
            (r"(\d+)\s*(?:-|~|ï½|to)\s*(\d+)", "range"),  # ç°¡ç‰ˆå¦‚ 30-50
        ]

        detected_range = None
        standard_format = None
        for pattern_str, p_type in range_patterns:
            match = re.search(pattern_str, ev.query)
            if match:
                groups = match.groups()
                if p_type == "range":
                    s, e = groups
                    standard_format = f"{s}-{e}"
                    detected_range = (
                        f"ã€åµæ¸¬åˆ°ç›®æ¨™ç¯„åœã€‘: {s} åˆ° {e} (æ¨™æº–æ ¼å¼: {standard_format})"
                    )
                elif p_type == "after":
                    s = groups[0]
                    if total_rows > 0:
                        standard_format = f"{s}-{total_rows - 1}"
                        detected_range = f"ã€åµæ¸¬åˆ°é–‹æ”¾ç¯„åœã€‘: ç¬¬ {s} ç­†ä¹‹å¾Œ (0-indexed ç¯„åœ: {standard_format})"
                    else:
                        standard_format = f"{s}+"
                        detected_range = f"ã€åµæ¸¬åˆ°é–‹æ”¾ç¯„åœã€‘: ç¬¬ {s} ç­†ä¹‹å¾Œ"
                elif p_type == "before":
                    e = groups[0]
                    standard_format = f"0-{e}"
                    detected_range = f"ã€åµæ¸¬åˆ°é–‹æ”¾ç¯„åœã€‘: ç¬¬ {e} ç­†ä»¥å‰ (0-indexed ç¯„åœ: {standard_format})"
                elif p_type == "single":
                    idx = groups[0]
                    standard_format = str(idx)
                    detected_range = f"ã€åµæ¸¬åˆ°ç›®æ¨™å–®é»ã€‘: ç¬¬ {idx} ç­†"
                break

        range_mandate = ""
        if detected_range:
            range_mandate = (
                f"\n!!! é‡è¦ï¼šç³»çµ±å·²è‡ªå‹•è­˜åˆ¥åˆ†æå€é–“ !!!\n"
                f"{detected_range}\n"
                f'è«‹å‹™å¿…åœ¨å·¥å…·åƒæ•¸ (å¦‚ target_segments) ä¸­å¡«å…¥: "{standard_format}"ã€‚\n'
                f"çµ•å°ç¦æ­¢éš¨æ„æ›´æ”¹æˆ–ç¸®æ¸›æ­¤ç¯„åœã€‚\n"
            )
            # åœ¨ä¸²æµä¸­çµ¦ç”¨æˆ¶åé¥‹ï¼Œå¢åŠ é€æ˜åº¦
            if ev.step_count == 1:
                ctx.write_event_to_stream(
                    ProgressEvent(
                        msg=f"â”€ [è­·æ¬„åŒæ­¥] åµæ¸¬åˆ°é—œéµç´¢å¼• {standard_format}ï¼Œå·²è‡ªå‹•è£œå…¨åˆ†æç¯„åœåƒæ•¸..."
                    )
                )

        # --- å®‰å…¨é–¥ï¼šè§£é–æ·±åº¦è¨ºæ–·åˆ†æ ---
        MAX_STEPS = 30
        is_last_step = ev.step_count >= MAX_STEPS

        tool_specs = self.tool_executor.list_tools()

        # --- ç¡¬æ ¸é™åˆ¶ï¼šStep 1 åªèƒ½è§€å¯Ÿï¼Œä¸èƒ½è·³æ¼”ç®—æ³• ---
        if ev.mode == "deep" and ev.step_count == 1:
            forbidden_step1 = [
                "hotelling_t2_analysis",
                "systemic_pca_analysis",
                "causal_relationship_analysis",
                "multivariate_anomaly_detection",
                "analyze_feature_importance",
            ]
            tool_specs = [t for t in tool_specs if t["name"] not in forbidden_step1]
            ctx.write_event_to_stream(
                ProgressEvent(
                    msg="â”€ 5-Why è¨ºæ–·å•Ÿå‹•ï¼šç¬¬ä¸€æ­¥å·²å¼·åˆ¶é–å®šç‚ºã€Œæ•¸æ“šè§€å¯Ÿèˆ‡é©—è­‰ã€éšæ®µã€‚"
                )
            )

        # --- æ¬„ä½æ¸…å–®æ™ºæ…§åˆ†é¡èˆ‡ç‰©ç†åç¨±è½‰è­¯ (Categorized Column Display) ---
        # æ’é™¤åŒ…å« ID, TIME, CONTEXT ç­‰é—œéµå­—çš„æ¬„ä½ä½œç‚º Target
        id_keywords = ["ID", "TIME", "CONTEXT", "LOT", "WAFER", "DATE"]
        metadata_cols = [
            p for p in params_list if any(k in p.upper() for k in id_keywords)
        ]
        core_features = [p for p in params_list if p not in metadata_cols]

        mapping_info = [f"{p} ({mappings.get(p, p)})" for p in core_features[:20]]

        all_columns_display = (
            f"ã€æ ¸å¿ƒæ•¸å€¼ç‰¹å¾µ (å¯ç”¨æ–¼ Target ({len(core_features)}å€‹)ã€‘: {', '.join(mapping_info)}...\n"
            f"ã€ä¸­ç¹¼/ID æ¬„ä½ (ä¸å¯ä½œç‚º Target ({len(metadata_cols)}å€‹)ã€‘: {', '.join(metadata_cols[:10])}...\n"
            "AI æç¤ºï¼šåš´ç¦é¸æ“‡ã€ŒID æ¬„ä½ã€ä½œç‚ºåˆ†æ targetã€‚è«‹å„ªå…ˆé¸æ“‡æ•¸å€¼å‹æ ¸å¿ƒç‰¹å¾µã€‚"
        )

        # æ§‹å»ºéå»æ­¥é©Ÿçš„èƒŒæ™¯è³‡è¨Š
        history_context = ""
        simplified_history = []
        if ev.prev_results:
            # åƒ…ä¿ç•™é—œéµçµæœï¼Œç¸®æ¸› Token
            for r in ev.prev_results:
                # [TOKEN OPTIMIZATION] é‡å°å¤§æ•¸æ“šå·¥å…·é€²è¡Œçµæœæ‘˜è¦
                # å¦‚æœæ˜¯ get_time_series_dataï¼Œçµ•å°ä¸è¦å°‡ raw data å¡å› context
                if r.get("tool") == "get_time_series_data":
                    res_data = r.get("result", {})
                    if isinstance(res_data, dict) and "data" in res_data:
                        # åªä¿ç•™ metadataï¼Œç§»é™¤å¯¦éš›æ•¸æ“šé»
                        truncated_result = str(
                            {
                                "status": "success",
                                "message": "Time series data retrieved successfully",
                                "parameters": res_data.get("parameters"),
                                "total_points": res_data.get("total_points"),
                                "target_range": res_data.get("target_range"),
                                "note": "Data omitted for token optimization (available in chart)",
                            }
                        )
                    else:
                        truncated_result = str(res_data)[:200]
                else:
                    # ä¸€èˆ¬å·¥å…·çµæœï¼šæˆªæ–·éé•·çš„è¼¸å‡ºä»¥ç¯€çœ Context
                    raw_result = str(r.get("result", ""))
                    truncated_result = (
                        raw_result[:800] + "...(ç•¥)"
                        if len(raw_result) > 800
                        else raw_result
                    )

                simplified_history.append(
                    {
                        "step": r.get("step"),
                        "tool": r.get("tool"),
                        "params": r.get("params"),
                        "result": truncated_result,  # [NEW] è®“ AI çœ‹è¦‹éå»çš„æ•¸æ“š
                        "monologue": r.get("monologue"),
                    }
                )

        # --- å‹•æ…‹è¿½è¹¤ Why å±¤ç´šã€è­‰æ“šå›æº¯èˆ‡ä¿®æ­£è­·æ¬„ ---
        current_why = 1
        hallucination_correction = ""
        last_monologue = ""
        key_evidence = ""

        if ev.prev_results:
            last_r = ev.prev_results[-1]
            last_monologue = last_r.get("monologue", "")

            # A. æå– Why å±¤ç´š (æƒæå…¨éƒ¨æ­·å²ï¼Œå–æœ€é«˜å€¼ï¼Œé˜²æ­¢å€’é€€)
            max_why_seen = 1
            for r in ev.prev_results:
                mono = r.get("monologue", "")
                why_matches = re.findall(r"\[Why\s*#(\d+)\]", mono)
                if why_matches:
                    max_why_seen = max(max_why_seen, max(int(w) for w in why_matches))
            current_why = max_why_seen

            # å¦‚æœæœ€å¾Œä¸€æ­¥å·²å« [Conclusion]ï¼Œä»£è¡¨è©²å±¤ Why å·²çµæ¡ˆï¼Œæ‡‰é€²å…¥ä¸‹ä¸€å±¤
            if "[Conclusion]" in last_monologue:
                current_why = max_why_seen + 1
                logger.info(
                    f"[Why Tracker] Last step concluded Why #{max_why_seen}, advancing to Why #{current_why}"
                )

            # B. æ‰¾å°‹é—œéµæ•¸æ“šè­‰æ“š (ä¾‹å¦‚ Hotelling T2 çš„ Top 3)
            all_summaries = []
            for r in ev.prev_results:
                res = r.get("result", {})
                if isinstance(res, dict) and "top_3_summary" in res:
                    all_summaries.append(
                        f"ç¬¬ {r.get('step')} æ­¥ç™¼ç¾: {res['top_3_summary']}"
                    )
            if all_summaries:
                key_evidence = "\nã€é—œéµæ­·å²è­‰æ“š (çµ•å°å„ªå…ˆåƒè€ƒ)ã€‘:\n" + "\n".join(
                    all_summaries
                )

            # C. ä¿®æ­£å¼·è¡Œé—œè¯å¹»è¦º
            if "242" in last_monologue and "20" in last_monologue:
                hallucination_correction = "ã€æ ¸å¿ƒä¿®æ­£ä»¤ã€‘åµæ¸¬åˆ°å‰åºæ­¥é©ŸéŒ¯èª¤åœ°å°‡ã€Œç¬¬ 242 ç­†ã€èˆ‡ã€Œç¬¬ 20 ç­†ã€é€²è¡Œäº†é—œè¯ã€‚é€™æ˜¯ä¸€å€‹é‚è¼¯éŒ¯èª¤ã€‚ç¬¬ 242 ç­†æ˜¯å…¨åŸŸç•°å¸¸é»ï¼Œè€Œç¬¬ 20 ç­†æ˜¯æ‚¨çš„ç›®æ¨™ã€‚è«‹çµ•å°ç¦æ­¢å†èªª 242 ä»£è¡¨ 20ï¼Œå°ˆæ³¨æ–¼åˆ†æç¬¬ 20 ç­†è·Ÿæ­£å¸¸æ•¸æ“šçš„å·®ç•°ã€‚"

        history_context = (
            "\n### å‰åºåˆ†æçµæœæ‘˜è¦ (å«æ•¸æ“šè¨˜æ†¶) ###\n"
            + json.dumps(simplified_history, ensure_ascii=False)
            + (key_evidence if key_evidence else "")
        )

        quality_stats = summary.get("quality_stats", {})
        null_count = quality_stats.get("null_column_count", 0)
        const_count = quality_stats.get("constant_column_count", 0)
        sparse_count = quality_stats.get("sparse_column_count", 0)

        # å°‡åƒåœ¾æ•¸æ“šæ¬„ä½æ¨™è¨˜ç‚ºã€Œé»‘åå–®ã€ï¼Œä¸å†æä¾›å…·é«”åç¨±ä»¥å… AI åˆ†å¿ƒ
        quality_info = f"ã€é»‘åå–®è­¦å ±ã€‘åµæ¸¬åˆ° {null_count} å€‹å…¨ç©ºæ¬„ä½èˆ‡ {const_count} å€‹å®šå€¼æ¬„ä½ã€‚é€™äº›æ¬„ä½å·²è¢«ç³»çµ±è‡ªå‹•å‰”é™¤ï¼Œ**çµ•å°ç¦æ­¢æåŠæˆ–åˆ†æå®ƒå€‘**ã€‚"
        if sparse_count > 0:
            quality_info += (
                f" å¦æœ‰ {sparse_count} å€‹æ¬„ä½æ•¸æ“šæ¥µåº¦ç¨€ç–ï¼Œè«‹å„ªå…ˆé¸æ“‡æ•¸æ“šå®Œæ•´çš„åƒæ•¸ã€‚"
            )

        # æ ¹æ“šæ¨¡å¼åˆ‡æ›æŒ‡ä»¤é›†
        mode_instruction = ""
        if ev.mode == "deep":
            mode_instruction = (
                "## ç•¶å‰æ¨¡å¼ï¼šæ·±åº¦è¨ºæ–· (5-Why Methodology - Global Sweep) ##\n"
                "ä½ å¿…é ˆåš´æ ¼éµå¾ªã€Œç”±æ·ºå…¥æ·±ã€è¿½æ ¹ç©¶åº•ã€çš„ç§‘å­¸è¨ºæ–·é‚è¼¯ï¼š\n"
                "1. **ã€å…¨å ´æƒæå¼·åˆ¶ä»¤ã€‘**: ç‚ºäº†ç¢ºä¿è¨ºæ–·çš„æœ€é«˜ç©©å®šåº¦èˆ‡é¿å…åè¦‹ï¼ŒåŸ·è¡Œ `hotelling_t2_analysis`, `compare_data_segments` æˆ– `systemic_pca_analysis` æ™‚ï¼Œ**å¿…é ˆ**å°‡ `parameters` è¨­ç‚º `'all'`ã€‚ç¦æ­¢è‡ªè¡ŒæŒ‘é¸ 3-5 å€‹åƒæ•¸ã€‚\n"
                "2. **ã€è¨ºæ–·ç¯€å¥ã€‘å…ˆå…¨é«”æª¢ã€å†é–å®šç—…ç¶**: \n"
                "   - ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ `compare_data_segments(parameters='all')` è§€å¯Ÿå…¨å ´å–®é»ä½ç§»ã€‚\n"
                "   - ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ `hotelling_t2_analysis(parameters='all')` åµæ¸¬ç³»çµ±æ€§çµ„åˆç•°å¸¸ã€‚\n"
                "   - å…©è€…è­‰æ“šé½Šå…¨å¾Œï¼Œæ‰èƒ½åœ¨ monologue ç¸½çµç•¶å‰çš„ Whyï¼Œä¸¦é€²å…¥ä¸‹ä¸€å€‹è¿½å•å±¤ç´šã€‚\n"
                "3. **ã€é ˜åŸŸçŸ¥è­˜äº¤æµ (Domain Exchange)ã€‘**: \n"
                "   - å¦‚æœä½¿ç”¨è€…çš„å•é¡Œåå‘ã€Œè£½ç¨‹åŸç†ã€ã€ã€Œç‰©ç†æ„ç¾©äº¤æµã€æˆ–ã€Œç¶­ä¿®ç¶“é©—æ¢è¨ã€è€Œéæ•¸æ“šè®€å–ï¼Œä½ æ‡‰å„ªå…ˆåˆ‡æ›ç‚ºå°ˆæ¥­é¡§å•è§’è‰²ã€‚\n"
                "   - åœ¨æ­¤æƒ…æ³ä¸‹ï¼Œä½¿ç”¨ `action: 'finish'` ä¸¦åœ¨å›è¦†ä¸­çµåˆç‰©ç†è­¯åèˆ‡ä½ çš„å…§å»ºçŸ¥è­˜åº«é€²è¡Œæ·±åº¦èªªæ˜ã€‚\n"
                "\n"
                "**ã€5-Why è¨ºæ–·çµæ§‹è¦ç¯„ (æ ¸å¿ƒå¼·åˆ¶åŸ·è¡Œ)ã€‘**\n"
                "1. åœ¨æ¯ä¸€è¼ªçš„ `monologue` ä¸­ï¼Œä½ å¿…é ˆåš´æ ¼æ¡ç”¨ä»¥ä¸‹çµæ§‹ï¼š\n"
                "   - **[Why #N]**: ç•¶å‰è¿½å–çš„ç•°å¸¸å±¤ç´š (ä¾‹å¦‚ï¼š[Why #1] è§£ææ•¸æ“šæ•´é«”åé›¢)\n"
                "   - **[Hypothesis]**: æ ¹æ“šæ•¸æ“šæˆ–ç‰©ç†æ„ç¾©æå‡ºçš„ã€æ ¸å¿ƒå‡è¨­ã€(ä¾‹å¦‚ï¼šæ‡·ç–‘æ˜¯çˆæº«æ³¢å‹•å°è‡´å“è³ªä¸‹é™)\n"
                "   - **[Action]**: è§£é‡‹é¸æ“‡ç‰¹å®šå·¥å…·çš„é‚è¼¯ã€‚\n"
                "   - **[Conclusion]**: æœ¬æ­¥çµæœè§£è®€åŠå…¶èˆ‡å‡è¨­çš„å°æ¯”ã€‚\n"
                "2. **è¿½æ ¹ç©¶åº•ä»¤**ï¼šå¦‚æœç•¶å‰æ­¥ç™¼ç¾æŸå€‹å…§éƒ¨åƒæ•¸æ˜¯ç•°å¸¸èµ·å› ï¼Œä½ **å¿…é ˆ**åœ¨ monologue çµæŸå‰æå‡ºä¸‹ä¸€å±¤æ¬¡çš„ Whyã€‚ç¦æ­¢åœ¨æœªæ¨å°è‡³åº•å±¤ç‰©ç†åŸå› å‰çµæŸåˆ†æã€‚\n"
                "\n"
                "**ã€ç‰©ç†æ„ç¾©å„ªå…ˆè¦ç¯„ã€‘**\n"
                "- ä½ ç¾åœ¨çœ‹åˆ°çš„æ¬„ä½æ¸…å–®å·²å«ã€Œç‰©ç†åç¨±ã€(å¦‚ï¼šOven Pressure)ã€‚è«‹åœ¨æ€è€ƒæ™‚ä»¥æ­¤å°æ‡‰é ˜åŸŸçŸ¥è­˜ã€‚\n"
                "- åœ¨æ¯ä¸€è¼ªçš„ `monologue` æ¬„ä½ä¸­ï¼Œä½ å¿…é ˆå…·é«”å›ç­”ï¼šæ ¹æ“šä¸Šä¸€æ­¥çš„æ•¸æ“šèˆ‡ç‰©ç†é‡ï¼ˆå¦‚ï¼šå£“åŠ›ã€æµé‡ï¼‰ï¼Œã€ç‚ºä»€éº¼ã€ä½ ç¾åœ¨è¦é¸æ“‡é€™å€‹å·¥å…·ï¼Ÿä½ æƒ³é©—è­‰ä»€éº¼å‡è¨­ï¼Ÿ"
            )
        else:
            mode_instruction = (
                "## ç•¶å‰æ¨¡å¼ï¼šå¿«é€Ÿå›æ‡‰ (Quick Response) ##\n"
                "ä½ çš„ç›®æ¨™æ˜¯åœ¨ **2 æ­¥å…§** çµ¦å‡ºç²¾ç¢ºçµè«–ï¼š\n"
                "1. å„ªå…ˆé¸æ“‡æœ€å¼·åŠ›çš„å–®ä¸€è¨ºæ–·å·¥å…· (å¦‚ `hotelling_t2_analysis` æˆ– `compare_data_segments`)ã€‚\n"
                "2. ç²å¾— Top 3 è²¢ç»åº¦å¾Œç«‹å³çµæ¡ˆï¼Œè§£é‡‹æ ¸å¿ƒåŸå› å³å¯ã€‚\n"
            )

        tools_json = json.dumps(tool_specs, ensure_ascii=False)
        tool_names_list = ", ".join([t["name"] for t in tool_specs])
        prompt_parts = [
            f"ä½ æ˜¯ä¸€å€‹æ©Ÿéˆä¸”åš´è¬¹çš„å·¥æ¥­æ•¸æ“šåˆ†æå°ˆå®¶ã€‚ç›®å‰æ˜¯è¨ºæ–·çš„ç¬¬ {ev.step_count} æ­¥ã€‚",
            f"åŸºç¤æ•¸æ“šè³‡è¨Š: ç•¶å‰æª”æ¡ˆå…±æœ‰ {total_rows} è¡Œæ•¸æ“šï¼Œ{total_cols} å€‹æ¬„ä½ã€‚",
            f"{range_mandate}",  # [æ ¸å¿ƒä¿®å¾©] ç›´æ¥æ³¨å…¥æ¨™æº–åŒ–å¾Œçš„ç¯„åœæŒ‡ç¤º
            f"æ•¸æ“šå“è³ªè­¦è¨Š (çµ•å°äº‹å¯¦): {quality_info}",
            f"æ‰€æœ‰å¯ç”¨æ¬„ä½ (éƒ¨åˆ†å±•ç¤º): {all_columns_display}",
            f"ã€åš´æ ¼å·¥å…·åç¨±æ¸…å–® (åªèƒ½ä½¿ç”¨ä»¥ä¸‹åç¨±ï¼Œç¦æ­¢è‡†é€ )ã€‘: {tool_names_list}",
            f"å·¥å…·è©³ç´°è¦æ ¼: {tools_json}",
            f"åˆ†æç›®æ¨™ (Query): {ev.query}",
            f"ã€ç•¶å‰å«Œç–‘åƒæ•¸æ±  (Suspect Pool)ã€‘: {ev.suspect_pool}",
            f"{history_context}",
            "",
            f"## ç•¶å‰æ¨¡å¼ï¼š{'æ·±åº¦è¨ºæ–· (5-Why)' if ev.mode == 'deep' else 'å¿«é€Ÿå›æ‡‰'} ##",
            mode_instruction,
            hallucination_correction,
            f"ç›®å‰è¨ºæ–·å±¤ç´š: [Why #{current_why}]",
            "## æ ¸å¿ƒåŸå‰‡ (åš´æ ¼åŸ·è¡Œ) ##",
            "1. **åƒæ•¸åç¨±ç²¾ç¢ºæ€§**: çµ•å°ç¦æ­¢ä½¿ç”¨é¡åˆ¥åç¨±ã€‚å¿…é ˆé¸å–å…·é«”çš„æ„Ÿæ¸¬å™¨ä»£ç¢¼ (å¦‚ 'PRESSDRY-DCS_A423')ã€‚",
            "2. **ã€5-Why å¼·åˆ¶çµæ§‹ã€‘**: ä½ çš„ `monologue` **å¿…é ˆ** åš´æ ¼éµå¾ªä»¥ä¸‹ Markdown æ ¼å¼ï¼š",
            "   ```",
            f"   [Why #{current_why}]: (æè¿°æœ¬å±¤è¿½æŸ¥çš„ç›®æ¨™)",
            "   [Hypothesis]: (æ ¹æ“šç‰©ç†æ„ç¾©æå‡ºçš„å‡è¨­)",
            "   [Action]: (è§£é‡‹ç‚ºä½•é¸æ“‡æ­¤å·¥å…·)",
            "   [Conclusion]: (æœ¬æ­¥åˆ†æçš„å…·é«”çµè«–ï¼Œä¸¦å®£å‘Šæ˜¯å¦é€²å…¥ä¸‹ä¸€å€‹ Why)",
            "   ```",
            "3. **å°æ¯”åˆ†æ (Abnormal vs Normal)**: ä»»ä½•åˆ†æéƒ½å¿…é ˆåŸºæ–¼å°æ¯”ã€‚è§£é‡‹ç›®æ¨™å€é–“èˆ‡åŸºæº–æ•¸æ“šçš„ Delta (å·®ç•°)ã€‚",
            "4. **åš´ç¦ç¡¬æ‹—**: ç¦æ­¢ç·¨é€ ç„¡é‚è¼¯çš„å› æœé—œè¯ï¼ˆå¦‚ 242 ä»£è¡¨ 20ï¼‰ã€‚",
            "5. **è¨˜æ†¶é‹ç”¨**: åƒè€ƒæ­·å²çµæœä¸­çš„ `result` è³‡æ–™ï¼Œä¸è¦é‡è¤‡åŸ·è¡Œã€‚",
            "6. **é€æ˜ç¨ç™½**: åœ¨ `monologue` ä¸­ç”¨ç¹é«”ä¸­æ–‡è§£é‡‹ä½ çš„æ€è€ƒè·¯å¾‘ã€‚",
            "7. **æ•¸æ“šèªªè©± (Delta-Driven)**: ä»»ä½•çµè«–éƒ½å¿…é ˆå»ºç«‹åœ¨ã€Œå·®ç•°ã€ä¹‹ä¸Š (ä¾‹å¦‚ï¼šç›®æ¨™å€é–“çš„ Z-Score åé›¢åŸºæº– 3 å€)ã€‚",
            f"8. **ç‹€æ…‹æé†’**: ç›®å‰æ˜¯ç¬¬ {ev.step_count} æ­¥ã€‚",
            "9. **ã€åš´æ ¼ç¯„åœä»¤ã€‘**: å¦‚æœä½¿ç”¨è€…æŒ‡å®šäº†æ•¸æ“šç¯„åœ (ä¾‹å¦‚ï¼š30-50, ç¬¬ 100 é»ç­‰)ï¼Œä½ **å¿…é ˆ**åœ¨å·¥å…·åƒæ•¸ä¸­ä½¿ç”¨ `target_segments` ç²¾ç¢ºå°æ‡‰ã€‚çµ•å°ç¦æ­¢ç§è‡ªç¸®æ¸›ç¯„åœï¼ˆå¦‚åªçœ‹ 30 é»ï¼‰ã€‚",
            "10. **ç¹é«”ä¸­æ–‡æŒ‡ä»¤**: ä½ å¿…é ˆå…¨é€šä½¿ç”¨ã€Œç¹é«”ä¸­æ–‡ã€é€²è¡Œæ€è€ƒèˆ‡å·¥å…·è¦åŠƒã€‚ç¦æ­¢ä½¿ç”¨è‹±æ–‡ã€‚",
            "11. **ã€å·¥å…·åç¨±ç²¾ç¢ºä»¤ã€‘**: `tool_name` **å¿…é ˆ**å¾ä¸Šæ–¹ã€Œåš´æ ¼å·¥å…·åç¨±æ¸…å–®ã€ä¸­ç²¾ç¢ºè¤‡è£½ã€‚ç¦æ­¢è‡ªè¡Œè‡†é€ æˆ–ç¸®å¯«å·¥å…·åç¨±ï¼ˆä¾‹å¦‚ï¼šç¦æ­¢ä½¿ç”¨ `analyze_correlation`ï¼Œæ­£ç¢ºåç¨±ç‚º `get_correlation_matrix` æˆ– `get_top_correlations`ï¼‰ã€‚",
            "## è¼¸å‡ºè¦ç¯„ ##",
            '1. è¼¸å‡ºç‚ºä¸€å€‹å®Œæ•´çš„ JSON ç‰©ä»¶ï¼ŒåŒ…å« "action", "tool_name", "params", "monologue", "suspect_pool" æ¬„ä½ã€‚',
            '2. "tool_name" å¿…é ˆæ˜¯ä¸Šæ–¹å·¥å…·æ¸…å–®ä¸­çš„ç²¾ç¢ºåç¨±ï¼Œä¸å¯è‡†é€ ã€‚',
            '3. "monologue" å¿…é ˆåš´æ ¼éµå®ˆä¸Šè¿° [Why] æ¨¡æ¿ã€‚',
            '4. "suspect_pool" æ‡‰åŒ…å«æ‚¨ç›®å‰èªç‚ºèˆ‡å•é¡Œç›¸é—œçš„æ‰€æœ‰æ„Ÿæ¸¬å™¨ä»£ç¢¼ (List of strings)ã€‚è«‹ç¹¼æ‰¿ä¸¦æ“´å……å®ƒã€‚',
            '4. "suspect_pool" æ‡‰åŒ…å«æ‚¨ç›®å‰èªç‚ºèˆ‡å•é¡Œç›¸é—œçš„æ‰€æœ‰æ„Ÿæ¸¬å™¨ä»£ç¢¼ (List of strings)ã€‚è«‹ç¹¼æ‰¿ä¸¦æ“´å……å®ƒã€‚',
            "",
            "## æ­»å··çªåœåŸå‰‡ (Dead-End Pivot Protocol) - è‡ªå‹•åŒ–è¦–è§’åˆ‡æ› ##",
            "ç•¶ä½ åœ¨æŸä¸€å±¤ Why åˆ†æä¸­ç™¼ç¾ã€Œç„¡é¡¯è‘—ç•°å¸¸ã€ã€ã€Œç›¸é—œæ€§ä½ã€æˆ–ã€Œæ‰¾ä¸å‡ºåŸå› ã€æ™‚ï¼Œ**åš´ç¦ç›´æ¥çµæ¡ˆ**ã€‚",
            "ä½ å¿…é ˆä¸»å‹•åˆ‡æ›åˆ†æç¶­åº¦ï¼Œå˜—è©¦ä»¥ä¸‹é€²éšæ¼”ç®—æ³•ä¾†çªç ´åƒµå±€ï¼š",
            "1. **å¦‚æœ Z-Score å‡æ­£å¸¸** â†’ æ”¹ç”¨ `local_outlier_factor` (LOF) åµæ¸¬å¯†åº¦ç•°å¸¸ (å°‹æ‰¾èº²åœ¨ç¾¤é«”ä¸­çš„ç•°é¡)ã€‚",
            "2. **å¦‚æœ ç›¸é—œä¿‚æ•¸ ä½** â†’ æ”¹ç”¨ `causal_relationship_analysis` (Granger) åµæ¸¬æ™‚é–“åºåˆ—ä¸Šçš„å› æœæ»¯å¾Œé—œä¿‚ã€‚",
            "3. **å¦‚æœ å–®é»æ•¸å€¼ å‡æ­£å¸¸** â†’ æ”¹ç”¨ `distribution_shift_analysis` (Wasserstein) åµæ¸¬æ•´é«”åˆ†ä½ˆæ˜¯å¦ç™¼ç”Ÿäº†å¾®å°çš„ç³»çµ±æ€§åç§»ã€‚",
            "4. **å¦‚æœ æ‰¾ä¸åˆ°é—œéµåƒæ•¸** â†’ æ”¹ç”¨ `analyze_feature_importance` (Random Forest) é€²è¡Œéç·šæ€§ç‰¹å¾µç¯©é¸ã€‚",
            "è¦å‰‡ï¼šä¸€æ—¦æ¨™æº–å·¥å…·æ’ç‰†ï¼Œmonologue å¿…é ˆå®£ç¨±ã€æ¨™æº–è¦–è§’æœªç™¼ç¾ç•°å¸¸ï¼Œåˆ‡æ›è‡³ [å·¥å…·å] é€²è¡Œæ·±å±¤ç¶­åº¦æƒæã€ã€‚",
            "",
            "## æ¼”ç®—æ³•æ¨è–¦å”è­° (Algorithm Recommendation Protocol) ##",
            "å¦‚æœä¸Šè¿°æ‰€æœ‰å…§éƒ¨å·¥å…·éƒ½ç„¡æ³•æœ‰æ•ˆè§£é‡‹ç¾è±¡ï¼Œä½ å¿…é ˆåˆ‡æ›ç‚ºã€æŠ€è¡“é¡§å•ã€è§’è‰²ï¼Œ",
            "æ ¹æ“šæ•¸æ“šç‰¹å¾µ (Data Pattern) åœ¨ `tool_gap` æ¬„ä½ä¸­ï¼Œæ¨è–¦ç”¨æˆ¶æ‡‰è©²å¼•å…¥çš„å¤–éƒ¨æ¼”ç®—æ³•ï¼š",
            "- **é€±æœŸæ€§/éœ‡ç›ª**: å»ºè­° `Fast Fourier Transform (FFT)` æˆ– `Wavelet Transform`ã€‚",
            "- **å¾®å°è¶¨å‹¢/è€åŒ–**: å»ºè­° `Mann-Kendall Test` æˆ– `CUSUM (ç´¯ç©å’Œæ§åˆ¶åœ–)`ã€‚",
            "- **éç·šæ€§è¤‡é›œé—œä¿‚**: å»ºè­° `XGBoost Feature Importance` æˆ– `Deep Autoencoder`ã€‚",
            "- **å¤šè®Šé‡å› æœç¶²**: å»ºè­° `Bayesian Network Structure Learning`ã€‚",
            "æ ¼å¼: åœ¨ JSON çš„ `tool_gap` æ¬„ä½ä¸­å…·é«”å¡«å¯«å»ºè­°çš„æ¼”ç®—æ³•åç¨±èˆ‡ç†ç”±ã€‚",
        ]
        prompt = "\n".join(prompt_parts)

        # 1. åªæœ‰ç¬¬ä¸€æ­¥é¡¯ç¤ºåº•å±¤å°é½Šè³‡è¨Šï¼Œæ¸›å°‘é‡è¤‡
        if ev.step_count == 1:
            ctx.write_event_to_stream(
                ProgressEvent(msg="â”€ æ­£åœ¨å°æ‡‰ç‰©ç†æ„Ÿæ¸¬å™¨è­¯åèˆ‡ç‰¹å¾µ...")
            )
            ctx.write_event_to_stream(
                ProgressEvent(msg="â”€ æ­£åœ¨å°é½Šæ­·å²è¨ºæ–·é‚è¼¯èˆ‡ 5-Why å‡è¨­...")
            )

        ctx.write_event_to_stream(
            ProgressEvent(
                msg=f"**[Step {ev.step_count}]** æ­£åœ¨åˆ†æä¸Šä¸‹æ–‡ä¸¦è¦åŠƒä¸‹ä¸€æ­¥è¡Œå‹•..."
            )
        )

        # å¼·åˆ¶é–‹å•Ÿ JSON æ¨¡å¼
        response = await self.llm.acomplete(prompt, json_mode=True)
        ctx.write_event_to_stream(
            ProgressEvent(msg="â”€ æ±ºç­–å·²ç”Ÿæˆï¼Œæº–å‚™åŸ·è¡Œè¨ºæ–·å·¥å…·...")
        )

        try:
            text = response.text.strip()
            # å„ªå…ˆè™•ç† Markdown ä»£ç¢¼å¡Š
            if "```" in text:
                text = text.split("```")[1].replace("json", "").strip()

            # å„ªå…ˆä½¿ç”¨ Regex æå– JSONï¼Œé˜²æ­¢ Ollama è¼¸å‡ºå¤šé¤˜æ–‡å­—æˆ–é‡è¤‡ JSON
            json_match = re.search(r"\{.*\}", response.text, re.DOTALL)
            if json_match:
                decision = json.loads(json_match.group(0))
            else:
                decision = json.loads(response.text)

            # --- ç¡¬æ ¸é˜²æ­»å¾ªç’°é‚è¼¯ ---
            tool_history = [
                (
                    r.get("tool"),
                    str(
                        r.get("params", {}).get("target")
                        or r.get("params", {}).get("parameter")
                    ),
                )
                for r in ev.prev_results
            ]
            current_tool = decision.get("tool_name")
            current_target = str(
                decision.get("params", {}).get("target")
                or decision.get("params", {}).get("parameter")
            )

            # å¦‚æœåŒä¸€å€‹å·¥å…·å°åŒä¸€å€‹ç›®æ¨™é€£çºŒåŸ·è¡Œè¶…é 2 æ¬¡ï¼Œå¼·åˆ¶ä¿®æ­£ç‚º finish
            repeat_count = tool_history.count((current_tool, current_target))
            if repeat_count >= 2:
                logger.warning(
                    f"Detected repeated tool call: {current_tool} on {current_target}. Forcing finish."
                )
                decision = {
                    "action": "finish",
                    "monologue": f"æª¢æ¸¬åˆ°é‡è¤‡åˆ†æè¡Œç‚º (å·²åŸ·è¡Œ {repeat_count} æ¬¡)ï¼Œç³»çµ±å¼·åˆ¶é€²å…¥æœ€çµ‚å½™æ•´éšæ®µä»¥æ‰“ç ´æ­»å¾ªç’°ã€‚",
                }

            action = decision.get("action", "call_tool")
            monologue = decision.get("monologue", "è¨ºæ–·ä¸­...")

            # --- å«Œç–‘åƒæ•¸æ± ç´¯ç©é‚è¼¯ (Suspect Pool Accumulation) ---
            new_suspects = decision.get("suspect_pool", [])
            if not isinstance(new_suspects, list):
                new_suspects = []

            # åˆä½µä¸¦å»é‡
            current_pool = list(set(ev.suspect_pool + new_suspects))
            # è¼”åŠ©ï¼šå¦‚æœ tool_name çš„ params ä¸­æœ‰æ˜ç¢ºçš„ target/parameterï¼Œä¹ŸåŠ å…¥ pool
            p_target = decision.get("params", {}).get("target") or decision.get(
                "params", {}
            ).get("parameter")
            if p_target and isinstance(p_target, str) and p_target != "all":
                if p_target not in current_pool:
                    current_pool.append(p_target)
            elif p_target and isinstance(p_target, list):
                for pt in p_target:
                    if pt not in current_pool:
                        current_pool.append(pt)

            # --- å·¥å…·ç¼ºå£å»ºè­°æ”¶é›† (Tool Gap Collection) ---
            tool_gap = decision.get("tool_gap")
            if tool_gap and isinstance(tool_gap, dict):
                tool_gaps = await ctx.get("tool_gaps", default=[])
                # é¿å…é‡è¤‡å»ºè­°
                existing_names = {g.get("name", "").lower() for g in tool_gaps}
                gap_name = tool_gap.get("name", "")
                if gap_name.lower() not in existing_names:
                    tool_gaps.append(tool_gap)
                    await ctx.set("tool_gaps", tool_gaps)
                    ctx.write_event_to_stream(
                        ProgressEvent(
                            msg=f"-- [å·¥å…·å»ºè­°] AI å»ºè­°å¼•å…¥: {gap_name} â€” {tool_gap.get('reason', '')}"
                        )
                    )

            # --- UI å„ªåŒ–ï¼šæ¸…ç†ç¨ç™½ä¸­çš„ JSON æˆ–ä»£ç¢¼å¡Šï¼Œé˜²æ­¢é»‘è‰²åº•æ¡†æ±¡æŸ“èŠå¤©å®¤ ---
            if isinstance(monologue, str):
                # ç§»é™¤ ```json ... ``` æˆ– ``` ... ``` ä»£ç¢¼å¡Š
                monologue = re.sub(r"```(?:json)?.*?\n", "", monologue)
                monologue = monologue.replace("```", "")
                # å¦‚æœ AI è¼¸å‡ºäº†ç´” JSON å­—ä¸²åœ¨ monologueï¼Œçµ¦äºˆé è¨­æ–‡å­—
                if monologue.strip().startswith("{") and monologue.strip().endswith(
                    "}"
                ):
                    monologue = "æ­£åœ¨æ ¹æ“šæ•¸æ“šç‰¹å¾µåŸ·è¡Œé€²éšé—œè¯æ€§è¨ºæ–·..."

            # 2. å‘Šè¨´ç”¨æˆ¶ AI æ±ºå®šè¦åšä»€éº¼ (å…§å¿ƒç¨ç™½)
            ctx.write_event_to_stream(ProgressEvent(msg=f"ğŸ’¡ ç­–ç•¥: {monologue}"))

            if action == "call_tool":
                tool_name = decision.get("tool_name")
                # 3. å‘Šè¨´ç”¨æˆ¶æ­£åœ¨åŸ·è¡Œä»€éº¼è€—æ™‚æ“ä½œ
                ctx.write_event_to_stream(
                    ProgressEvent(msg=f"ğŸ› ï¸ åŸ·è¡Œå·¥å…·: {tool_name} (æ­£åœ¨é‹ç®—æ•¸æ“š...)")
                )

        except Exception as e:
            # ç™¼ç”Ÿè§£æéŒ¯èª¤æ™‚çš„å¼·åˆ¶ä¿®å¾©é‚è¼¯
            logger.error(f"Error parsing LLM decision: {e}. Raw: {response.text}")
            # å¦‚æœæ˜¯ç¬¬ä¸€æ­¥å°±å¤±æ•—ï¼Œå¼·åˆ¶é€²è¡Œæ•¸æ“šæ¦‚è¦½åˆ†æï¼Œä¸å‡†ç›´æ¥ finish
            if ev.step_count == 1:
                action = "call_tool"
                tool_name = "get_data_overview"
                params = {"file_id": ev.file_id}
                monologue = "åŸå®šè¨ˆç•«è§£æå¤±æ•—ï¼Œå¼·åˆ¶å•Ÿå‹•æ•¸æ“šæ¦‚è¦½ä»¥æ‰“ç ´åƒµå±€ã€‚"
                decision = {
                    "action": action,
                    "tool_name": tool_name,
                    "params": params,
                    "monologue": monologue,
                }
            else:
                action = "finish"
                monologue = "é€£çºŒåˆ†æå‡ºç¾è§£æå›°é›£ï¼Œæº–å‚™é€²è¡Œæœ€çµ‚å½™æ•´ã€‚"
                decision = {"action": action, "monologue": monologue}

        if action == "finish" or is_last_step:
            # [Why Conclusion Registration - Finish Path]
            if ev.mode == "deep":
                last_tool_name = ""
                last_tool_result = {}
                if ev.prev_results:
                    last_tool_name = ev.prev_results[-1].get("tool", "")
                    lr = ev.prev_results[-1].get("result", {})
                    last_tool_result = lr if isinstance(lr, dict) else {}
                await self._register_why_conclusion(
                    ctx,
                    monologue,
                    current_why,
                    last_tool_name,
                    last_tool_result,
                    ev.step_count,
                )

            # çµæŸå‰æª¢æŸ¥æ˜¯å¦æœ‰å¯ç¹ªåœ–æ•¸æ“š (get_time_series_data)
            chart_data = None
            for step_res in ev.prev_results:
                if step_res.get("tool") == "get_time_series_data":
                    res_data = step_res.get("result", {})
                    if (
                        isinstance(res_data, dict)
                        and "data" in res_data
                        and res_data["data"]
                    ):
                        chart_data = res_data
                        break

            total_rows = summary.get("total_rows", 0) if summary else 0
            total_cols = len(params_list) if params_list else 0

            if chart_data:
                # å„ªå…ˆè·³è½‰åˆ°è¦–è¦ºåŒ–æ­¥é©Ÿï¼Œé€™æœƒç¢ºä¿ UI æ¸²æŸ“åœ–è¡¨
                return VisualizingEvent(
                    data=chart_data,
                    query=ev.query,
                    file_id=ev.file_id,
                    session_id=ev.session_id,
                    history=ev.history,
                    mode=ev.mode,
                    row_count=chart_data.get("total_points", total_rows),
                    col_count=len(chart_data.get("data", {}).keys()),
                    mappings=mappings,
                    suspect_pool=current_pool,
                )

            # å»ºç«‹é¡¯ç¤ºåç¨±æ˜ å°„
            full_display_mappings = {p: mappings.get(p, p) for p in params_list}

            # å„ªåŒ–ï¼šæå–å…·é«”çš„åˆ†æçµæœæ‘˜è¦ï¼Œé¿å… AI æ··æ·†
            aggregated_data = {
                "monologue_history": monologue,
                "latest_analysis_results": ev.prev_results[-1].get("result")
                if ev.prev_results
                else None,
                "full_tool_history": ev.prev_results,
            }

            return SummarizeEvent(
                data=aggregated_data,
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                row_count=total_rows,
                col_count=total_cols,
                mappings=full_display_mappings,
                suspect_pool=current_pool,
            )

        # å¦å‰‡ï¼ŒåŸ·è¡Œå·¥å…·ä¸¦é€²å…¥ä¸‹ä¸€æ­¥å¾ªç’°
        tool_name = decision.get("tool_name")
        params = decision.get("params", {})
        if not isinstance(params, dict):
            params = {}
        params["file_id"] = ev.file_id

        # --- [Smart Override] 5-Why åƒæ•¸é¸å–å¹³è¡¡é‚è¼¯èˆ‡å€é–“è­·æ¬„ ---
        # A. å€é–“è‡ªå‹•ç¹¼æ‰¿ï¼šå¦‚æœ query ä¸­æœ‰ 30-50 ä¸”å·¥å…·æ”¯æ´ä½†åƒæ•¸æ¼æ‰ï¼Œè‡ªå‹•è£œé½Š
        if "target_segments" not in params or not params["target_segments"]:
            # ä½¿ç”¨èˆ‡ Pre-processor ä¸€è‡´çš„é«˜éšæª¢æ¸¬æ¨¡å¼
            range_check_patterns = [
                r"ç¬¬?\s*(\d+)\s*(?:ç­†)?\s*(?:åˆ°|è‡³|~|ï½|to|-|èˆ‡)\s*ç¬¬?\s*(\d+)\s*(?:ç­†)?",
                r"ç¬¬?\s*(\d+)\s*(?:ç­†)?\s*(?:ä¹‹å¾Œ|ä»¥å¾Œ|èµ·|onwards|\+)",
                r"ç¬¬?\s*(\d+)\s*(?:ç­†)?\s*(?:ä»¥å‰|ä¹‹å‰|æ­¢|before|up to)",
                r"(\d+)\s*[~-]\s*(\d+)",
                r"ç¬¬\s*(\d+)\s*(?:ç­†|ç‰‡|å€‹|æ¢|åˆ—|çµ„|è™Ÿ|æ¨£æœ¬|è³‡æ–™)",  # å–®é»æ¨¡å¼
            ]
            for rp in range_check_patterns:
                rm = re.search(rp, ev.query)
                if rm:
                    groups = rm.groups()
                    if len(groups) == 2:
                        params["target_segments"] = f"{groups[0]}-{groups[1]}"
                    elif len(groups) == 1:
                        # ç°¡å–®è™•ç† Suffix / Prefix
                        if (
                            "ä¹‹å¾Œ" in rm.group(0)
                            or "ä»¥å¾Œ" in rm.group(0)
                            or "+" in rm.group(0)
                        ):
                            params["target_segments"] = (
                                f"{groups[0]}-{total_rows - 1}"
                                if total_rows > 0
                                else f"{groups[0]}+"
                            )
                        elif "ä»¥å‰" in rm.group(0) or "ä¹‹å‰" in rm.group(0):
                            params["target_segments"] = f"0-{groups[0]}"
                        else:
                            # å–®é»æ¨¡å¼ï¼šå¦‚ã€Œç¬¬30ç­†ã€
                            params["target_segments"] = str(groups[0])

                    if params.get("target_segments"):
                        ctx.write_event_to_stream(
                            ProgressEvent(
                                msg=f"â”€ [è­·æ¬„åŒæ­¥] åµæ¸¬åˆ°é—œéµç´¢å¼• {params['target_segments']}ï¼Œå·²è‡ªå‹•è£œé½Šåƒæ•¸..."
                            )
                        )
                    break

            # [Fallback] ç•¶ query ä¸­å®Œå…¨æ²’æœ‰æ•¸å­—ç¯„åœæ™‚ (å¦‚ã€Œæ•´å€‹æ™‚é–“æ®µã€ã€ã€Œæ¯”è¼ƒå…¨éƒ¨æ•¸æ“šã€)
            # ä¸”å·¥å…·ç¢ºå¯¦éœ€è¦ target_segmentsï¼Œä½¿ç”¨æ™ºæ…§é è¨­
            if (
                "target_segments" not in params or not params["target_segments"]
            ) and tool_name == "compare_data_segments":
                if total_rows > 0:
                    # é è¨­ç­–ç•¥ï¼šä»¥å¾ŒåŠæ®µ (50%~100%) ä½œç‚º targetï¼Œå‰åŠæ®µä½œç‚º baseline
                    midpoint = total_rows // 2
                    params["target_segments"] = f"{midpoint}-{total_rows - 1}"
                    ctx.write_event_to_stream(
                        ProgressEvent(
                            msg=f"â”€ [è­·æ¬„è£œé½Š] æœªåµæ¸¬åˆ°ç›®æ¨™å€é–“ï¼Œè‡ªå‹•æ¡ç”¨å¾ŒåŠæ®µæ•¸æ“š ({midpoint}-{total_rows - 1}) ä½œç‚ºæ¯”è¼ƒç›®æ¨™..."
                        )
                    )
                else:
                    # æ¥µç«¯é˜²ç¦¦ï¼šç„¡æ³•å–å¾— total_rowsï¼Œä½¿ç”¨ 0-å…¨åŸŸ
                    params["target_segments"] = "0-999"
                    ctx.write_event_to_stream(
                        ProgressEvent(
                            msg="â”€ [è­·æ¬„è£œé½Š] ç„¡æ³•ç¢ºå®šæ•¸æ“šç¯„åœï¼Œä½¿ç”¨é è¨­ç¯„åœ..."
                        )
                    )

        # A2. é€šç”¨å¿…å¡«åƒæ•¸è‡ªå‹•è£œé½Šè­·æ¬„ï¼š
        # ç•¶å·¥å…·éœ€è¦ parameters/target/parameter ä½† AI æ¼å‚³æ™‚ï¼Œ
        # å˜—è©¦å¾å«Œç–‘åƒæ•¸æ±  (suspect_pool) æˆ– monologue ä¸­è‡ªå‹•æå–
        tool_instance = self.tool_executor.get_tool(tool_name)
        if tool_instance:
            # ç­–ç•¥ A2-1: é å…ˆæª¢æŸ¥ä¸¦ invalid éŒ¯èª¤çš„åƒæ•¸ (ä¾‹å¦‚ target='30')
            if tool_name == "analyze_feature_importance" and params.get("target"):
                t_val = str(params["target"]).strip()
                # å¦‚æœæ˜¯ç´”æ•¸å­—ä¸”é•·åº¦çŸ­ï¼Œè¦–ç‚ºç„¡æ•ˆè¡Œè™Ÿå¼•ç”¨ï¼Œæ”¹ç‚ºè‡ªå‹•è£œé½Š
                if t_val.isdigit() and len(t_val) < 10:
                    ctx.write_event_to_stream(
                        ProgressEvent(
                            msg=f"â”€ [è­·æ¬„ä¿®æ­£] åµæ¸¬åˆ°ç„¡æ•ˆç›®æ¨™ '{t_val}' (å¯èƒ½æ˜¯è¡Œè™Ÿ)ï¼Œå˜—è©¦è‡ªå‹•ä¿®æ­£ç‚ºä¸Šä¸‹æ–‡ä¸­çš„åƒæ•¸..."
                        )
                    )
                    del params["target"]  # åˆªé™¤å®ƒï¼Œè®“å¾Œé¢çš„é‚è¼¯è£œé½Š

            missing_keys = [
                p
                for p in tool_instance.required_params
                if p != "file_id" and not params.get(p)
            ]

            if missing_keys:
                # å˜—è©¦å¾ monologue ä¸­æå–å·¥æ¥­æ„Ÿæ¸¬å™¨åç¨±
                extracted_from_monologue = []
                if monologue:
                    import re as _re

                    extracted_from_monologue = _re.findall(
                        r"[A-Z][A-Z0-9]*[-_][A-Z0-9]+[-_][A-Z0-9]+", monologue
                    )
                    extracted_from_monologue = list(
                        dict.fromkeys(extracted_from_monologue)
                    )  # å»é‡ä¿åº

                for missing_key in missing_keys:
                    # ç‰¹æ®Šè™•ç†ï¼štarget_segments å¾ query ä¸­æå–è¡Œè™Ÿ
                    if missing_key == "target_segments":
                        import re as _re2

                        # å˜—è©¦å¾ query ä¸­æå–å–®é»æˆ–å€é–“ç´¢å¼•
                        single_point = _re2.search(
                            r"ç¬¬\s*(\d+)\s*(?:ç­†|ç‰‡|å€‹|æ¢|åˆ—|çµ„|è™Ÿ|æ¨£æœ¬|è³‡æ–™)", ev.query
                        )
                        if single_point:
                            params["target_segments"] = single_point.group(1)
                            ctx.write_event_to_stream(
                                ProgressEvent(
                                    msg=f"â”€ [è­·æ¬„è£œé½Š] å¾ç”¨æˆ¶å•é¡Œä¸­æå–åˆ°ç›®æ¨™ç´¢å¼•: {single_point.group(1)}ï¼Œå·²å¡«å…¥ 'target_segments'"
                                )
                            )
                        elif total_rows > 0:
                            # å…¨åŸŸ fallbackï¼šquery ä¸­ç„¡ç´¢å¼•æ•¸å­—æ™‚ï¼Œè‡ªå‹•ä½¿ç”¨å¾ŒåŠæ®µ
                            midpoint = total_rows // 2
                            params["target_segments"] = f"{midpoint}-{total_rows - 1}"
                            ctx.write_event_to_stream(
                                ProgressEvent(
                                    msg=f"â”€ [è­·æ¬„è£œé½Š] æœªåµæ¸¬åˆ°å…·é«”ç›®æ¨™å€é–“ï¼Œè‡ªå‹•ä»¥å¾ŒåŠæ®µ ({midpoint}-{total_rows - 1}) ä½œç‚ºæ¯”è¼ƒå°è±¡..."
                                )
                            )
                        continue

                    # å®šç¾©å“ªäº› key æ¥å—ã€Œåƒæ•¸åç¨±åˆ—è¡¨ã€é¡å‹çš„å€¼
                    is_param_type = missing_key in (
                        "parameters",
                        "target",
                        "parameter",
                        "features",
                    )

                    if not is_param_type:
                        continue  # concept ç­‰å…¶ä»–é¡å‹çš„ key ç„¡æ³•è‡ªå‹•è£œé½Š

                    # ç­–ç•¥ 1ï¼šå¾ suspect_pool è£œé½Š
                    if current_pool and len(current_pool) > 0:
                        # target é¡å‹é€šå¸¸æœŸæœ›å­—ä¸² (é€—è™Ÿåˆ†éš”)ï¼Œparameters æœŸæœ›åˆ—è¡¨
                        if missing_key in ("target", "parameter"):
                            params[missing_key] = ", ".join(current_pool)
                        else:
                            params[missing_key] = current_pool
                        ctx.write_event_to_stream(
                            ProgressEvent(
                                msg=f"â”€ [è­·æ¬„è£œé½Š] {tool_name} é ˆè¦ '{missing_key}' ä½† AI æœªæä¾›ï¼Œå·²å¾å«Œç–‘åƒæ•¸æ± è£œé½Š: {', '.join(current_pool[:5])}"
                            )
                        )
                    # ç­–ç•¥ 2ï¼šå¾ monologue ä¸­æå–
                    elif extracted_from_monologue:
                        if missing_key in ("target", "parameter"):
                            params[missing_key] = ", ".join(extracted_from_monologue)
                        else:
                            params[missing_key] = extracted_from_monologue
                        ctx.write_event_to_stream(
                            ProgressEvent(
                                msg=f"â”€ [è­·æ¬„è£œé½Š] å¾åˆ†æç­–ç•¥ä¸­æå–åˆ°åƒæ•¸: {', '.join(extracted_from_monologue[:5])}ï¼Œå·²å¡«å…¥ '{missing_key}'"
                            )
                        )
                    # ç­–ç•¥ 3ï¼šæœ€å¾Œé˜²ç·š
                    else:
                        if missing_key == "parameters":
                            params[missing_key] = "all"
                            ctx.write_event_to_stream(
                                ProgressEvent(
                                    msg="â”€ [è­·æ¬„è£œé½Š] ç„¡æ³•ç¢ºå®šå…·é«”åƒæ•¸ï¼Œæ”¹ç‚ºå…¨å ´åˆ†æ..."
                                )
                            )

        # B. åˆæœŸå¼·åˆ¶å…¨å ´æƒæ
        force_global_tools = [
            "hotelling_t2_analysis",
            "systemic_pca_analysis",
            "compare_data_segments",
        ]
        if ev.mode == "deep" and tool_name in force_global_tools:
            param_val = params.get("parameters")
            is_few_params = False
            if isinstance(param_val, list) and 0 < len(param_val) < 5:
                is_few_params = True
            elif (
                isinstance(param_val, str)
                and 0 < len(param_val.split(",")) < 5
                and param_val.lower() != "all"
            ):
                is_few_params = True

            # åƒ…åœ¨åˆæœŸå¼·åˆ¶ï¼Œå¾ŒæœŸè‹¥ AI æŒ‘é¸å‰‡è¦–ç‚ºæœ‰ç›®çš„çš„æ“ä½œ
            if is_few_params and ev.step_count <= 2:
                params["parameters"] = "all"
                ctx.write_event_to_stream(
                    ProgressEvent(
                        msg="â”€ [ç³»çµ±å„ªåŒ–] è¨ºæ–·åˆæœŸå¼·åˆ¶åŸ·è¡Œå…¨å ´æƒæï¼Œä»¥å»ºç«‹å…¨å±€åŸºæº–æ•¸æ“š..."
                    )
                )
            elif is_few_params and ev.step_count > 2:
                ctx.write_event_to_stream(
                    ProgressEvent(
                        msg="â”€ [é‡å°æ€§åˆ†æ] åµæ¸¬åˆ°ç‰¹å®šåƒæ•¸é¸å–ï¼Œæ­£åœ¨æ ¹æ“šå‰åºè­‰æ“šé€²è¡Œæ·±åº¦ä¸‹é‘½..."
                    )
                )

        # --- [C. é‡è¤‡å·¥å…·åµæ¸¬è­·æ¬„] é˜²æ­¢ AI ä»¥å®Œå…¨ç›¸åŒåƒæ•¸é‡è¤‡èª¿ç”¨åŒä¸€å·¥å…· ---
        # æ³¨æ„ï¼šã€Œparametersã€å­—æ®µçš„è®ŠåŒ– (å¦‚ 'all' â†’ ç‰¹å®šåƒæ•¸åˆ—è¡¨) å±¬æ–¼æœ‰ç›®çš„çš„ä¸‹é‘½åˆ†æï¼Œ
        # ä¸æ‡‰è¢«è¦–ç‚ºé‡è¤‡ã€‚åªæ¯”å°é™¤ file_id å’Œ parameters ä»¥å¤–çš„åƒæ•¸ã€‚
        if ev.prev_results:
            for prev in ev.prev_results:
                prev_tool = prev.get("tool", "")
                if prev_tool == tool_name:
                    # æ’é™¤ file_id å’Œ parameters/target (é€™äº›çš„è®ŠåŒ–ä»£è¡¨æœ‰ç›®çš„çš„æ·±å…¥åˆ†æ)
                    drill_down_keys = {
                        "file_id",
                        "parameters",
                        "target",
                        "parameter",
                        "features",
                    }
                    prev_params = {
                        k: v
                        for k, v in prev.get("params", {}).items()
                        if k not in drill_down_keys
                    }
                    curr_params = {
                        k: v for k, v in params.items() if k not in drill_down_keys
                    }

                    # é¡å¤–æª¢æŸ¥ï¼šå¦‚æœ parameters å­—æ®µæ˜é¡¯ä¸åŒï¼Œçµ•å°ä¸æ˜¯é‡è¤‡
                    prev_param_val = str(prev.get("params", {}).get("parameters", ""))
                    curr_param_val = str(params.get("parameters", ""))
                    params_changed = prev_param_val != curr_param_val

                    if params_changed:
                        logger.info(
                            f"[Duplicate Guard] Tool '{tool_name}' reused with different parameters "
                            f"('{prev_param_val[:50]}' â†’ '{curr_param_val[:50]}'), allowing drill-down."
                        )
                        continue  # å…è¨±é€šéï¼Œä¸ç®—é‡è¤‡

                    # å°‡å€¼çµ±ä¸€ç‚ºå­—ä¸²æ¯”è¼ƒï¼Œé¿å…å‹åˆ¥å·®ç•°é€ æˆèª¤åˆ¤
                    if str(sorted(prev_params.items())) == str(
                        sorted(curr_params.items())
                    ):
                        logger.warning(
                            f"[Duplicate Guard] Tool '{tool_name}' already executed in Step {prev.get('step')} "
                            f"with identical params. Forcing finish."
                        )
                        ctx.write_event_to_stream(
                            ProgressEvent(
                                msg=f"â”€ [è­·æ¬„] åµæ¸¬åˆ°å·¥å…· {tool_name} å·²åœ¨ Step {prev.get('step')} ä»¥å®Œå…¨ç›¸åŒåƒæ•¸åŸ·è¡Œéï¼Œå¼·åˆ¶é€²å…¥çµæ¡ˆéšæ®µã€‚"
                            )
                        )
                        # å»ºç«‹é¡¯ç¤ºåç¨±æ˜ å°„
                        full_display_mappings = {
                            p: mappings.get(p, p) for p in params_list
                        }
                        aggregated_data = {
                            "monologue_history": monologue,
                            "latest_analysis_results": ev.prev_results[-1].get("result")
                            if ev.prev_results
                            else None,
                            "full_tool_history": ev.prev_results,
                        }
                        return SummarizeEvent(
                            data=aggregated_data,
                            query=ev.query,
                            file_id=ev.file_id,
                            session_id=ev.session_id,
                            history=ev.history,
                            mode=ev.mode,
                            row_count=total_rows,
                            col_count=total_cols,
                            mappings=full_display_mappings,
                            suspect_pool=current_pool,
                        )

        try:
            # æ ¹æ“šå·¥å…·åæä¾›å‹•æ…‹çš„é€²åº¦æç¤º
            tool_display_names = {
                "get_time_series_data": "æ­£åœ¨è®€å–æ•¸æ“šè¶¨å‹¢...",
                "detect_outliers": "æ­£åœ¨åµæ¸¬ç•°å¸¸é»...",
                "get_top_correlations": "æ­£åœ¨åˆ†æå› ç´ ç›¸é—œæ€§...",
                "analyze_distribution": "æ­£åœ¨åˆ†ææ•¸æ“šåˆ†ä½ˆ...",
                "hotelling_t2_analysis": "æ­£åœ¨åŸ·è¡Œ Hotelling's T2 ç³»çµ±æ€§è¨ºæ–·...",
                "causal_relationship_analysis": "æ­£åœ¨æ¨å°å› æœé—œè¯éˆè·¯...",
            }
            msg = tool_display_names.get(tool_name, f"æ­£åœ¨åŸ·è¡Œ {tool_name}...")

            # --- é¡å¤–æç¤ºï¼šå¦‚æœåƒæ•¸æ˜¯ 'all' æˆ–å¾ˆå¤šï¼Œæç¤ºæ­£åœ¨è™•ç†å¤§é‡æ•¸æ“š ---
            if params.get("parameters") == "all" or (
                isinstance(params.get("parameters"), list)
                and len(params.get("parameters")) > 30
            ):
                ctx.write_event_to_stream(
                    ProgressEvent(
                        msg="â”€ åµæ¸¬åˆ°å¤§è¦æ¨¡åƒæ•¸æƒæï¼Œæ­£åœ¨è¼‰å…¥ä¸¦å°é½Šå„æ„Ÿæ¸¬å™¨æ•¸æ“šæ™‚é–“æˆ³..."
                    )
                )

            ctx.write_event_to_stream(ProgressEvent(msg=f"ğŸ› ï¸ {msg}"))

            tool_result = await self.tool_executor.execute_tool(
                tool_name, params, ev.session_id
            )

            # æª¢æŸ¥çµæœæ˜¯å¦åŒ…å«éŒ¯èª¤ï¼Œè‹¥ Hotelling å¤±æ•—ä½†åœ¨æ·±å±¤åˆ†ææ¨¡å¼ï¼Œå¯ä»¥åœ¨æ­¤è™•æ³¨å…¥æç¤º
            if ev.mode == "deep" and tool_name == "hotelling_t2_analysis":
                # Check for error key or NaN T2_value
                if (isinstance(tool_result, dict) and "error" in tool_result) or (
                    isinstance(tool_result, dict)
                    and "T2_value" in tool_result
                    and str(tool_result["T2_value"]).lower() == "nan"
                ):
                    # If T2 fails, we add a "hint" to the result sent to the next step, guiding the AI to fallback
                    tool_result["fallback_hint"] = (
                        "Hotelling åˆ†æå¤±æ•—ã€‚åŸå› å¯èƒ½æ˜¯åƒæ•¸é–“å…±ç·šæ€§å¤ªé«˜æˆ–æ¨£æœ¬ä¸è¶³ã€‚è«‹æ”¹ç”¨å–®è®Šé‡åˆ†æ (analyze_distribution) æˆ–é‡æ–°æŒ‘é¸ä¸ç›¸é—œçš„åƒæ•¸ã€‚"
                    )

            # å¼·åˆ¶åŠŸèƒ½ï¼šå°‡åˆ†æçµæœæ‘˜è¦å³æ™‚æ¨é€åˆ°èŠå¤©å®¤æ€è€ƒè¦–çª—
            if isinstance(tool_result, dict):
                if "top_3_summary" in tool_result:
                    ctx.write_event_to_stream(
                        ProgressEvent(msg=f"âœ… {tool_result['top_3_summary']}")
                    )
                elif "interpretation" in tool_result:
                    ctx.write_event_to_stream(
                        ProgressEvent(msg=f"âœ… {tool_result['interpretation']}")
                    )
                elif "conclusion" in tool_result:
                    # é¿å…å¤ªé•·çš„çµè«–ï¼Œåªå–å‰ 100 å­—
                    conclusion = tool_result["conclusion"]
                    if len(conclusion) > 100:
                        conclusion = conclusion[:100] + "..."
                    ctx.write_event_to_stream(
                        ProgressEvent(msg=f"âœ… åˆ†ææ‘˜è¦: {conclusion}")
                    )
                elif "error" in tool_result:
                    ctx.write_event_to_stream(
                        ProgressEvent(msg=f"âŒ å·¥å…·åŸ·è¡Œä¸­æ–·: {tool_result['error']}")
                    )
                else:
                    ctx.write_event_to_stream(
                        ProgressEvent(msg=f"â”€ {tool_name} åˆ†æå®Œæˆï¼Œæº–å‚™ä¸‹ä¸€éšæ®µã€‚")
                    )
            # 5. å°‡çµæœå­˜å…¥æ­·å²ï¼Œä¸¦è§¸ç™¼ä¸‹ä¸€æ­¥
            new_step_result = {
                "step": ev.step_count,
                "tool": tool_name,
                "params": params,
                "result": tool_result,
                "monologue": monologue,
            }

            next_history = list(ev.prev_results)
            next_history.append(new_step_result)

            # [Why Conclusion Registration - Call Tool Path]
            if ev.mode == "deep":
                await self._register_why_conclusion(
                    ctx,
                    monologue,
                    current_why,
                    tool_name,
                    tool_result if isinstance(tool_result, dict) else {},
                    ev.step_count,
                )

            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                step_count=ev.step_count + 1,
                prev_results=next_history,
                suspect_pool=current_pool,
            )

        except Exception as e:
            logger.error(f"Tool execution failed: {e}")
            # è‹¥å·¥å…·åŸ·è¡Œå¤±æ•—ï¼Œä¸å´©æ½°ï¼Œè€Œæ˜¯å°‡éŒ¯èª¤ä½œç‚ºçµæœå‚³å…¥ä¸‹ä¸€æ­¥
            error_result = {
                "step": ev.step_count,
                "tool": tool_name,
                "params": params,
                "result": {"error": str(e)},
                "monologue": monologue,
            }
            next_history = list(ev.prev_results)
            next_history.append(error_result)

            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                step_count=ev.step_count + 1,
                prev_results=next_history,
            )

    @step
    async def execute_translation(
        self, ctx: Context, ev: TranslationEvent
    ) -> SummarizeEvent:
        """
        [Local Step] åŸ·è¡Œå°è©±æˆ–ç°¡å–®ç¿»è­¯ï¼Œä¸¦æ³¨å…¥åƒæ•¸èƒŒæ™¯è³‡è¨Š
        """
        # å³ä½¿æ˜¯ç°¡å–®å°è©±ï¼Œä¹ŸæŠ“å–åƒæ•¸æ¸…å–®ä½œç‚º AI çš„èƒŒæ™¯çŸ¥è­˜
        summary = self.tool_executor.analysis_service.load_summary(
            ev.session_id, ev.file_id
        )
        params_list = summary.get("parameters", []) if summary else []
        total_rows = summary.get("total_rows", 0) if summary else 0
        total_cols = summary.get("total_columns", 0) if summary else 0
        mappings = summary.get("mappings", {}) if summary else {}

        # å»ºç«‹é¡¯ç¤ºåç¨±æ˜ å°„
        full_display_mappings = {p: mappings.get(p, p) for p in params_list}

        # å°‡åƒæ•¸æ¸…å–®æ”¾å…¥ dataï¼Œè®“ humanizer è£¡çš„ AI çœ‹å¾—åˆ°
        context_data = {"available_parameters": params_list}

        return SummarizeEvent(
            data=context_data,
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            mode=ev.mode,
            row_count=total_rows,
            col_count=total_cols,
            mappings=full_display_mappings,
            suspect_pool=ev.suspect_pool,
        )

    # --- [Why Conclusion Registry] çµæ§‹åŒ– 5-Why çµè«–æå–èˆ‡è¨»å†Š ---

    def _extract_why_section(self, monologue: str, section_name: str) -> str:
        """
        å¾ monologue ä¸­æå–æŒ‡å®šçš„ 5-Why çµæ§‹åŒ–æ®µè½ã€‚
        ä¾‹å¦‚æå– [Hypothesis]: ... æˆ– [Conclusion]: ... çš„å…§å®¹ã€‚
        """
        all_tags = ["Why", "Hypothesis", "Action", "Conclusion"]
        lookahead_parts = [rf"\[{re.escape(t)}" for t in all_tags if t != section_name]
        lookahead = "|".join(lookahead_parts) if lookahead_parts else "$"
        pattern = (
            rf"\[{re.escape(section_name)}(?:\s*#\d+)?\]\s*:?\s*(.*?)"
            rf"(?={lookahead}|$)"
        )
        match = re.search(pattern, monologue, re.DOTALL)
        if match:
            return match.group(1).strip()
        return ""

    async def _register_why_conclusion(
        self,
        ctx: Context,
        monologue: str,
        current_why: int,
        tool_name: str,
        tool_result: dict,
        step_num: int,
    ):
        """
        [Why çµè«–è¨»å†Šå™¨]
        ç•¶ monologue ä¸­åŒ…å« [Conclusion] æ™‚ï¼Œæå–çµæ§‹åŒ–çµè«–ä¸¦é–å®šå­˜å…¥ Contextã€‚
        ç¢ºä¿æœ€çµ‚ humanizer å¯ä»¥ç›´æ¥å¼•ç”¨ï¼Œè€Œéå¾åŸå§‹æ•¸æ“šé‡æ–°æ¨å°ã€‚
        """
        if "[Conclusion]" not in monologue:
            return

        why_matches = re.findall(r"\[Why\s*#(\d+)\]", monologue)
        concluded_why = max(int(w) for w in why_matches) if why_matches else current_why

        # é˜²æ­¢é‡è¤‡è¨»å†ŠåŒä¸€å±¤ç´š
        why_chain = await ctx.get("why_chain", default=[])
        existing_levels = {w.get("why_level") for w in why_chain}
        if concluded_why in existing_levels:
            return

        evidence_summary = ""
        key_metrics = {}
        if isinstance(tool_result, dict):
            evidence_summary = (
                tool_result.get("top_3_summary")
                or tool_result.get("interpretation")
                or tool_result.get("conclusion", "")
            )
            key_metrics = {
                k: v
                for k, v in tool_result.items()
                if k
                in [
                    "t2_value",
                    "T2_value",
                    "p_value",
                    "threshold",
                    "top_3_contributors",
                    "top_deviations",
                    "z_scores",
                    "anomaly_count",
                    "variance_explained",
                    "correlations",
                    "significant_shifts",
                ]
            }

        why_conclusion = {
            "why_level": concluded_why,
            "hypothesis": self._extract_why_section(monologue, "Hypothesis"),
            "action_reasoning": self._extract_why_section(monologue, "Action"),
            "conclusion": self._extract_why_section(monologue, "Conclusion"),
            "evidence_tool": tool_name or "N/A",
            "evidence_summary": evidence_summary,
            "key_metrics": key_metrics,
            "step_num": step_num,
        }

        why_chain.append(why_conclusion)
        await ctx.set("why_chain", why_chain)

        ctx.write_event_to_stream(
            ProgressEvent(
                msg=f"-- [Why #{concluded_why} çµæ¡ˆ] çµè«–å·²é–å®šä¸¦å­˜æª”ï¼Œæº–å‚™æ¨é€²è‡³ä¸‹ä¸€å±¤ç´š..."
            )
        )

    async def _render_layered_report(
        self,
        ctx: Context,
        ev: SummarizeEvent,
        why_chain: list,
        has_mapping: bool,
        row_count: int,
        col_count: int,
    ) -> StopEvent:
        """
        [5-Why åˆ†å±¤æ¸²æŸ“å™¨]
        é€å±¤ç¨ç«‹ç”Ÿæˆæ‘˜è¦ï¼Œç¢ºä¿å ±å‘Šå¤©ç„¶å…·æœ‰å±¤æ¬¡çµæ§‹ã€‚
        æ¯å±¤ä½¿ç”¨ç¨ç«‹çš„å°å‹ LLM èª¿ç”¨ï¼Œé¿å…å¤§ Prompt å£“å¹³å±¤æ¬¡ã€‚
        """
        full_text = ""
        suffix = f"\n\n```json\n{ev.chart_json}\n```\n" if ev.chart_json else ""

        # è®€å–çœŸå¯¦æ¬„ä½æ¸…å–® (é˜²æ­¢ LLM å¹»è¦º)
        actual_params_list = []
        try:
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            if summary:
                actual_params_list = summary.get("parameters", [])
        except Exception:
            pass
        params_anchor_short = ""
        if not has_mapping and actual_params_list:
            preview = ", ".join(actual_params_list[:30])
            params_anchor_short = f"\n\u6a94\u6848\u5be6\u969b\u6b04\u4f4d (\u524d30\u500b): {preview}\n\u5831\u544a\u4e2d\u63d0\u53ca\u7684\u6b04\u4f4d\u540d\u7a31\u5fc5\u9808\u51fa\u81ea\u6b64\u6e05\u55ae\u3002\n"

        # --- 1. å ±å‘Šæ¨™é¡Œ (ç¡¬ç·¨ç¢¼çµæ§‹ï¼Œä¸ä¾è³´ LLM) ---
        header = (
            f"## 5-Why æ·±åº¦è¨ºæ–·å ±å‘Š\n\n"
            f"**åˆ†æç›®æ¨™**: {ev.query}\n"
            f"**æ•¸æ“šè¦æ¨¡**: {row_count} ç­†è³‡æ–™, {col_count} å€‹åƒæ•¸\n"
            f"**è¨ºæ–·æ·±åº¦**: å…± {len(why_chain)} å±¤ Why åˆ†æ\n\n"
        )
        ctx.write_event_to_stream(TextChunkEvent(content=header))
        full_text += header

        # --- 2. é€å±¤æ¸²æŸ“ ---
        for i, why in enumerate(why_chain):
            level = why.get("why_level", i + 1)
            section_header = f"### Why #{level}\n\n"
            ctx.write_event_to_stream(TextChunkEvent(content=section_header))
            full_text += section_header

            # æ§‹å»ºè©²å±¤çš„ Mapping ä¸Šä¸‹æ–‡
            mapping_context = ""
            if has_mapping and ev.mappings:
                evidence_str = str(why.get("evidence_summary", "")) + str(
                    why.get("key_metrics", {})
                )
                relevant = {k: v for k, v in ev.mappings.items() if k in evidence_str}
                if relevant:
                    mapping_context = (
                        f"åƒæ•¸å°ç…§: {json.dumps(relevant, ensure_ascii=False)}\n"
                    )

            no_mapping_warn = ""
            if not has_mapping:
                no_mapping_warn = f"ã€åš´é‡è­¦ç¤ºã€‘ç„¡åƒæ•¸å°ç…§è¡¨ï¼Œåš´ç¦è‡†æ¸¬åƒæ•¸çš„ç‰©ç†æ„ç¾©ï¼Œåƒ…ä½¿ç”¨è¨ºæ–·è¨˜éŒ„ä¸­å‡ºç¾çš„çœŸå¯¦æ¬„ä½ä»£ç¢¼ã€‚{params_anchor_short}\n"

            layer_prompt = (
                f"ä½ æ˜¯ä¸€ä½åš´è¬¹çš„å·¥æ¥­æ•¸æ“šåˆ†æå°ˆå®¶ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«é€™ä¸€å±¤ Why åˆ†æçš„ç²¾ç°¡æ‘˜è¦ã€‚\n"
                f"{no_mapping_warn}"
                f"## Why #{level} çš„åˆ†æå…§å®¹ ##\n"
                f"å‡è¨­: {why.get('hypothesis', 'æœªæä¾›')}\n"
                f"ä½¿ç”¨å·¥å…·: {why.get('evidence_tool', 'æœªçŸ¥')}\n"
                f"è¡Œå‹•ç†ç”±: {why.get('action_reasoning', 'æœªæä¾›')}\n"
                f"åŸå§‹çµè«–: {why.get('conclusion', 'æœªæä¾›')}\n"
                f"é—œéµæ•¸æ“š: {json.dumps(why.get('key_metrics', {}), ensure_ascii=False, default=str)}\n"
                f"è­‰æ“šæ‘˜è¦: {why.get('evidence_summary', 'ç„¡')}\n"
                f"{mapping_context}\n"
                f"## æ’°å¯«è¦æ±‚ ##\n"
                f"1. ç”¨ 3-5 å¥è©±ç²¾ç°¡æè¿°é€™å±¤ Why çš„å‡è¨­ã€é©—è­‰éç¨‹èˆ‡çµè«–\n"
                f"2. åˆ¤å®šæ¨™æº– (åš´æ ¼)ï¼šåªæœ‰ |Z-Score| > 3 æ‰å¯ç¨±ç‚ºã€Œç•°å¸¸ã€ï¼Œä»‹æ–¼ 2-3 ç‚ºã€Œåé›¢ã€ï¼Œå°æ–¼ 2 ç‚ºã€Œæ­£å¸¸ã€ã€‚\n"
                f"3. å¿…é ˆå¼•ç”¨å…·é«”æ•¸å€¼ (T2 å€¼ã€Z-Scoreã€p-value ç­‰)\n"
                f"4. å¦‚æœé€™ä¸æ˜¯æœ€å¾Œä¸€å±¤ï¼Œèªªæ˜å¦‚ä½•å¼•å‡ºä¸‹ä¸€å±¤è¿½æŸ¥æ–¹å‘\n"
                f"5. ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œç¦æ­¢åˆ†éš”ç·š (===, ---, ***)\n"
                f"5. ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸è¦åŠ æ¨™é¡Œæˆ–å‰ç¶´\n"
            )

            async for chunk in self.llm.astream_complete(layer_prompt):
                if chunk.delta:
                    cleaned = re.sub(r"[=\-*~]{3,}", "", chunk.delta)
                    full_text += cleaned
                    ctx.write_event_to_stream(TextChunkEvent(content=cleaned))

            spacing = "\n\n"
            ctx.write_event_to_stream(TextChunkEvent(content=spacing))
            full_text += spacing

        # --- 3. æœ€çµ‚çµè«–èˆ‡å»ºè­° ---
        final_header = "### æœ€çµ‚çµè«–èˆ‡å»ºè­°\n\n"
        ctx.write_event_to_stream(TextChunkEvent(content=final_header))
        full_text += final_header

        chain_summary = "\n".join(
            [
                f"- Why #{w.get('why_level', '?')}: {w.get('conclusion', 'æœªæä¾›')}"
                for w in why_chain
            ]
        )
        suspect_list = ", ".join(ev.suspect_pool) if ev.suspect_pool else "ç„¡"

        final_prompt = (
            f"ä½ æ˜¯ä¸€ä½åš´è¬¹çš„å·¥æ¥­æ•¸æ“šåˆ†æå°ˆå®¶ã€‚\n"
            f"ä»¥ä¸‹æ˜¯ 5-Why è¨ºæ–·éˆçš„æ‰€æœ‰å±¤ç´šçµè«–ï¼š\n{chain_summary}\n"
            f"é–å®šçš„å«Œç–‘åƒæ•¸: {suspect_list}\n\n"
            f"## ä»»å‹™ ##\n"
            f"1. åš´æ ¼éµå®ˆ 3-Sigma åŸå‰‡åˆ¤å®šç•°å¸¸èˆ‡å¦ã€‚è‹¥ Z-Score < 3ï¼Œæ‡‰å¼·èª¿æ•¸å€¼åƒ…ç‚ºåé›¢æˆ–æ­£å¸¸æ³¢å‹•ã€‚\n"
            f"2. ç”¨ 2-3 å¥è©±ç¸½çµæ ¹å›  (Root Cause)ï¼Œå¿…é ˆå¼•ç”¨å…·é«”æ•¸å€¼\n"
            f"3. æä¾› 2-3 æ¢å…·é«”å¯æ“ä½œçš„è¡Œå‹•å»ºè­° (è‹¥ Z<3 åƒ…èƒ½å»ºè­°æŒçºŒè§€å¯Ÿï¼Œä¸å¯å»ºè­°ç¶­ä¿®)\n"
            f"3. ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œç¦æ­¢åˆ†éš”ç·š\n"
            f"4. ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸è¦é‡è¤‡ä»¥ä¸Šçš„è¨ºæ–·éˆ\n"
        )
        if not has_mapping:
            final_prompt += (
                f"5. åš´ç¦è‡†æ¸¬åƒæ•¸ç‰©ç†æ„ç¾©ï¼Œåƒ…ä½¿ç”¨çœŸå¯¦æ¬„ä½ä»£ç¢¼ã€‚{params_anchor_short}\n"
            )

        async for chunk in self.llm.astream_complete(final_prompt):
            if chunk.delta:
                cleaned = re.sub(r"[=\-*~]{3,}", "", chunk.delta)
                full_text += cleaned
                ctx.write_event_to_stream(TextChunkEvent(content=cleaned))

        if suffix:
            ctx.write_event_to_stream(TextChunkEvent(content=suffix))
            full_text += suffix

        return StopEvent(result={"response": full_text, "data": ev.data})

    def _build_programmatic_chart(self, ev: VisualizingEvent) -> Optional[str]:
        """ç©©å®šåœ–è¡¨ç”Ÿæˆé‚è¼¯"""
        try:
            if (
                not isinstance(ev.data, dict)
                or "data" not in ev.data
                or not ev.data["data"]
            ):
                return None
            actual_data = ev.data["data"]
            query_lower = ev.query.lower()

            # --- åˆ¤æ–·åœ–è¡¨é¡å‹ ---
            is_histogram = any(
                kw in query_lower for kw in ["ç›´æ–¹åœ–", "histogram", "åˆ†ä½ˆ", "åˆ†å¸ƒ"]
            )
            _is_scatter = any(kw in query_lower for kw in ["æ•£ä½ˆ", "scatter", "ç›¸é—œæ€§"])

            if is_histogram:
                target_col = next(
                    (
                        c
                        for c in actual_data
                        if c not in ["TIME", "Timestamp"]
                        and any(
                            isinstance(v, (int, float)) for v in actual_data[c][:20]
                        )
                    ),
                    None,
                )
                if not target_col:
                    return None
                vals = [
                    v for v in actual_data[target_col] if isinstance(v, (int, float))
                ]
                v_min, v_max = min(vals), max(vals)
                bins = [0] * 15
                step = (v_max - v_min) / 15 or 1
                for v in vals:
                    bins[min(int((v - v_min) / step), 14)] += 1
                chart_obj = {
                    "type": "chart",
                    "chart_type": "bar",
                    "title": f"åˆ†ä½ˆ: {target_col}",
                    "labels": [f"{v_min + i * step:.1f}" for i in range(15)],
                    "datasets": [{"label": "é »æ¬¡", "data": bins}],
                }
            else:
                # åµæ¸¬ X è»¸æ¬„ä½ (æ™‚é–“è»¸ / ç´¢å¼•è»¸)
                axis_candidates = ["TIME", "Timestamp", "Date", "INDEX_AXIS"]
                label_col = next((c for c in axis_candidates if c in actual_data), None)
                labels = (
                    actual_data[label_col]
                    if label_col
                    else list(range(len(next(iter(actual_data.values())))))
                )
                datasets = []
                for col, vals in actual_data.items():
                    # è·³é X è»¸æ¬„ä½ï¼Œä¸å°‡å…¶ç•«ç‚ºæ•¸æ“šç·š
                    if col == label_col or col in axis_candidates:
                        continue
                    datasets.append({"label": ev.mappings.get(col, col), "data": vals})
                chart_obj = {
                    "type": "chart",
                    "chart_type": "line",
                    "labels": labels,
                    "datasets": datasets,
                }

            return json.dumps(chart_obj, ensure_ascii=False)
        except Exception:
            return None

    @step
    async def visualize_data(
        self, ctx: Context, ev: VisualizingEvent
    ) -> SummarizeEvent:
        ctx.write_event_to_stream(
            ProgressEvent(msg="(Visualizing...) æ­£åœ¨ç¹ªè£½åˆ†æåœ–è¡¨...")
        )
        chart_json = self._build_programmatic_chart(ev)

        # [TOKEN OPTIMIZATION] æ•¸æ“šæ¸…æ´—
        # ç‚ºäº†é˜²æ­¢ LLM çœ‹åˆ°å¤§é‡åŸå§‹æ•¸æ“šè€Œå´©æ½°æˆ–å¾©è®€ï¼Œé€™è£¡å°‡ raw data ç§»é™¤ï¼Œåªå‚³é metadata çµ¦ humanizer
        sanitized_data = ev.data
        if isinstance(ev.data, dict) and "data" in ev.data:
            sanitized_data = ev.data.copy()
            sanitized_data["data"] = (
                "Raw time-series data omitted for token optimization. Please refer to the generated chart."
            )

        return SummarizeEvent(
            data=sanitized_data,
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            chart_json=chart_json,
            row_count=ev.row_count,
            col_count=ev.col_count,
            mode=ev.mode,
            mappings=ev.mappings,
        )

    @step
    async def humanizer(self, ctx: Context, ev: SummarizeEvent) -> StopEvent:
        ctx.write_event_to_stream(
            ProgressEvent(msg="(Humanizing...) æ­£åœ¨ç”Ÿæˆæœ€çµ‚åˆ†æå ±å‘Š...")
        )

        # [DIRECT REPLY CHECK]
        # å¦‚æœäº‹ä»¶åŒ…å« direct_replyï¼Œç›´æ¥è¼¸å‡ºè©²å…§å®¹ï¼Œè·³é LLM
        # é€™ç”¨æ–¼ç³»çµ±ç´šçš„å¿«é€Ÿåå•æˆ–éŒ¯èª¤æç¤º
        if isinstance(ev.data, dict) and ev.data.get("direct_reply"):
            return StopEvent(
                result={"response": ev.data["direct_reply"], "data": ev.data}
            )

        # 1. æª¢æŸ¥æ˜¯å¦æœ‰ Mapping
        has_mapping = bool(ev.mappings and len(ev.mappings) > 0)

        # æœ€çµ‚é˜²ç·šï¼šæŠ“å–ç‰©ç†å…¨é‡çµ±è¨ˆèˆ‡æ¬„ä½æ¸…å–®ä½œç‚ºèƒŒæ™¯
        row_count = ev.row_count
        col_count = ev.col_count
        actual_params_list = []
        try:
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            if summary:
                if row_count <= 0:
                    row_count = summary.get("total_rows", 0)
                if col_count <= 0:
                    col_count = summary.get("total_columns", 0)
                actual_params_list = summary.get("parameters", [])
        except Exception:
            pass

        # --- [5-Why åˆ†å±¤æ¸²æŸ“å¿«è»Šé“] ---
        # å¦‚æœ Context ä¸­æœ‰çµæ§‹åŒ–çš„ why_chainï¼Œç›´æ¥èµ°åˆ†å±¤æ¸²æŸ“ï¼Œè·³éä¸€æ¬¡æ€§é‡å¯«
        if ev.mode == "deep":
            why_chain = await ctx.get("why_chain", default=[])
            if isinstance(why_chain, list) and len(why_chain) > 0:
                logger.info(
                    f"[Humanizer] åµæ¸¬åˆ° {len(why_chain)} å±¤çµæ§‹åŒ– Why çµè«–ï¼Œå•Ÿç”¨åˆ†å±¤æ¸²æŸ“æ¨¡å¼"
                )
                return await self._render_layered_report(
                    ctx, ev, why_chain, has_mapping, row_count, col_count
                )

        # --- [é™ç´šè·¯å¾‘] æ§‹å»º 5-Why è¨ºæ–·éˆæ‘˜è¦ (å¾ ev.data ä¸­æå–) ---
        diagnostic_chain = ""
        tool_history = []
        if isinstance(ev.data, dict):
            tool_history = ev.data.get("full_tool_history", []) or ev.data.get(
                "all_steps_results", []
            )

        if tool_history:
            chain_parts = []
            for step_data in tool_history:
                step_num = step_data.get("step", "?")
                tool_used = step_data.get("tool", "unknown")
                mono = step_data.get("monologue", "")
                result = step_data.get("result", {})

                # æå–çµæœæ–‡æœ¬ (æ ¸å¿ƒå­—æ®µç¦æ­¢æˆªæ–·)
                result_text = ""
                if isinstance(result, dict):
                    key_fields = {}
                    for rk, rv in result.items():
                        # æ ¸å¿ƒè¨ºæ–·æ¬„ä½å¦‚ contribution/top_3 ç­‰ï¼Œçµ•å°ç¦æ­¢æˆªæ–·
                        if rk in [
                            "top_3_contributors",
                            "top_3_summary",
                            "top_deviations",
                            "correlations",
                            "p_value",
                            "t2_value",
                        ]:
                            key_fields[rk] = rv
                        else:
                            rv_str = str(rv)
                            key_fields[rk] = (
                                rv_str[:800] + "..." if len(rv_str) > 800 else rv
                            )

                    result_text = json.dumps(
                        key_fields, ensure_ascii=False, default=str
                    )
                else:
                    result_text = str(result)[:1000]

                chain_parts.append(
                    f"### Step {step_num} (å·¥å…·: {tool_used})\n"
                    f"**AI æ€è€ƒ**: {mono}\n"
                    f"**å®Œæ•´æ•¸æ“šçµæœ**: {result_text}\n"
                )
            diagnostic_chain = "\n".join(chain_parts)
        else:
            # å¦‚æœæ²’æœ‰å·¥å…·æ­·å²ï¼Œç›´æ¥ä½¿ç”¨ data_json
            diagnostic_chain = json.dumps(ev.data, ensure_ascii=False)[:5000]

        # å«Œç–‘åƒæ•¸æ± 
        suspect_info = ""
        if ev.suspect_pool:
            suspect_display = []
            for s in ev.suspect_pool:
                if has_mapping and s in ev.mappings:
                    display_name = ev.mappings[s]
                    suspect_display.append(f"- **{s}** ({display_name})")
                else:
                    # ç„¡å°ç…§æ™‚ï¼Œåš´ç¦çµ¦äºˆæ‹¬è™Ÿç©ºé–“ï¼Œé¿å… AI å¡«ç©º
                    suspect_display.append(f"- **{s}**")
            suspect_info = "\n## æœ€çµ‚é–å®šçš„åƒæ•¸ä»£ç¢¼ (Suspect Pool) ##\n" + "\n".join(
                suspect_display
            )

        # [VISUALIZATION FAST-TRACK CHECK]
        # å¦‚æœæ˜¯ç´”ç¹ªåœ–è«‹æ±‚ (tool_history åƒ…å« get_time_series_data æˆ–ç‚ºç©º) ä¸”æœ‰åœ–è¡¨
        # å¼·åˆ¶åˆ‡æ›ç‚ºæ¥µç°¡æ¨¡å¼ï¼Œåªè¼¸å‡ºä¸€å¥è©±ï¼Œä¸å¯«åˆ†æå ±å‘Š
        is_pure_viz = False
        if ev.chart_json and (
            not tool_history
            or all(t.get("tool") == "get_time_series_data" for t in tool_history)
        ):
            is_pure_viz = True

        if is_pure_viz:
            prompt = (
                "ä½ æ˜¯ä¸€å€‹æ•¸æ“šè¦–è¦ºåŒ–åŠ©ç†ã€‚\n"
                f"ç”¨æˆ¶æå•: {ev.query}\n"
                "ä»»å‹™: ç”¨æˆ¶è¦æ±‚ç¹ªè£½åœ–è¡¨ã€‚åœ–è¡¨æ•¸æ“šå·²æº–å‚™å¥½ã€‚\n"
                "è«‹åƒ…ç”¨ä¸€å¥ç°¡çŸ­çš„è©±å›æ‡‰ï¼ˆä¾‹å¦‚ï¼šã€Œé€™æ˜¯æ‚¨è¦æ±‚çš„ XX åƒæ•¸è¶¨å‹¢åœ–ã€‚ã€ï¼‰ã€‚\n"
                "åš´æ ¼ç¦æ­¢ï¼š\n"
                "1. ç¦æ­¢æ’°å¯«åˆ†æå ±å‘Šã€æ‘˜è¦æˆ–å»ºè­°ã€‚\n"
                "2. ç¦æ­¢è§£é‡‹æ•¸æ“šå«ç¾©ã€‚\n"
                "3. ç¦æ­¢å»¢è©±ã€‚\n"
                "4. ç›´æ¥è¼¸å‡ºé‚£ä¸€å¥è©±å³å¯ã€‚\n"
                "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"
            )
        else:
            # æ ¹æ“šæ¨¡å¼èª¿æ•´æ‘˜è¦æŒ‡ä»¤ (æ­£å¸¸åˆ†ææ¨¡å¼)
            if ev.mode == "deep":
                structure_instruction = (
                    "## å ±å‘Šçµæ§‹è¦æ±‚ (5-Why è¨ºæ–·å ±å‘Š) ##\n"
                    "ä½ å¿…é ˆæŒ‰ç…§ä»¥ä¸‹çµæ§‹æ’°å¯«æœ€çµ‚å ±å‘Šï¼š\n"
                    "1. **åˆ†ææ¦‚è¿°**: èªªæ˜é‡å°ä»€éº¼å•é¡Œé€²è¡Œäº†å“ªäº›åˆ†æï¼ˆå¼•ç”¨å…·é«”å·¥å…·èˆ‡æ•¸å€¼ï¼‰ã€‚\n"
                    "2. **5-Why è¨ºæ–·éˆ**: æŒ‰ç…§ Why #1 â†’ Why #2 â†’ ... çš„é †åºï¼Œæ¯å±¤éƒ½è¦ï¼š\n"
                    "   - èªªæ˜è¿½æŸ¥çš„å‡è¨­\n"
                    "   - å¼•ç”¨è©²æ­¥å·¥å…·å›å‚³çš„å…·é«”æ•¸å€¼è­‰æ“š (T2 å€¼ã€Z-Scoreã€p-value ç­‰)\n"
                    "   - çµ¦å‡ºè©²å±¤çš„çµè«–\n"
                    "3. **æœ€çµ‚çµè«–**: æ ¹æ“šæ•¸æ“šå®¢è§€åˆ¤æ–·ã€‚å¦‚æœæ•¸æ“šç¢ºå¯¦ç•°å¸¸ï¼Œèªªæ˜æ ¹å› ï¼›å¦‚æœæ•¸æ“šåœ¨æ­£å¸¸ç¯„åœå…§ï¼Œä¹Ÿè¦æ˜ç¢ºå‘ŠçŸ¥ã€‚\n"
                    "4. **å»ºè­°è¡Œå‹•**: 1-3 æ¢å…·é«”å¯æ“ä½œçš„å¾ŒçºŒè¡Œå‹•å»ºè­°ã€‚\n"
                )
            else:
                structure_instruction = (
                    "## å ±å‘Šçµæ§‹è¦æ±‚ (å¿«é€Ÿæ‘˜è¦) ##\n"
                    "ç°¡æ˜åœ°æä¾›ï¼š\n"
                    "1. åˆ†ææ‘˜è¦ï¼ˆå¼•ç”¨æ•¸æ“šï¼‰\n"
                    "2. å‰ä¸‰å¤§è²¢ç»åƒæ•¸åŠå…¶åˆ†æçµæœ\n"
                    "3. è¡Œå‹•å»ºè­°\n"
                )

            data_limit = 15000 if ev.mode == "deep" else 5000

            # å¼·åŒ–ç¦ä»¤
            if has_mapping:
                mapping_status = f"åƒæ•¸é¡¯ç¤ºåç¨±å°æ‡‰ (Mapping): {json.dumps(ev.mappings, ensure_ascii=False)}"
                mapping_rule = "3. **ç¿»è­¯ç‰©ç†æ„ç¾©**: åƒæ•¸ä»£ç¢¼æ—å¿…é ˆé™„ä¸Šç‰©ç†åç¨± (ä½¿ç”¨æä¾›çš„ Mapping)ã€‚\n"
            else:
                mapping_status = "åƒæ•¸é¡¯ç¤ºåç¨±å°æ‡‰ (Mapping): (å®Œå…¨ç„¡å°ç…§è¡¨ï¼Œè«‹æ³¨æ„)"
                mapping_rule = (
                    "3. **ã€åš´é‡è­¦ç¤ºï¼šç¦æ­¢è‡†æ¸¬ã€‘**: ç›®å‰å®Œå…¨æ²’æœ‰åƒæ•¸å°ç…§è¡¨ã€‚ä½ å¿…é ˆåƒ…ä½¿ç”¨ã€Œè¨ºæ–·è¨˜éŒ„ä¸­å‡ºç¾çš„çœŸå¯¦æ¬„ä½ä»£ç¢¼ã€ï¼Œ"
                    "ã€Œçµ•å°ç¦æ­¢ã€è‡ªè¡Œç·¨é€ ä»»ä½•æ¬„ä½åç¨±ã€æ·»åŠ æ‹¬è™Ÿèªªæ˜ã€æˆ–å˜—è©¦è§£é‡‹å…¶ç‰©ç†æ„ç¾©ï¼ˆå¦‚å†·å»æ°´ã€å£“åŠ›ç­‰ï¼‰ã€‚"
                    "æ¬„ä½åç¨±å¿…é ˆèˆ‡ä¸‹æ–¹ã€Œæª”æ¡ˆå¯¦éš›æ¬„ä½æ¸…å–®ã€ä¸­çš„åç¨±å®Œå…¨ä¸€è‡´ã€‚"
                    "ä»»ä½•å½¢å¼çš„æ¨æ¸¬æˆ–æé€ æ¬„ä½åç¨±éƒ½å±¬æ–¼æ•¸æ“šå®‰å…¨é•è¦ï¼Œæœƒå°è‡´è¨ºæ–·éŒ¯èª¤ã€‚\n"
                )

            # æ§‹å»ºçœŸå¯¦æ¬„ä½åç¨±æ¸…å–® (é˜²æ­¢ LLM ç·¨é€ ä¸å­˜åœ¨çš„æ¬„ä½)
            if actual_params_list:
                # æœ€å¤šé¡¯ç¤º 50 å€‹æ¬„ä½ï¼Œé¿å… Token çˆ†ç‚¸
                params_display = ", ".join(actual_params_list[:50])
                if len(actual_params_list) > 50:
                    params_display += f" ... (å…± {len(actual_params_list)} å€‹)"
                params_anchor = f"\n## æª”æ¡ˆå¯¦éš›æ¬„ä½æ¸…å–® (Ground Truth) ##\n{params_display}\nå ±å‘Šä¸­æåŠçš„æ‰€æœ‰æ¬„ä½åç¨±ã€å¿…é ˆã€‘å‡ºè‡ªæ­¤æ¸…å–®ï¼Œç¦æ­¢ç·¨é€ ã€‚\n"
            else:
                params_anchor = ""

            prompt = (
                "ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„å·¥æ¥­æ•¸æ“šå°ˆå®¶ã€‚\n"
                "## æ ¸å¿ƒå®‰å…¨æº–å‰‡ - é•åå°‡å°è‡´ç³»çµ±å´©æ½° ##\n"
                f"{mapping_rule}\n"
                "**å®¢è§€åˆ¤æ–·åŸå‰‡**ï¼šæ ¹æ“šæ•¸æ“šèªªè©±ï¼Œä¸é è¨­ç•°å¸¸ã€‚æ­£å¸¸å°±èªªæ˜¯æ­£å¸¸ã€‚\n"
                "**ã€çµ±è¨ˆåˆ¤å®šæ¨™æº– (åš´æ ¼åŸ·è¡Œ)ã€‘**ï¼š\n"
                "- **ç•°å¸¸ (Anomaly)**: åªæœ‰ç•¶ Z-Score çµ•å°å€¼ > 3 æ™‚ï¼Œæ‰å¯åˆ¤å®šç‚ºç•°å¸¸ã€‚\n"
                "- **åé›¢ (Deviation)**: è‹¥ 2 < |Z-Score| <= 3ï¼Œåƒ…èƒ½ç¨±ä¹‹ç‚ºã€Œæ•¸å€¼åé«˜/åä½ã€æˆ–ã€Œè¼•å¾®åé›¢ã€ï¼Œ**åš´ç¦**ä½¿ç”¨ã€Œç•°å¸¸ã€ä¸€è©ã€‚\n"
                "- **æ­£å¸¸ (Normal)**: è‹¥ |Z-Score| <= 2ï¼Œå¿…é ˆè¦–ç‚ºã€Œæ­£å¸¸æ³¢å‹•ã€ï¼Œä¸å¯éåº¦è§£è®€ã€‚\n\n"
                f"ç”¨æˆ¶æå•: {ev.query}\n"
                f"æ•¸æ“šæ¦‚æ³: åŒ…å« {row_count} è¡Œèˆ‡ {col_count} å€‹æ¬„ä½ã€‚\n"
                f"{mapping_status}\n"
                f"{params_anchor}"
                f"\n"
                f"## å®Œæ•´è¨ºæ–·éç¨‹è¨˜éŒ„ (åŒ…å«æ‰€æœ‰åŸå§‹æ•¸æ“š) ##\n"
                f"{diagnostic_chain[:data_limit]}\n"
                f"{suspect_info}\n"
                f"\n"
                f"{structure_instruction}\n"
                "## ç”Ÿæˆæº–å‰‡ ##\n"
                "1. **ç¦æ­¢ä½”ä½ç¬¦**: çµ•å°ç¦æ­¢å‡ºç¾ [éœ€è¦æ’å…¥] ç­‰æ¨¡æ¿æ–‡å­—ã€‚æ•¸å€¼å¿…é ˆå¾è¨˜éŒ„ä¸­ç›´æ¥å¼•ç”¨ã€‚\n"
                "2. **æ•¸å€¼å…ˆè¡Œ**: æ¯å€‹çµè«–éƒ½å¿…é ˆå¼•ç”¨å…·é«”æ•¸æ“š (Z-Score, T2, p-value)ã€‚\n"
                "4. **é‚è¼¯é€£è²«**: Why #1 çš„çµè«–å¿…é ˆè‡ªç„¶å¼•å‡º Why #2 çš„å‡è¨­ã€‚\n"
                "5. **STRICT CHINESE (å¼·åˆ¶ç¹é«”ä¸­æ–‡)**: ä½ å¿…é ˆä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡æ’°å¯«å ±å‘Šã€‚çµ•å°ç¦æ­¢ä½¿ç”¨è‹±æ–‡æˆ–ç°¡é«”ä¸­æ–‡ã€‚\n"
                "6. **åˆ¤å®šåš´è¬¹**: çœ‹åˆ° Z=2.x çš„æ•¸æ“šæ™‚ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€Œæœªé” 3-Sigma ç•°å¸¸æ¨™æº–ã€ï¼Œé€™ä¸æ˜¯ç•°å¸¸ã€‚\n"
                "7. **ç¦æ­¢é‡è¤‡**: æ¯å±¤ Why å¿…é ˆæœ‰æ–°çš„ç™¼ç¾ã€‚\n"
                "8. **ç¦æ­¢è‡†æ¸¬**: å†æ¬¡å¼·èª¿ï¼Œè‹¥ç„¡ Mappingï¼Œå ±å‘Šä¸­åš´ç¦å‡ºç¾ä»»ä½•ä»£ç¢¼ä»¥å¤–çš„æè¿°æ€§è¡“èªã€‚\n"
                "9. **è¡Œå‹•å»ºè­°åˆ†ç´š (Crucial)**: \n"
                "   - è‹¥å…¨å ´ |Z-Score| < 3ï¼š**åš´ç¦**å»ºè­°ã€Œç«‹å³æª¢æŸ¥ã€ã€ã€Œæ ¡æº–ã€æˆ–ã€Œç¶­ä¿®ã€ã€‚åƒ…èƒ½å»ºè­°ã€ŒæŒçºŒç›£æ§ã€æˆ–ã€Œé—œæ³¨è¶¨å‹¢ã€ã€‚\n"
                "   - è‹¥ |Z-Score| > 3ï¼šæ‰å¯å»ºè­°å¯¦è³ªçš„è¨­å‚™æª¢æŸ¥æˆ–åƒæ•¸èª¿æ•´ã€‚\n"
                "10. **è¼¸å‡ºç´”æ–‡å­—**: æœ€çµ‚å ±å‘Šå¿…é ˆæ˜¯ä¹¾æ·¨çš„ Markdown æ ¼å¼ã€‚çµ•å°ç¦æ­¢è¼¸å‡ºåŸå§‹çš„ JSON ç‰©ä»¶æˆ–å­—å…¸ä»£ç¢¼ã€‚\n"
                "11. **ç¦æ­¢åˆ†éš”ç·š**: çµ•å°ç¦æ­¢ä½¿ç”¨ ===ã€---ã€*** ç­‰é€£çºŒç¬¦è™Ÿä½œç‚ºåˆ†éš”ç·šã€‚æ®µè½ä¹‹é–“åªç”¨ç©ºè¡Œæˆ– Markdown æ¨™é¡Œåˆ†éš”ã€‚"
            )

        full_text = ""
        suffix = f"\n\n```json\n{ev.chart_json}\n```\n" if ev.chart_json else ""

        # --- çœŸä¸²æµé–‹å§‹ ---
        async for chunk in self.llm.astream_complete(prompt):
            if chunk.delta:
                # æ¸…ç† LLM è¼¸å‡ºä¸­çš„å„ç¨®åƒåœ¾å­—å…ƒ
                cleaned = chunk.delta
                # 1. ç§»é™¤åˆ†éš”ç·šç¬¦è™Ÿ (===, ---, *** ç­‰é€£çºŒ 3 å€‹ä»¥ä¸Š)
                cleaned = re.sub(r"[=\-*~]{3,}", "", cleaned)
                # 2. ç§»é™¤ JSON æ®˜ç•™å­—å…ƒ (LLM å¾ JSON æ¨¡å¼åˆ‡æ›æ™‚çš„æ®˜ç•™)
                if not full_text.strip():
                    # å ±å‘Šé–‹é ­ï¼šç§»é™¤å¸¸è¦‹çš„ JSON æ®˜ç•™å‰ç¶´
                    cleaned = re.sub(r'^[\s@",{}\[\]\\:;`]+', "", cleaned)

                if cleaned.strip() or not chunk.delta.strip():  # ä¿ç•™ç©ºè¡Œä½†å»é™¤ç´”åƒåœ¾è¡Œ
                    full_text += cleaned
                    ctx.write_event_to_stream(TextChunkEvent(content=cleaned))

        # æœ€çµ‚æ•´é«”æ¸…ç†ï¼šç§»é™¤å ±å‘Šé–‹é ­å¯èƒ½æ®˜ç•™çš„ JSON ç¢ç‰‡
        full_text = re.sub(r'^[\s@",{}\[\]\\:;`]*\n*', "", full_text)

        if suffix:
            ctx.write_event_to_stream(TextChunkEvent(content=suffix))
            full_text += suffix

        return StopEvent(result={"response": full_text, "data": ev.data})


class LLMAnalysisAgent:
    """ç‚ºäº†å…¼å®¹å¤–éƒ¨èª¿ç”¨çš„å°è£é¡"""

    def __init__(
        self, tool_executor: ToolExecutor, analysis_service: AnalysisService, **kwargs
    ):
        self.workflow = SigmaAnalysisWorkflow(tool_executor, analysis_service)
        self.memories = {}

    def _get_memory(self, session_id: str):
        if session_id not in self.memories:
            self.memories[session_id] = ChatMemoryBuffer.from_defaults(
                token_limit=16000
            )
        return self.memories[session_id]

    async def stream_analyze(
        self, session_id: str, file_id: str, user_question: str, analysis_service=None
    ):
        memory = self._get_memory(session_id)
        history_str = "\n".join([f"{m.role}: {m.content}" for m in memory.get_all()])

        handler = self.workflow.run(
            query=user_question,
            file_id=file_id,
            session_id=session_id,
            history=history_str,
        )

        async for event in handler.stream_events():
            event_type = type(event).__name__
            if event_type == "TextChunkEvent":
                yield json.dumps(
                    {"type": "text_chunk", "content": event.content}, ensure_ascii=False
                )
            elif event_type == "ProgressEvent":
                yield json.dumps(
                    {"type": "thought", "content": event.msg}, ensure_ascii=False
                )
            elif event_type == "MonologueEvent":
                yield json.dumps(
                    {"type": "thought", "content": f"æ€è€ƒ: {event.monologue}"},
                    ensure_ascii=False,
                )
            elif event_type == "ToolCallEvent":
                yield json.dumps(
                    {"type": "tool_call", "tool": event.tool, "params": event.params},
                    ensure_ascii=False,
                )
            elif event_type == "ToolResultEvent":
                yield json.dumps(
                    {"type": "tool_result", "tool": event.tool, "result": event.result},
                    ensure_ascii=False,
                )

        final_result = await handler
        memory.put(ChatMessage(role="user", content=user_question))
        memory.put(
            ChatMessage(role="assistant", content=final_result.get("response", ""))
        )

        yield json.dumps(
            {
                "type": "response",
                "content": final_result.get("response"),
                "tool_result": final_result.get("data"),
            },
            ensure_ascii=False,
        )

    async def clear_session(self, session_id: str = "default"):
        if session_id in self.memories:
            self.memories[session_id].reset()
