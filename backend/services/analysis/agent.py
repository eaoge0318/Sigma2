import json
import logging
import requests
import httpx
from typing import Any, Dict, Optional
from llama_index.core.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
    ChatMessage,
)
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.llms.callbacks import llm_completion_callback
from llama_index.core.workflow import (
    Event,
    StartEvent,
    StopEvent,
    Workflow,
    step,
    Context,
)
from .tools.executor import ToolExecutor

logger = logging.getLogger(__name__)


class CustomOllamaLLM(CustomLLM):
    """
    è‡ªå®šç¾© Ollama LLM å°è£
    ä½¿ç”¨ requests ç›´æ¥ç™¼é€è«‹æ±‚ï¼Œæ”¯æ´å®Œæ•´ URL (å¦‚ http://ip:port/api/chat)
    è§£æ±º LlamaIndex åŸç”Ÿ Ollama å¥—ä»¶å° URL æ ¼å¼èˆ‡é€£ç·šçš„é™åˆ¶
    """

    model_name: str
    api_url: str
    timeout: float = 120.0

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=32768,
            num_output=4096,
            model_name=self.model_name,
        )

    @llm_completion_callback()
    async def acomplete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        """éä¸²æµå®Œæ•´å›å‚³ (ç”¨æ–¼æ„åœ–èˆ‡å·¥å…·é¸æ“‡ï¼Œé€Ÿåº¦è¼ƒå¿«)"""
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(self.api_url, json=payload)
                response.raise_for_status()
                result = response.json()
                content = result.get("message", {}).get("content", "")
                return CompletionResponse(text=content)
        except Exception as e:
            logger.error(f"Ollama Async é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(
                f"ç„¡æ³•éåŒæ­¥é€£ç·šè‡³ Ollama: {str(e)} (URL: {self.api_url})"
            )

    async def astream_complete(
        self, prompt: str, **kwargs: Any
    ) -> CompletionResponseGen:
        """æ ¸å¿ƒä¸²æµå›å‚³ (ç”¨æ–¼æœ€å¾Œçš„æ‘˜è¦å ±å‘Š)"""
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": True,
        }
        try:
            full_content = ""
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST", self.api_url, json=payload
                ) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if not line:
                            continue
                        chunk = json.loads(line)
                        if "message" in chunk:
                            content = chunk["message"].get("content", "")
                            full_content += content
                            yield CompletionResponse(text=content, delta=content)
                        if chunk.get("done"):
                            break
        except Exception as e:
            logger.error(f"Ollama Stream é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(f"ç„¡æ³•ä¸²æµé€£ç·šè‡³ Ollama: {str(e)}")

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        try:
            response = requests.post(self.api_url, json=payload, timeout=self.timeout)
            response.raise_for_status()
            result = response.json()
            content = result.get("message", {}).get("content", "")
            return CompletionResponse(text=content)
        except Exception as e:
            logger.error(f"Ollama é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(f"ç„¡æ³•é€£ç·šè‡³ Ollama: {str(e)} (URL: {self.api_url})")

    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        # æš«ä¸æ”¯æ´ä¸²æµï¼Œç›´æ¥è¿”å›ä¸€æ¬¡æ€§çµæœ
        response = self.complete(prompt, **kwargs)
        yield response


# ========== äº‹ä»¶å®šç¾© (Events) ==========


class IntentEvent(Event):
    """æ„åœ–è­˜åˆ¥å¾Œçš„äº‹ä»¶"""

    query: str
    intent: str  # "analysis", "translation", "chat"
    file_id: str
    session_id: str
    history: str


class AnalysisEvent(Event):
    """åŸ·è¡Œåœ°ç«¯åˆ†æçš„äº‹ä»¶"""

    query: str
    file_id: str
    session_id: str
    history: str


class TranslationEvent(Event):
    """åŸ·è¡Œç°¡å–®ç¿»è­¯æˆ–èŠå¤©çš„äº‹ä»¶"""

    query: str
    session_id: str
    history: str


class VisualizingEvent(Event):
    """æ•¸æ“šå¯è¦–åŒ–/ç¹ªåœ–ç«™çš„äº‹ä»¶"""

    data: Any
    query: str
    session_id: str
    history: str
    row_count: int = 0
    col_count: int = 0
    mappings: Dict[str, str] = {}


class SummarizeEvent(Event):
    """åŸ·è¡Œçµæœç¸½çµçš„äº‹ä»¶ (åŒ…å«å¯é¸çš„åœ–è¡¨ JSON)"""

    data: Any
    query: str
    session_id: str
    history: str
    chart_json: Optional[str] = None
    row_count: int = 0
    col_count: int = 0


class ToolCallEvent(Event):
    """å·¥å…·èª¿ç”¨é–‹å§‹äº‹ä»¶"""

    tool: str
    params: Dict


class TextChunkEvent(Event):
    """æµå¼æ–‡æœ¬ç¢ç‰‡çš„äº‹ä»¶"""

    content: str


class ToolResultEvent(Event):
    """å·¥å…·èª¿ç”¨çµæœå®Œæˆäº‹ä»¶"""

    tool: str
    result: Any


class ProgressEvent(Event):
    """é€šç”¨é€²åº¦æ›´æ–°äº‹ä»¶"""

    msg: str


class ConceptExpansionEvent(Event):
    """ç•¶æœç´¢å¤±æ•—æ™‚ï¼Œè«‹æ±‚ LLM æ“´å±•æ¦‚å¿µçš„äº‹ä»¶"""

    query: str
    original_concept: str
    file_id: str
    session_id: str
    history: str


class ErrorEvent(Event):
    """éŒ¯èª¤äº‹ä»¶"""

    error: str
    session_id: str


# ========== Workflow å¯¦ä½œ ==========


class SigmaAnalysisWorkflow(Workflow):
    """
    Sigma2 æ™ºèƒ½åˆ†æå·¥ä½œæµ
    å°‡ LLM ä½œç‚ºèª¿åº¦å“¡èˆ‡ç¸½çµå“¡ï¼Œè€Œå°‡è¤‡é›œé‹ç®—äº¤çµ¦åœ°ç«¯ Python åŸ·è¡Œã€‚
    """

    def __init__(
        self,
        tool_executor: ToolExecutor,
        model_name: str = "llama3:latest",
        ollama_api_url: str = "http://localhost:11434/api/chat",
        timeout: int = 180,
    ):
        super().__init__(timeout=timeout, verbose=True)
        self.tool_executor = tool_executor
        self.llm = CustomOllamaLLM(
            model_name=model_name,
            api_url=ollama_api_url,
            timeout=180.0,
        )

    @step
    async def route_intent(
        self, ctx: Context, ev: StartEvent
    ) -> IntentEvent | ErrorEvent:
        """
        [LLM Step] æ„åœ–è­˜åˆ¥å·¥ä½œç«™
        """
        ctx.write_event_to_stream(
            ProgressEvent(msg="(Thinking...) æ­£åœ¨ç†è§£æ‚¨çš„å•é¡Œä¸¦æº–å‚™åˆ†æé‡é»...")
        )
        query = getattr(ev, "query", None)
        file_id = getattr(ev, "file_id", None)
        session_id = getattr(ev, "session_id", None)
        history = getattr(ev, "history", "")

        if not query:
            return ErrorEvent(error="æœªæä¾›å•é¡Œ", session_id=session_id)

        prompt = (
            "ä½ æ˜¯ä¸€å€‹æ„åœ–è­˜åˆ¥å°ˆå“¡ã€‚è«‹æ ¹æ“šå°è©±æ­·å²èˆ‡ç•¶å‰å•é¡Œåˆ¤æ–·é¡å‹ã€‚\n"
            f"å°è©±æ­·å²:\n{history}\n"
            f"ç”¨æˆ¶å•é¡Œ: {query}\n"
            "é¡å‹å¿…é ˆæ˜¯ä»¥ä¸‹ä¹‹ä¸€:\n"
            "- 'analysis': ç”¨æˆ¶æƒ³åˆ†ææ•¸æ“šã€æŸ¥æ¬„ä½ã€ç¹ªåœ–æˆ–ä»»ä½•æ¶‰åŠ CSV è³‡æ–™çš„æ“ä½œã€‚\n"
            "- 'translation': ç”¨æˆ¶æƒ³ç¿»è­¯å…§å®¹æˆ–é€²è¡Œç°¡å–®å°è©±ã€‚\n"
            "- 'chat': å…¶ä»–çš„ä¸€èˆ¬èŠå¤©ã€‚\n"
            "è«‹åƒ…å›å‚³é¡å‹åç¨±ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡æ€è€ƒï¼Œä½†åªè¼¸å‡ºé¡å‹å–®å­—ã€‚"
        )

        response = await self.llm.acomplete(prompt)
        intent = str(response.text).strip().lower()

        logger.info(f"ğŸ¯ [Intent Station] Intent: {intent}")

        return IntentEvent(
            query=query,
            intent=intent,
            file_id=file_id,
            session_id=session_id,
            history=history,
        )

    @step
    async def dispatch_work(
        self, ctx: Context, ev: IntentEvent
    ) -> AnalysisEvent | TranslationEvent:
        # Normalize and log intent for debugging
        intent = (ev.intent or "").strip().lower()
        logger.info(f"ğŸ”„ [Dispatch Station] Routing intent: {repr(intent)}")

        if "analysis" in intent or "chart" in intent or "data" in intent:
            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
            )
        else:
            return TranslationEvent(
                query=ev.query, session_id=ev.session_id, history=ev.history
            )

    @step
    async def execute_analysis(
        self, ctx: Context, ev: AnalysisEvent
    ) -> VisualizingEvent | ConceptExpansionEvent:
        """
        [Local Step] æ•¸æ“šåˆ†æå·¥ä½œç«™
        """
        ctx.write_event_to_stream(
            ProgressEvent(
                msg="(Executing...) æ­£åœ¨æƒæåœ°ç«¯è³‡æ–™åº«ï¼Œå°‹æ‰¾æœ€ç›¸é—œçš„æ¬„ä½èˆ‡åƒæ•¸..."
            )
        )
        # ç²å–å·¥å…·æ¸…å–®

        tool_specs = ""
        for name, tool in self.tool_executor.tools.items():
            tool_specs += (
                f"- {name}: {tool.description} (éœ€è¦åƒæ•¸: {tool.required_params})\n"
            )

        # ç²å–æ–‡ä»¶æ‘˜è¦ä»¥æä¾›å¯ç”¨æ¬„ä½è³‡è¨Šçµ¦ LLM
        logger.info(f"ğŸ” [Analysis Station] Loading summary for file: {ev.file_id}")
        summary = self.tool_executor.analysis_service.load_summary(
            ev.session_id, ev.file_id
        )
        available_params = ""
        if summary:
            params_list = summary.get("parameters", [])
            mappings = summary.get("mappings", {})
            # å¦‚æœæ¬„ä½å¤ªå¤šï¼Œåªå–ä¸€éƒ¨åˆ†æˆ–é—œéµè³‡è¨Š (æ¸›å°‘ LLM è² æ“”ï¼Œå¾ 100 é™ç‚º 40)
            sampled_params = params_list[:40]
            param_desc = []
            for p in sampled_params:
                m = mappings.get(p, "")
                param_desc.append(f"{p} ({m})" if m else p)
            available_params = ", ".join(param_desc)
            if len(params_list) > 40:
                available_params += "... (ç­‰æ›´å¤šæ¬„ä½)"

        prompt = (
            "ä½ æ˜¯ä¸€å€‹å·¥å…·èª¿ç”¨å°ˆå®¶ã€‚è«‹æ ¹æ“šæ­·å²èˆ‡å•é¡Œé¸æ“‡åˆé©çš„å·¥å…·ä¸¦æå–åƒæ•¸ã€‚\n"
            f"å°è©±æ­·å²:\n{ev.history}\n"
            f"å¯ç”¨æ•¸æ“šæ¬„ä½ (æ ¼å¼: ä»£ç¢¼ (ä¸­æ–‡åç¨±)): {available_params}\n"
            f"å·¥å…·æ¸…å–®:\n{tool_specs}\n"
            f"ç”¨æˆ¶å•é¡Œ: {ev.query}\n"
            f"æ–‡ä»¶ ID: {ev.file_id}\n"
            "**é‡è¦è¦å‰‡**:\n"
            "1. å¦‚æœç”¨æˆ¶å•ã€Œæœ‰å“ªäº›æ¬„ä½ã€æˆ–ã€Œåˆ—å‡ºåƒæ•¸ã€ï¼Œ**å¿…é ˆ** èª¿ç”¨ get_parameter_listã€‚\n"
            "2. å¦‚æœæ˜¯ç”¨æˆ¶è¦æ±‚ã€Œç•«åœ–ã€ã€ã€Œè¶¨å‹¢ã€ã€ã€Œåˆ†ä½ˆã€ï¼Œè«‹èª¿ç”¨æ•¸æ“šç²å–å·¥å…·ï¼ˆå¦‚ get_time_series_dataï¼‰ã€‚\n"
            "3. get_time_series_data çš„åƒæ•¸ 'parameters' å¿…é ˆæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œä¾‹å¦‚ ['MEDIC-ABB_B41']ã€‚\n"
            "4. å¦‚æœç”¨æˆ¶å•çš„æ¦‚å¿µä¸åœ¨æ¸…å–®ä¸­ï¼Œè«‹èª¿ç”¨ search_parameters_by_conceptã€‚\n"
            "5. è«‹å›å‚³ JSON æ ¼å¼: {'tool_name': '...', 'params': {...}}\n"
        )

        response = await self.llm.acomplete(prompt)
        try:
            clean_text = str(response.text).strip()
            # 1. ç§»é™¤ Markdown æ¨™è¨˜
            if "```" in clean_text:
                import re

                match = re.search(r"```(?:json)?(.*?)```", clean_text, re.DOTALL)
                if match:
                    clean_text = match.group(1).strip()

            # 2. å˜—è©¦è§£æ JSON
            import ast

            try:
                decision = json.loads(clean_text)
            except json.JSONDecodeError:
                # Fallback: å˜—è©¦ä½¿ç”¨ ast.literal_eval è™•ç† Python é¢¨æ ¼çš„å­—å…¸ (å–®å¼•è™Ÿ)
                try:
                    decision = ast.literal_eval(clean_text)
                except Exception:
                    logger.warning(f"JSON Parsing failed completely. Raw: {clean_text}")
                    raise

            if not isinstance(decision, dict):
                raise ValueError("Parsed result is not a dictionary")

            tool_name = decision.get("tool_name")
            params = decision.get("params", {})
            params["file_id"] = ev.file_id

            # --- åƒæ•¸è£œå¼· ( hallu-correction ) ---
            # å¦‚æœ LLM æŠŠ parameters å¯«æˆ column_name æˆ– parameter
            if "parameter" in params and "parameters" not in params:
                val = params["parameter"]
                params["parameters"] = val if isinstance(val, list) else [val]
            if "column_name" in params and "parameters" not in params:
                val = params["column_name"]
                params["parameters"] = val if isinstance(val, list) else [val]
            if "keyword" in params and "concept" not in params:
                params["concept"] = params["keyword"]

            # ç¢ºä¿ parameters æ°¸é æ˜¯æ‰å¹³åˆ—è¡¨ (é¿å… [[...]] ç™¼ç”Ÿ)
            if "parameters" in params and isinstance(params["parameters"], list):
                if len(params["parameters"]) > 0 and isinstance(
                    params["parameters"][0], list
                ):
                    params["parameters"] = params["parameters"][0]

            logger.info(
                f"ğŸ› ï¸ [Analysis Station] Calling tool: {tool_name} with params: {params}"
            )
            # ç™¼é€å·¥å…·é–‹å§‹äº‹ä»¶åˆ°ä¸²æµ
            ctx.write_event_to_stream(
                ProgressEvent(
                    msg=f"(Planning...) åˆ†æç­–ç•¥å·²æ“¬å®šï¼Œæ­£åœ¨å•Ÿå‹• '{tool_name}' ä»¥æª¢ç´¢å°æ‡‰æ•¸æ“š..."
                )
            )
            ctx.write_event_to_stream(ToolCallEvent(tool=tool_name, params=params))

            result = self.tool_executor.execute_tool(tool_name, params, ev.session_id)

            # è¨ˆç®—å–å¾—ç­†æ•¸ (é‡å°ä¸åŒå·¥å…·çµæœå‹æ…‹)
            data_count = "éƒ¨åˆ†"
            if isinstance(result, list):
                data_count = len(result)
            elif isinstance(result, dict) and "data" in result:
                # å–å¾— data å­—å…¸ä¸­ç¬¬ä¸€å€‹åˆ—è¡¨çš„é•·åº¦ä½œç‚ºç­†æ•¸
                first_val = (
                    next(iter(result["data"].values())) if result["data"] else []
                )
                if isinstance(first_val, list):
                    data_count = len(first_val)

            total_cols = summary.get("total_columns", 0) if summary else 0

            # ç™¼é€å·¥å…·çµæœäº‹ä»¶åˆ°ä¸²æµ
            ctx.write_event_to_stream(ToolResultEvent(tool=tool_name, result=result))
            ctx.write_event_to_stream(
                ProgressEvent(
                    msg=f"(Executing...) æ•¸æ“šæª¢ç´¢å®Œæˆï¼Œå…±å–å¾— {data_count} ç­†è¨˜éŒ„ï¼Œæº–å‚™é€²è¡Œå¯è¦–åŒ–åŠ å·¥è™•ç†..."
                )
            )

            logger.info(
                f"ğŸ“¦ [Analysis Station] Tool Result Keys: {list(result.keys()) if isinstance(result, dict) else 'non-dict'}, count={data_count}"
            )

            # --- èªç¾©æ“´å±•é‚è¼¯ (Self-Correction) ---
            # å¦‚æœæ˜¯æœç´¢å·¥å…·ä½†æ²’çµæœï¼Œä¸”é‚„æ²’é‡è©¦éï¼Œå‰‡é€²å…¥æ“´å±•æµç¨‹
            is_search_tool = tool_name in [
                "get_parameter_list",
                "search_parameters_by_concept",
            ]
            has_no_results = not result.get("parameters") and not result.get(
                "matched_parameters"
            )

            # ä½¿ç”¨ ctx.store åšç‚ºç‹€æ…‹å„²å­˜ (LlamaIndex Workflow æ¨™æº–æ–¹å¼)
            retry_count = await ctx.store.get("retry_count", default=0)

            if is_search_tool and has_no_results and retry_count < 1:
                logger.info(
                    "ğŸ” [Analysis Station] No results found. Attempting semantic expansion..."
                )
                await ctx.store.set("retry_count", retry_count + 1)
                concept = params.get("keyword") or params.get("concept") or ev.query
                return ConceptExpansionEvent(
                    query=ev.query,
                    original_concept=concept,
                    file_id=ev.file_id,
                    session_id=ev.session_id,
                    history=ev.history,
                )
            # ------------------------------------
            # --- æ™ºæ…§è·¯å¾‘åˆ†æµ (Smart Skip) ---
            # å¦‚æœçµæœä¸åŒ…å«æ•¸æ“šï¼ˆä¾‹å¦‚åªæ˜¯ç²å–æ¬„ä½åˆ—è¡¨ï¼‰ï¼Œå‰‡è·³éç¹ªåœ–ç«™ï¼Œç›´æ¥é€²å…¥ç¸½çµç«™
            has_data = isinstance(result, dict) and "data" in result
            if has_data:
                logger.info(
                    "ğŸ¨ [Analysis Station] Data found. Routing to Visualizing Station."
                )
                return VisualizingEvent(
                    data=result,
                    query=ev.query,
                    session_id=ev.session_id,
                    history=ev.history,
                    row_count=data_count if isinstance(data_count, int) else 0,
                    col_count=total_cols,
                    mappings=mappings if isinstance(mappings, dict) else {},
                )
            else:
                logger.info(
                    "â­ï¸ [Analysis Station] No data for chart. Skipping to Summary Station."
                )
                return SummarizeEvent(
                    data=result,
                    query=ev.query,
                    session_id=ev.session_id,
                    history=ev.history,
                    row_count=data_count if isinstance(data_count, int) else 0,
                    col_count=total_cols,
                )

        except Exception as e:
            logger.error(f"âŒ [Analysis Station] Error: {e}")
            return SummarizeEvent(
                data=f"åˆ†æå·¥å…·åŸ·è¡Œå¤±æ•—: {str(e)}",
                query=ev.query,
                session_id=ev.session_id,
                history=ev.history,
                row_count=0,
                col_count=0,
            )

    @step
    async def expand_concept(
        self, ctx: Context, ev: ConceptExpansionEvent
    ) -> AnalysisEvent:
        """
        [LLM Step] Semantic Expansion Station
        """
        logger.info(f"ğŸ§  [Expansion Station] Expanding concept: {ev.original_concept}")

        prompt = (
            "ä½ æ˜¯ä¸€å€‹å·¥æ¥­èˆ‡è‡ªå‹•åŒ–å°ˆå®¶ã€‚ç”¨æˆ¶åœ¨åœ°ç«¯è³‡æ–™åº«æœå°‹é—œéµå­—å¤±æ•—äº†ã€‚\n"
            f"åŸå§‹é—œéµå­—: {ev.original_concept}\n"
            "è«‹æ€è€ƒåœ¨å¯¦éš›çš„ç”Ÿç”¢æ•¸æ“šé›†ï¼ˆCSVï¼‰ä¸­ï¼Œé€™å€‹æ¦‚å¿µå¯èƒ½å°æ‡‰çš„å°ˆæ¥­è‹±æ–‡è¡“èªã€ç¸®å¯«æˆ–å¸¸è¦‹æ¬„ä½åç¨±ã€‚\n"
            "ä¾‹å¦‚ï¼šã€Œå£“åŠ›ã€å¯èƒ½å°æ‡‰ 'Pressure', 'PRESS', 'Bar', 'PSI' ç­‰ã€‚\n"
            "è«‹æä¾› 3-5 å€‹æœ€å¯èƒ½çš„æ›¿ä»£é—œéµå­—ï¼Œä¸¦ä»¥ JSON åˆ—è¡¨æ ¼å¼å›å‚³ï¼š\n"
            '["term1", "term2", ...]\n'
            "ä¸è¦å›å‚³å…¶ä»–æ–‡å­—ã€‚"
        )

        response = await self.llm.acomplete(prompt)
        try:
            expanded_terms = json.loads(str(response.text).strip())
            new_query = f"è«‹å¹«æˆ‘æœå°‹ä»¥ä¸‹ç›¸é—œæ¬„ä½: {', '.join(expanded_terms)}"
            logger.info(f"ğŸ’¡ [Expansion Station] Expanded to: {expanded_terms}")

            return AnalysisEvent(
                query=new_query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history
                + f"\nç³»çµ±æç¤º: è‡ªå‹•é‡è©¦èªç¾©æ“´å±•æœå°‹: {expanded_terms}",
            )
        except Exception as e:
            logger.warning(f"âš ï¸ [Expansion Station] Expansion process failed: {str(e)}")
            # å¦‚æœè§£æå¤±æ•—ï¼Œå°±ç”¨åŸå§‹å•é¡Œå†è©¦ä¸€æ¬¡ï¼Œä½†æœƒå› ç‚º retry_count åœæ­¢
            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
            )

    @step
    async def execute_translation(
        self, ctx: Context, ev: TranslationEvent
    ) -> VisualizingEvent:
        # Debug Log
        logger.info(
            f"ğŸ”„ [Translation Station] Processing translation intent for query: {ev.query}"
        )
        return VisualizingEvent(
            data=None,
            query=ev.query,
            session_id=ev.session_id,
            history=ev.history,
        )

    def _build_programmatic_chart(self, ev: VisualizingEvent) -> Optional[str]:
        """
        ç´”ç¨‹å¼é‚è¼¯ç”Ÿæˆçš„åœ–è¡¨ JSONï¼Œå®Œå…¨ä¸ä¾è³´ LLMï¼Œç¢ºä¿ 100% ç©©å®šã€‚
        """
        try:
            if (
                not isinstance(ev.data, dict)
                or "data" not in ev.data
                or not ev.data["data"]
            ):
                return None

            actual_data = ev.data["data"]
            query_lower = ev.query.lower()
            is_histogram = (
                any(kw in query_lower for kw in ["ç›´æ–¹åœ–", "histogram", "åˆ†ä½ˆ", "åˆ†å¸ƒ"])
                and "è¶¨å‹¢" not in query_lower
            )
            is_scatter = any(kw in query_lower for kw in ["æ•£ä½ˆ", "scatter", "ç›¸é—œæ€§"])
            is_dual_axis = any(kw in query_lower for kw in ["é›™è»¸", "dual", "ä¸åŒåˆ»åº¦"])

            if is_histogram:
                # ... [Histogram logic preserved] ...
                logger.info("ğŸ“Š [Visualizer] Building Histogram.")
                target_col = None
                for col, vals in actual_data.items():
                    if col not in [
                        "CONTEXTID",
                        "TIME",
                        "Timestamp",
                        "Date",
                        "æ™‚é–“",
                    ] and any(isinstance(v, (int, float)) for v in vals[:20]):
                        target_col = col
                        break

                if not target_col:
                    return None
                values = [
                    v for v in actual_data[target_col] if isinstance(v, (int, float))
                ]
                if not values:
                    return None

                v_min, v_max = min(values), max(values)
                if v_min == v_max:
                    v_max += 1.0
                num_bins = 15
                bin_size = (v_max - v_min) / num_bins
                bins = [0] * num_bins
                for v in values:
                    idx = int((v - v_min) / bin_size)
                    idx = min(idx, num_bins - 1)
                    bins[idx] += 1

                labels = [
                    f"{v_min + i * bin_size:.2f}-{v_min + (i + 1) * bin_size:.2f}"
                    for i in range(num_bins)
                ]
                chart_obj = {
                    "type": "chart",
                    "chart_type": "bar",
                    "title": f"{ev.mappings.get(target_col, target_col)} æ•¸æ“šåˆ†ä½ˆç›´æ–¹åœ–",
                    "labels": labels,
                    "datasets": [
                        {
                            "label": "é »æ¬¡",
                            "data": bins,
                            "backgroundColor": "rgba(54, 162, 235, 0.6)",
                            "borderColor": "rgb(54, 162, 235)",
                        }
                    ],
                }
                final_labels, datasets = labels, chart_obj["datasets"]

            elif is_scatter:
                logger.info("ğŸ“Š [Visualizer] Building Scatter Plot.")
                numeric_cols = []
                for col, vals in actual_data.items():
                    if col not in [
                        "CONTEXTID",
                        "TIME",
                        "Timestamp",
                        "Date",
                        "æ™‚é–“",
                    ] and any(isinstance(v, (int, float)) for v in vals[:10]):
                        numeric_cols.append(col)

                if len(numeric_cols) < 2:
                    return None  # æ•£ä½ˆåœ–è‡³å°‘è¦å…©å€‹ç¶­åº¦

                col_x, col_y = numeric_cols[0], numeric_cols[1]
                scatter_data = []
                for i in range(min(len(actual_data[col_x]), 100)):  # é™åˆ¶ 100 é»
                    vx, vy = actual_data[col_x][i], actual_data[col_y][i]
                    if isinstance(vx, (int, float)) and isinstance(vy, (int, float)):
                        scatter_data.append({"x": vx, "y": vy})

                chart_obj = {
                    "type": "chart",
                    "chart_type": "scatter",
                    "title": f"ç›¸é—œæ€§åˆ†æ: {ev.mappings.get(col_x, col_x)} vs {ev.mappings.get(col_y, col_y)}",
                    "datasets": [
                        {
                            "label": f"{ev.mappings.get(col_x, col_x)} / {ev.mappings.get(col_y, col_y)}",
                            "data": scatter_data,
                        }
                    ],
                    "options": {
                        "scales": {
                            "x": {
                                "title": {
                                    "display": True,
                                    "text": ev.mappings.get(col_x, col_x),
                                },
                                "grid": {"display": True},
                            },
                            "y": {
                                "title": {
                                    "display": True,
                                    "text": ev.mappings.get(col_y, col_y),
                                }
                            },
                        }
                    },
                }
                final_labels, datasets = [], chart_obj["datasets"]

            else:
                # è¶¨å‹¢åœ– (Line Chart) é‚è¼¯ï¼Œå¢åŠ é›™è»¸åµæ¸¬
                labels = []
                label_col = next(
                    (
                        c
                        for c in ["CONTEXTID", "TIME", "Timestamp", "Date", "æ™‚é–“"]
                        if c in actual_data
                    ),
                    None,
                )
                first_col_data = next(iter(actual_data.values()))
                labels = (
                    actual_data[label_col]
                    if label_col
                    else list(range(1, len(first_col_data) + 1))
                )

                datasets = []
                max_points = 50
                for col_name, values in actual_data.items():
                    if col_name == label_col:
                        continue
                    sampled = values[:: max(1, len(values) // max_points)][:max_points]
                    if not any(isinstance(x, (int, float)) for x in sampled[:5]):
                        continue

                    friendly_name = ev.mappings.get(col_name, col_name)
                    datasets.append(
                        {
                            "label": friendly_name,
                            "raw_max": max(
                                [v for v in sampled if isinstance(v, (int, float))]
                                or [0]
                            ),
                            "data": sampled,
                        }
                    )
                    if len(datasets) >= 30:
                        break

                if not datasets:
                    return None

                # é›™è»¸è‡ªå‹•åµæ¸¬ï¼šå¦‚æœæœ€å¤§å€¼ç›¸å·® 10 å€ä»¥ä¸Šï¼Œä¸”ç”¨æˆ¶æ²’åå° OR ç”¨æˆ¶æ˜ç¢ºè¦æ±‚
                if (len(datasets) >= 2 and is_dual_axis) or (
                    len(datasets) >= 2
                    and max(d["raw_max"] for d in datasets)
                    / (min(d["raw_max"] for d in datasets) or 1)
                    > 10
                ):
                    logger.info(
                        "ğŸ“Š [Visualizer] Auto-detected scale mismatch, enabling Dual Y-Axis."
                    )
                    # å°‡æœ€å¤§å€¼è¼ƒå¤§çš„ dataset æ”¾åˆ°äº†å³è»¸ (y1)
                    idx_max = 0
                    curr_max = -1
                    for i, d in enumerate(datasets):
                        if d["raw_max"] > curr_max:
                            curr_max = d["raw_max"]
                            idx_max = i

                    datasets[idx_max]["yAxisID"] = "y1"
                    chart_options = {
                        "scales": {
                            "y1": {
                                "type": "linear",
                                "display": True,
                                "position": "right",
                                "title": {
                                    "display": True,
                                    "text": datasets[idx_max]["label"],
                                },
                                "grid": {"drawOnChartArea": False},
                            }
                        }
                    }
                else:
                    chart_options = {}

                final_labels = labels[:: max(1, len(labels) // max_points)][:max_points]
                chart_obj = {
                    "type": "chart",
                    "chart_type": "line",
                    "title": f"æ•¸æ“šè¶¨å‹¢åˆ†æ: {ev.query[:20]}",
                    "labels": final_labels,
                    "datasets": datasets,
                    "options": chart_options,
                }

            logger.info(
                f"ğŸ“Š [Visualizer] Generated chart_obj with {len(datasets)} datasets and {len(final_labels)} labels."
            )
            return json.dumps(chart_obj, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ [Visualizer] Programmatic chart build failed: {e}")
            return None

    @step
    async def visualize_data(
        self, ctx: Context, ev: VisualizingEvent
    ) -> SummarizeEvent:
        """
        [Hybrid Step] ç¹ªåœ–å·¥ä½œç«™ - å…ˆå˜—è©¦ç¨‹å¼ç”Ÿæˆï¼Œå¤±æ•—æ‰ç”¨ LLM (ç›®å‰å¼·åˆ¶ç¨‹å¼ç”Ÿæˆä»¥æ±‚ç©©å®š)
        """
        ctx.write_event_to_stream(
            ProgressEvent(
                msg="(Visualizing...) æŠ€è¡“åˆ†æå®Œæˆï¼Œæ­£åœ¨ç²¾æº–ç¹ªè£½æ•¸æ“šè¶¨å‹¢åœ–..."
            )
        )

        # æ”¹ç‚ºç¨‹å¼åŒ–ç”Ÿæˆï¼Œç¢ºä¿ç©©å®š
        chart_json = self._build_programmatic_chart(ev)

        if chart_json:
            logger.info(
                f"ğŸ¨ [Visualizer] Programmatic chart generated. Len: {len(chart_json)}"
            )
            # å®Œæ•´å°å‡º JSON ä»¥ä¾¿é™¤éŒ¯ (åƒ…é™ç›®å‰é–‹ç™¼åµéŒ¯éšæ®µ)
            logger.debug(f"ğŸ¨ [Visualizer] Full Chart JSON: {chart_json}")
        else:
            logger.info("ğŸ¨ [Visualizer] No chart generated (data invalid or empty).")

        return SummarizeEvent(
            data=ev.data,
            query=ev.query,
            session_id=ev.session_id,
            history=ev.history,
            chart_json=chart_json,
            row_count=ev.row_count,
            col_count=ev.col_count,
        )

    @step
    async def humanizer(self, ctx: Context, ev: SummarizeEvent) -> StopEvent:
        """
        [LLM Step] çµæœç¸½çµå·¥ä½œç«™ - å°ˆæ³¨æ–¼è‡ªç„¶èªè¨€åˆ†æå ±å‘Š
        """
        ctx.write_event_to_stream(
            ProgressEvent(
                msg="(Humanizing...) åœ–è¡¨å·²ç”Ÿæˆï¼Œæ­£åœ¨æ’°å¯«åˆ†æå ±å‘Šä¸¦æä¾›å°ˆå®¶å»ºè­°..."
            )
        )
        logger.info("âœï¸ [Humanizer Station] Generating summary...")

        # æ•¸æ“šæŠ½æ¨£ (é¿å…æ•¸æ“šé‡éå¤§å°è‡´ Prompt è¶…é•·æˆ– LLM æ··äº‚)
        display_data = ev.data
        if isinstance(ev.data, dict) and "data" in ev.data:
            # è¤‡è£½ä¸€ä»½ä¾†æŠ½æ¨£
            display_data = ev.data.copy()
            actual_data = ev.data.get("data", {})
            sampled_data = {}
            for k, v in actual_data.items():
                if isinstance(v, list) and len(v) > 50:
                    # æ¯ 20 é»å– 1 é»ï¼Œç¢ºä¿ LLM èƒ½çœ‹åˆ°è¶¨å‹¢ä½†åˆä¸æœƒæ·¹æ²’åœ¨æ•¸å­—ä¸­
                    step = len(v) // 50
                    sampled_data[k] = v[::step][:50]
                else:
                    sampled_data[k] = v
            display_data["data"] = sampled_data
            display_data["_is_sampled"] = True
            display_data["_original_count"] = (
                len(next(iter(actual_data.values()))) if actual_data else 0
            )

        # å®‰å…¨åºåˆ—åŒ–å‡½å¼
        def safe_json_dumps(obj):
            try:
                return json.dumps(obj, ensure_ascii=False, default=str)
            except Exception as e:
                logger.error(f"âŒ [Humanizer] JSON serialization failed: {e}")
                return "{}"

        data_json_str = safe_json_dumps(ev.data)
        logger.info(
            f"ğŸ“Š [Humanizer] Injecting data into prompt. Size: {len(data_json_str)} chars"
        )

        prompt = (
            "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å·¥æ¥­æ•¸æ“šåˆ†æå°ˆå®¶ã€‚ä½  **å¿…é ˆå…¨ç¨‹ä½¿ç”¨ ç¹é«”ä¸­æ–‡ (Traditional Chinese)** å›ç­”ç”¨æˆ¶ã€‚\n"
            "å¦‚æœä½ æœ‰åœ–è¡¨æ•¸æ“šï¼Œè«‹çµåˆåœ–è¡¨å…§å®¹é€²è¡Œæ·±å…¥çš„å°ˆå®¶ç´šåˆ†æã€‚\n"
            "\n"
            "**è¼¸å‡ºæº–å‰‡**:\n"
            "1. **åš´ç¦è‹±æ–‡å›è¦†**: æ‰€æœ‰çš„æè¿°ã€ç¸½çµéƒ½å¿…é ˆæ˜¯ç¹é«”ä¸­æ–‡ã€‚\n"
            "2. **å°ˆå®¶å»ºè­°**: è«‹é‡å°åˆ†æçµæœçµ¦å‡º 3-5 é»å…·é«”çš„å°ˆæ¥­å»ºè­°ã€‚\n"
            "3. **æ ¼å¼èªªæ˜**: åœ–è¡¨å·²ç¶“åœ¨ä¸‹æ–¹ç”±ç³»çµ±è‡ªå‹•åŠ è¼‰ï¼Œä½ ä¸éœ€è¦å†æ¬¡è¼¸å‡º JSON æ ¼å¼ï¼Œåªéœ€å°ˆæ³¨æ–¼æ–‡å­—åˆ†æã€‚\n"
            "\n"
            f"ç”¨æˆ¶å•é¡Œ: {ev.query}\n"
            f"æœ¬æ¬¡åˆ†ææ¶µè“‹æ•¸æ“šç­†æ•¸: {ev.row_count}\n"
            f"æœ¬æ¬¡åˆ†ææ¶µè“‹ç¸½æ¬„ä½æ•¸: {ev.col_count}\n"
            f"é‡è¦æç¤º: åƒ…é‡å°é€™ {ev.row_count} ç­†æ•¸æ“šé€²è¡Œç²¾ç¢ºæè¿°ï¼Œåš´ç¦è™›æ§‹æ•¸æ“šé‡ã€‚ç¸½æ¬„ä½æ•¸æ‡‰ä»¥ {ev.col_count} ç‚ºæº–ã€‚\n"
            f"åˆ†ææ•¸æ“šç¯„ä¾‹: {json.dumps(ev.data, ensure_ascii=False, default=str)[:2000]} (å·²çœç•¥éé•·éƒ¨åˆ†)\n"
            f"å°è©±æ­·å²: {ev.history}\n"
        )

        # å¼·åŠ›é˜²è­·ï¼šåµæ¸¬ Prompt è‡ªèº«æ˜¯å¦å«ææ¯€ç‰©ä»¶æ¨™è¨˜
        if "[object Object]" in prompt:
            logger.warning(
                "âš ï¸ [Humanizer] [object Object] detected in incoming prompt history/data."
            )

        # ç¢ºä¿ chart_json çµ•å°æ˜¯å­—ä¸²æˆ– None
        final_chart_json = ev.chart_json
        if final_chart_json and not isinstance(final_chart_json, str):
            logger.warning(
                f"âš ï¸ [Humanizer] chart_json is not a string (type={type(final_chart_json)}), forcing conversion."
            )
            final_chart_json = json.dumps(final_chart_json, ensure_ascii=False)

        if final_chart_json and "[object Object]" in str(final_chart_json):
            logger.error(
                "âš ï¸ [Humanizer] Caught [object Object] in chart_json. (Reporting but NOT clearing as requested for debug)"
            )

        # å°‡åœ–è¡¨è¨­ç‚ºå¾Œç½® (Suffix)ï¼Œé¿å…æ‰“å­—æ©Ÿæ¸²æŸ“æ™‚åœ–è¡¨ä¸€ç›´é‡æ–°è·³å‹•
        suffix = ""
        if final_chart_json:
            suffix = f"\n\n```json\n{final_chart_json}\n```\n"
            logger.info(
                f"ğŸ¨ [Humanizer] Chart JSON prepared as suffix. Len: {len(suffix)}"
            )
        else:
            logger.info("ğŸ¨ [Humanizer] No chart JSON to inject.")

        logger.info("âœï¸ [Humanizer] Starting stream...")

        # å»ºç«‹ä¸€å€‹å…§éƒ¨è®Šæ•¸ä¾†ç²¾ç¢ºè¿½è¹¤ä¸²æµå…§å®¹ï¼Œä¸ä¾è³´ chunk.text
        streamed_text = ""

        async for chunk in self.llm.astream_complete(prompt):
            # Ensure content is string to prevent [object Object]
            delta = chunk.delta
            if delta is not None:
                content_str = str(delta)
                # æŸäº› LLM å¯èƒ½æœƒå›å‚³ç‰©ä»¶æˆ–å¥‡æ€ªçš„å­—ä¸²
                if content_str == "[object Object]":
                    logger.warning(
                        "âš ï¸ [Humanizer] [object Object] detected in LLM stream chunk!"
                    )
                    content_str = ""  # Clear it if it's just the literal object string

                if content_str:
                    # åµæ¸¬éæ¿¾: è‹¥ Chunk åŒ…å« [object Object]
                    if "[object Object]" in content_str:
                        logger.warning(
                            "âš ï¸ [Humanizer] [object Object] detected in LLM stream chunk!"
                        )
                        content_str = content_str.replace(
                            "[object Object]", "(æ•¸æ“šç•°å¸¸)"
                        )

                    ctx.write_event_to_stream(TextChunkEvent(content=content_str))
                    streamed_text += content_str

        # åœ¨ä¸²æµçµæŸå¾Œï¼Œå†æ³¨å…¥åœ–è¡¨ Suffix
        if suffix:
            logger.info(
                f"ğŸ¨ [Humanizer] Appending chart suffix to finished stream... len={len(suffix)}"
            )
            # æä¾›æ›´å¤šå‰ç¶´ç´°ç¯€ä»¥ä¾¿ç¢ºèªæ˜¯å¦æœ‰ [object Object]
            logger.info(f"ğŸ¨ [Humanizer] Suffix sample (200 chars): {suffix[:200]}")
            ctx.write_event_to_stream(TextChunkEvent(content=suffix))

        # æœ€çµ‚å®Œæ•´å›æ‡‰å…§å®¹
        full_response = streamed_text + suffix

        result = {
            "response": full_response,
            "tool_result": ev.data,
            "tool_used": "AnalysisTool",
            "thoughts": ["æ•¸æ“šåˆ†æå®Œæˆ", "æ­£åœ¨ç­‰å¾…çµæœæ¸²æŸ“"],
        }
        return StopEvent(result=result)

    @step
    async def handle_error(self, ctx: Context, ev: ErrorEvent) -> StopEvent:
        """
        [Local Step] éŒ¯èª¤è™•ç†ç«™
        ç•¶ä»»ä½•æ­¥é©Ÿæ‹‹å‡º ErrorEvent æ™‚ï¼Œåœ¨æ­¤æ””æˆªä¸¦å›å‚³éŒ¯èª¤è¨Šæ¯ã€‚
        """
        logger.error(f"âŒ [Error Station] Workflow Error: {ev.error}")
        result = {
            "response": f"æŠ±æ­‰ï¼Œç³»çµ±é‹ä½œå‡ºç¾éŒ¯èª¤ï¼š{ev.error}",
            "tool_result": None,
            "thoughts": ["æµç¨‹ä¸­æ–·", "éŒ¯èª¤è™•ç†å®Œæˆ"],
        }
        return StopEvent(result=result)


# ç‚ºäº†ä¿æŒå‘ä¸Šç›¸å®¹æ€§ï¼Œæˆ‘å€‘ä¿ç•™ LLMAnalysisAgent é¡åˆ¥åï¼Œä½†å…§éƒ¨åˆ‡æ›ç‚º Workflow
class LLMAnalysisAgent:
    def __init__(self, tool_executor: ToolExecutor, **kwargs):
        self.workflow = SigmaAnalysisWorkflow(tool_executor=tool_executor, **kwargs)
        # ä¾ç…§ session_id å„²å­˜è¨˜æ†¶
        self.memories: Dict[str, ChatMemoryBuffer] = {}

    def _get_memory(self, session_id: str) -> ChatMemoryBuffer:
        if session_id not in self.memories:
            self.memories[session_id] = ChatMemoryBuffer.from_defaults(token_limit=8000)
        return self.memories[session_id]

    async def analyze(
        self, session_id: str, file_id: str, user_question: str
    ) -> Dict[str, Any]:
        memory = self._get_memory(session_id)
        history_msgs = memory.get_all()
        history_str = "\n".join([f"{m.role}: {m.content}" for m in history_msgs])

        result = await self.workflow.run(
            query=user_question,
            file_id=file_id,
            session_id=session_id,
            history=history_str,
        )

        # ç´€éŒ„è¨˜æ†¶
        memory.put(ChatMessage(role="user", content=user_question))
        memory.put(ChatMessage(role="assistant", content=result.get("response", "")))

        return result

    async def stream_analyze(
        self, session_id: str, file_id: str, user_question: str, analysis_service=None
    ):
        """
        [Generator] ä¸²æµåˆ†æç”¨æˆ¶å•é¡Œ (Workflow æ¨¡å¼)
        """
        memory = self._get_memory(session_id)
        history_msgs = memory.get_all()
        history_str = "\n".join([f"{m.role}: {m.content}" for m in history_msgs])

        try:
            handler = self.workflow.run(
                query=user_question,
                file_id=file_id,
                session_id=session_id,
                history=history_str,
                timeout=180,
            )

            # ç”¨æ–¼è¿½è¹¤æœ¬æ¬¡å°è©±ç”¢ç”Ÿçš„æ–°å…§å®¹ (ç”¨æ–¼éæ¿¾æ ¡é©—)
            newly_accumulated_text = ""

            async for event in handler.stream_events():
                # æª¢æŸ¥åœæ­¢ä¿¡è™Ÿ
                if analysis_service and analysis_service.is_generation_stopped(
                    session_id
                ):
                    yield json.dumps(
                        {"type": "error", "content": "[ç³»çµ±æç¤º] ç”Ÿæˆå·²æ‰‹å‹•åœæ­¢"},
                        ensure_ascii=False,
                    )
                    return  # ç›´æ¥é€€å‡º

                # Workflow äº‹ä»¶è™•ç†
                event_type = type(event).__name__
                if event_type == "IntentEvent":
                    yield json.dumps(
                        {
                            "type": "thought",
                            "content": f"(Thinking...) æ­£åœ¨åˆ†ææ„åœ–: {event.intent}",
                        },
                        ensure_ascii=False,
                    )
                elif event_type == "AnalysisEvent":
                    yield json.dumps(
                        {
                            "type": "thought",
                            "content": "(Scanning/Retrieving...) æ­£åœ¨å¾åœ°ç«¯è³‡æ–™åº«æª¢ç´¢æ•¸æ“š...",
                        },
                        ensure_ascii=False,
                    )
                elif event_type == "SummarizeEvent":
                    yield json.dumps(
                        {
                            "type": "thought",
                            "content": "(Humanizing...) æ•¸æ“šå½™æ•´å®Œæˆï¼Œæ­£åœ¨å°‡æŠ€è¡“åƒæ•¸è½‰åŒ–ç‚ºæ˜“æ‡‚çš„ä¸­æ–‡åˆ†æå ±å‘Š...",
                        },
                        ensure_ascii=False,
                    )
                elif event_type == "ProgressEvent":
                    yield json.dumps(
                        {"type": "thought", "content": event.msg},
                        ensure_ascii=False,
                    )
                elif event_type == "TranslationEvent":
                    yield json.dumps(
                        {"type": "thought", "content": "æ­£åœ¨æº–å‚™å°è©±å›æ‡‰..."},
                        ensure_ascii=False,
                    )
                elif event_type == "VisualizingEvent":
                    yield json.dumps(
                        {
                            "type": "thought",
                            "content": "(Visualizing...) æ•¸æ“šæª¢ç´¢æˆåŠŸï¼Œæ­£åœ¨ç¹ªè£½åˆ†æåœ–è¡¨...",
                        },
                        ensure_ascii=False,
                    )
                elif event_type == "ConceptExpansionEvent":
                    yield json.dumps(
                        {
                            "type": "thought",
                            "content": f"ğŸ” ç™¼ç¾ç„¡ç›´æ¥åŒ¹é…æ¬„ä½ï¼Œæ­£åœ¨é€²è¡Œèªç¾©æ“´å±•åˆ†æ: {event.original_concept}",
                        },
                        ensure_ascii=False,
                    )
                elif event_type == "TextChunkEvent":
                    # TextChunkEvent ç”± Humanizer è§¸ç™¼
                    content = event.content
                    if not isinstance(content, str):
                        try:
                            content = json.dumps(content, ensure_ascii=False)
                        except Exception:
                            content = str(content)

                    # ç´¯è¨ˆæ ¡é©— (é˜²æ­¢ [object Object] è¢«åˆ‡åˆ†åœ¨å¤šå€‹ chunk)
                    newly_accumulated_text += content
                    if "[object Object]" in newly_accumulated_text:
                        # å¦‚æœæ˜¯æœ¬å€å¡ŠåŒ…å«å®Œæ•´ç‰©ä»¶ï¼Œå‰‡æ¸…ç†
                        if "[object Object]" in content:
                            content = content.replace("[object Object]", "(æ•¸æ“šç•°å¸¸)")
                        else:
                            # å¯èƒ½æ˜¯è·¨ Chunk çš„ [object Object]ï¼Œäº¤ç”±æœ€å¾Œçš„ final_text å–ä»£è™•ç†
                            pass

                    if content:
                        yield json.dumps(
                            {"type": "text_chunk", "content": content},
                            ensure_ascii=False,
                        )
                elif event_type == "ToolCallEvent":
                    yield json.dumps(
                        {
                            "type": "tool_call",
                            "tool": event.tool,
                            "params": event.params,
                        },
                        ensure_ascii=False,
                    )
                elif event_type == "ToolResultEvent":
                    # æŸäº›å·¥å…·å›å‚³çš„æ˜¯ ToolOutput ç‰©ä»¶ï¼Œéœ€æå– content
                    res = event.result
                    if hasattr(res, "content"):
                        res = res.content
                    elif not isinstance(
                        res, (dict, list, str, int, float, bool, type(None))
                    ):
                        res = str(res)

                    # å†æ¬¡å°å­—ä¸²çµæœåšå®‰å…¨æª¢æŸ¥
                    if str(res) == "[object Object]":
                        res = {
                            "status": "success",
                            "message": "å·¥å…·åŸ·è¡Œå®Œæˆï¼Œä½†å›å‚³æ ¼å¼ç•°å¸¸ (Caught [object Object] in Python)",
                        }

                    # é˜²æ­¢ res ç‚º None æˆ–éä¸²è¡ŒåŒ–å°è±¡
                    try:
                        # æ¸¬è©¦åºåˆ—åŒ–
                        json.dumps(res)
                    except Exception:
                        res = str(res)

                    yield json.dumps(
                        {
                            "type": "tool_result",
                            "tool": event.tool,
                            "result": res,
                        },
                        ensure_ascii=False,
                    )

            # ç­‰å¾…æœ€çµ‚çµæœ
            final_result = await handler
            # å¦‚æœ workflow å›å‚³çš„æ˜¯å°è±¡è€Œé dictï¼Œå˜—è©¦è½‰åŒ–
            if not isinstance(final_result, dict):
                final_result = {"response": str(final_result)}

            # ç´€éŒ„è¨˜æ†¶ (ç§»é™¤åœ–è¡¨æ•¸æ“šå¡Šä»¥ç¯€çœ Token ä¸¦é˜²æ­¢ææ¯€æ•¸æ“šæ±¡æŸ“è¨˜æ†¶)
            import re

            final_text = str(final_result.get("response", "")) or ""

            # åµæ¸¬éæ¿¾: è‹¥æœ€çµ‚æ–‡æœ¬åŒ…å« [object Object]
            if "[object Object]" in final_text:
                logger.warning(
                    "âš ï¸ [SSE] [object Object] detected in final response content."
                )

            # ç§»é™¤æ‰€æœ‰ ```json ... ``` å€å¡Š
            memory_safe_text = re.sub(
                r"```json.*?```",
                "\n(äº’å‹•å¼åœ–è¡¨æ•¸æ“šå·²å¾å°è©±è¨˜æ†¶ä¸­ç§»é™¤ä»¥ç¯€çœ Token)\n",
                final_text,
                flags=re.DOTALL,
            )

            # å­˜å…¥è¨˜æ†¶å‰åµæ¸¬
            if "[object Object]" in memory_safe_text:
                logger.warning(
                    "âš ï¸ [Memory] [object Object] found in text being saved to memory."
                )

            memory.put(ChatMessage(role="user", content=user_question))
            memory.put(ChatMessage(role="assistant", content=memory_safe_text))

            yield json.dumps(
                {
                    "type": "response",
                    "content": final_text,
                    "tool_result": final_result.get("tool_result"),
                },
                ensure_ascii=False,
            )
        except Exception as e:
            logger.error(f"âŒ [Stream Analyze] Critical error: {str(e)}", exc_info=True)
            yield json.dumps(
                {
                    "type": "error",
                    "content": f"åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}ã€‚è«‹å˜—è©¦ç¸®çŸ­å•é¡Œæˆ–é‡æ–°é¸æ“‡æª”æ¡ˆã€‚",
                },
                ensure_ascii=False,
            )

    async def clear_session(self, session_id: str = "default"):
        if session_id in self.memories:
            self.memories[session_id].reset()
