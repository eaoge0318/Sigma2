import json
import logging
import asyncio
import httpx
import requests
import re
from typing import List, Dict, Any, Optional, Union
from llama_index.core.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
    ChatMessage,
)
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.llms.callbacks import llm_completion_callback
from llama_index.core.workflow import (
    Event,
    StartEvent,
    StopEvent,
    Workflow,
    step,
    Context,
)
import config
from .types import (
    IntentEvent,
    AnalysisEvent,
    TranslationEvent,
    VisualizingEvent,
    SummarizeEvent,
    ProgressEvent,
    TextChunkEvent,
    ToolCallEvent,
    ToolResultEvent,
    ErrorEvent,
)
from .tools.executor import ToolExecutor
from .analysis_service import AnalysisService

logger = logging.getLogger(__name__)


class CustomOllamaLLM(CustomLLM):
    """
    è‡ªå®šç¾©é«˜æ•ˆ Ollama å°è£ï¼Œæ”¯æŒ httpx ç•°æ­¥è«‹æ±‚
    """

    model_name: str
    api_url: str
    timeout: float = 120.0

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=32768,
            num_output=4096,
            model_name=self.model_name,
        )

    @llm_completion_callback()
    async def acomplete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        """æ ¸å¿ƒéä¸²æµå›å‚³ï¼Œå„ªåŒ–å›æ‡‰é€Ÿåº¦"""
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(self.api_url, json=payload)
                response.raise_for_status()
                result = response.json()
                content = result.get("message", {}).get("content", "")
                return CompletionResponse(text=content)
        except Exception as e:
            logger.error(f"Ollama Async é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(f"ç„¡æ³•éåŒæ­¥é€£ç·šè‡³ Ollama: {str(e)}")

    async def astream_complete(
        self, prompt: str, **kwargs: Any
    ) -> CompletionResponseGen:
        """æ ¸å¿ƒä¸²æµå›å‚³ï¼Œç”¨æ–¼å³æ™‚æ‰“å­—æ©Ÿæ•ˆæœ"""
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": True,
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST", self.api_url, json=payload
                ) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if not line:
                            continue
                        chunk = json.loads(line)
                        if "message" in chunk:
                            content = chunk["message"].get("content", "")
                            yield CompletionResponse(text=content, delta=content)
                        if chunk.get("done"):
                            break
        except Exception as e:
            logger.error(f"Ollama Stream é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(f"ç„¡æ³•ä¸²æµé€£ç·šè‡³ Ollama: {str(e)}")

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        response = requests.post(self.api_url, json=payload, timeout=self.timeout)
        result = response.json()
        return CompletionResponse(text=result.get("message", {}).get("content", ""))

    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        yield self.complete(prompt, **kwargs)


class SigmaAnalysisWorkflow(Workflow):
    """
    Sigma2 æ™ºèƒ½åˆ†æå·¥ä½œæµ (é«˜æ€§èƒ½ä¿®æ­£ç‰ˆ)
    """

    def __init__(
        self,
        tool_executor: ToolExecutor,
        analysis_service: AnalysisService,
        model_name: str = config.LLM_MODEL,
        ollama_api_url: str = config.LLM_API_URL,
        timeout: int = 180,
    ):
        super().__init__(timeout=timeout, verbose=True)
        self.tool_executor = tool_executor
        self.analysis_service = analysis_service
        # ä½¿ç”¨è‡ªå®šç¾©æ¥µé€Ÿå¼•æ“ï¼Œä¸¦å…±äº«å¯¦ä¾‹ä»¥é™ä½é–‹éŠ·
        self.llm = CustomOllamaLLM(model_name=model_name, api_url=ollama_api_url)
        self.llm_json = self.llm  # æ•…æ„ä¸é–‹ JSON æ¨¡å¼ä»¥æå‡ä¸²æµéˆæ´»æ€§

    @step
    async def route_intent(
        self, ctx: Context, ev: StartEvent
    ) -> IntentEvent | ErrorEvent:
        # å‘å‰ç«¯ç™¼é€åˆæ­¥åé¥‹
        ctx.write_event_to_stream(ProgressEvent(msg="â”€ æ­£åœ¨å¿«é€ŸåŒ¹é…æŒ‡ä»¤è·¯å¾‘..."))
        query = getattr(ev, "query", "").strip()
        file_id = getattr(ev, "file_id", None)
        session_id = getattr(ev, "session_id", None)
        history = getattr(ev, "history", "")

        if not query:
            return ErrorEvent(error="æœªæä¾›å•é¡Œ", session_id=session_id)

        # --- æ¥µé€Ÿç¡¬é«”æ±ºç­– (Heuristic Logic) ---
        # 1. å¦‚æœæœ‰ File_ID ä¸”å­—æ•¸ä¸å¤šï¼Œçµ•å¤§å¤šæ•¸éƒ½æ˜¯åˆ†æéœ€æ±‚ï¼Œç›´æ¥é€šé—œ
        if file_id and len(query) < 20:
            intent = "analysis"
        else:
            # 2. é—œéµå­—æ“´å±•éæ¿¾
            analysis_keywords = [
                "åˆ†æ",
                "ç›¸é—œæ€§",
                "ç•°å¸¸",
                "è¶¨å‹¢",
                "æ¬„ä½",
                "æ•¸æ“š",
                "æ‰¾å‡º",
                "é›¢ç¾¤",
                "åˆ†ä½ˆ",
                "å¹¾ç­†",
                "å¤šå°‘",
                "è¡Œæ•¸",
                "ç•«",
                "åœ–",
            ]
            query_lower = query.lower()
            if any(kw in query_lower for kw in analysis_keywords):
                intent = "analysis"
            else:
                # 3. åªæœ‰é•·é›£å¥ä¸”ä¸æ˜ç¢ºæ™‚ï¼Œæ‰å‹•ç”¨ LLM (ä¸”ä½¿ç”¨æ¥µç°¡æŒ‡ä»¤)
                try:
                    prompt = f"Categorize as 'analysis' or 'chat': {query}\nReply only 1 word."
                    response = await self.llm.acomplete(prompt)
                    intent = str(response.text).strip().lower()
                except:
                    intent = "analysis"

        return IntentEvent(
            query=query,
            intent=intent,
            file_id=file_id,
            session_id=session_id,
            history=history,
            mode=ev.mode,
        )

    @step
    async def handle_error(self, ctx: Context, ev: ErrorEvent) -> StopEvent:
        """
        [Local Step] éŒ¯èª¤è™•ç†ç«™
        """
        logger.error(f"[Error Station] Workflow Error: {ev.error}")
        return StopEvent(
            result={
                "response": "æŠ±æ­‰ï¼Œç³»çµ±é‹ä½œå‡ºç¾éŒ¯èª¤ï¼š" + str(ev.error),
                "data": None,
            }
        )

    @step
    async def dispatch_work(
        self, ctx: Context, ev: IntentEvent
    ) -> Union[AnalysisEvent, TranslationEvent, SummarizeEvent]:
        intent = (ev.intent or "").strip().lower()
        query_lower = ev.query.lower()

        # --- é›¶å»¶é²å¿«è»Šé“ (Metadata Fast-Track) ---
        # å¦‚æœåªæ˜¯æƒ³çŸ¥é“æ¬„ä½æ¸…å–®ã€è¡Œæ•¸æˆ–æª”æ¡ˆæ‘˜è¦ï¼Œæ²’å¿…è¦å‹•ç”¨ AI å¤§è…¦
        summary_keywords = [
            "æœ‰å“ªäº›æ¬„ä½",
            "æ¬„ä½æ¸…å–®",
            "æ‰€æœ‰åƒæ•¸",
            "å¹¾ç­†è³‡æ–™",
            "ç¸½è¡Œæ•¸",
            "å¹¾è¡Œ",
            "æ‘˜è¦",
            "æ¦‚æ³",
            "é€™ä»½æª”æ¡ˆ",
            "ç°¡ä»‹",
        ]
        if "analysis" in intent and (any(kw in query_lower for kw in summary_keywords)):
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            if summary:
                ctx.write_event_to_stream(
                    ProgressEvent(
                        msg="â”€ æª¢æ¸¬åˆ°åŸºç¤è³‡è¨Š/æ‘˜è¦æŸ¥è©¢ï¼Œæ­£åœ¨å¾å¿«å–ç›´æ¥æå–ç­”æ¡ˆ..."
                    )
                )
                params_list = summary.get("parameters", [])
                total_rows = summary.get("total_rows", 0)
                categories = summary.get("categories", {})

                # æ§‹å»ºçµæ§‹åŒ–æ‘˜è¦
                cat_info = ", ".join(
                    [f"{k} ({len(v)}å€‹)" for k, v in categories.items()]
                )
                quality_stats = summary.get("quality_stats", {})

                content = (
                    f"### æª”æ¡ˆæ¦‚æ³æ‘˜è¦\n\n"
                    f"æ­¤æª”æ¡ˆå…±æœ‰ **{len(params_list)}** å€‹æ¬„ä½ï¼Œç¸½è¨ˆ **{total_rows}** ç­†æ•¸æ“šã€‚\n"
                    f"**æ•¸æ“šåˆ†é¡**: {cat_info}\n\n"
                )

                # æ–°å¢ï¼šå“è³ªæè¿° (æ›´è©³ç´°ç‰ˆ)
                quality_msg = []
                null_cols = quality_stats.get("null_columns_preview", [])
                const_cols = quality_stats.get("constant_columns_preview", [])
                sparse_cols = quality_stats.get("sparse_columns_preview", [])

                if quality_stats.get("null_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {len(null_cols)} å€‹é«˜ç¼ºå¤±ç‡æ¬„ä½ ({', '.join(null_cols[:3])}...)"
                    )

                if quality_stats.get("sparse_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {quality_stats['sparse_column_count']} å€‹ç¨€ç–æ¬„ä½ (çœŸå€¼æ¯”ä¾‹ < 80%ï¼Œå¦‚ {', '.join(sparse_cols[:3])})"
                    )

                if quality_stats.get("constant_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {quality_stats['constant_column_count']} å€‹å®šå€¼/å…¨é›¶æ¬„ä½ (å¦‚ {', '.join(const_cols[:3])})"
                    )

                if quality_msg:
                    content += (
                        f"**æ•¸æ“šå“è³ªè­¦è¨Š**: \n- " + "\n- ".join(quality_msg) + "\n\n"
                    )
                else:
                    content += f"**æ•¸æ“šå“è³ª**: æ•¸æ“šå®Œæ•´ï¼Œç„¡æ˜é¡¯ç¼ºå¤±æˆ–ç¨€ç–æ¬„ä½ã€‚\n\n"

                content += f"æ‚¨å¯ä»¥å•æˆ‘é—œæ–¼é€™äº›åƒæ•¸çš„è¶¨å‹¢ã€ç•°å¸¸åµæ¸¬æˆ–ç›¸é—œæ€§åˆ†æã€‚"

                if any(
                    kw in query_lower
                    for kw in ["æ¬„ä½", "åƒæ•¸", "æ¸…å–®", "å“ªäº›", "å“ªå…©å€‹", "å“ªå¹¾å€‹"]
                ):
                    content += (
                        f"\n\nå…¨éƒ¨æ¬„ä½æ¸…å–®å¦‚ä¸‹ (å…± {len(params_list)} å€‹):\n"
                        f"{', '.join(params_list)}"
                    )
                    # å¼·åˆ¶æ””æˆªï¼šç”¨æˆ¶è¿½å•ç©ºå€¼æ¬„ä½
                    if (
                        "ç©ºå€¼" in query_lower
                        or "ç¼ºå¤±" in query_lower
                        or "å“ªå…©å€‹" in query_lower
                    ):
                        null_cols = quality_stats.get("null_columns_preview", [])
                        if null_cols:
                            content += f"\n\n### ğŸ”´ é«˜ç¼ºå¤±ç‡æ¬„ä½è©³ç´°æ¸…å–®:\n**{', '.join(null_cols)}**\n(é€™äº›æ¬„ä½å¹¾ä¹ç‚ºç©ºï¼Œå»ºè­°å¿½ç•¥æˆ–æª¢æŸ¥ä¾†æº)"
                return SummarizeEvent(
                    data={"final_decision": content, "all_steps_results": []},
                    query=ev.query,
                    file_id=ev.file_id,
                    session_id=ev.session_id,
                    history=ev.history,
                    mode=ev.mode,
                    row_count=total_rows,
                    col_count=len(params_list),
                )

        if "analysis" in intent:
            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
            )
        return TranslationEvent(
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            mode=ev.mode,
        )

    @step
    async def execute_analysis(
        self, ctx: Context, ev: AnalysisEvent
    ) -> Union[AnalysisEvent, VisualizingEvent, SummarizeEvent]:
        """
        [Local Step] åŸ·è¡Œæ™ºæ…§åˆ†ææ±ºç­– (æ”¯æŒæœ€å¤š 3 æ­¥çš„å¾ªç’°è¨ºæ–·)
        """
        summary = self.tool_executor.analysis_service.load_summary(
            ev.session_id, ev.file_id
        )
        params_list = summary.get("parameters", []) if summary else []
        total_cols = len(params_list)
        total_rows = summary.get("total_rows", 0) if summary else 0

        # åªæœ‰åœ¨ç¬¬ä¸€æ­¥é¡¯ç¤ºè©³ç´°æª¢ç´¢è¨Šæ¯ï¼Œå¾ŒçºŒæ­¥æ•¸é¡¯ç¤ºç°¡æ½”é€²åº¦
        if ev.step_count == 1:
            ctx.write_event_to_stream(
                ProgressEvent(
                    msg=f"â”€ æ­£åœ¨åˆå§‹åŒ–åˆ†æç’°å¢ƒï¼Œé–å®š {total_cols} å€‹åŸå§‹æ¬„ä½..."
                )
            )
        else:
            ctx.write_event_to_stream(ProgressEvent(msg=f"â”€ æ­£åœ¨æº–å‚™å»¶ä¼¸åˆ†æé‚è¼¯..."))
        mappings = summary.get("mappings", {}) if summary else {}

        # --- å®‰å…¨é–¥ï¼šé™åˆ¶æ­¥æ•¸é˜²æ­¢ç„¡çª®è¿´åœˆ ---
        MAX_STEPS = 3
        is_last_step = ev.step_count >= MAX_STEPS

        tool_specs = self.tool_executor.list_tools()
        all_columns = ", ".join(params_list)

        # æ§‹å»ºéå»æ­¥é©Ÿçš„èƒŒæ™¯è³‡è¨Š
        history_context = ""
        if ev.prev_results:
            history_context = "\n### å‰åºåˆ†æçµæœæ‘˜è¦ ###\n" + json.dumps(
                ev.prev_results, ensure_ascii=False
            )

        quality_stats = summary.get("quality_stats", {})
        null_count = quality_stats.get("null_column_count", 0)
        const_count = quality_stats.get("constant_column_count", 0)
        sparse_count = quality_stats.get("sparse_column_count", 0)

        null_preview = ", ".join(quality_stats.get("null_columns_preview", []))
        const_preview = ", ".join(quality_stats.get("constant_columns_preview", []))
        sparse_preview = ", ".join(quality_stats.get("sparse_columns_preview", []))

        quality_info = (
            f"åµæ¸¬åˆ° {null_count} å€‹ç©ºå€¼æ¬„ä½ (ä¾‹: {null_preview})"
            if null_count > 0
            else "ç„¡æ˜é¡¯ç©ºå€¼æ¬„ä½"
        )
        quality_info += (
            f"ï¼›{const_count} å€‹å®šå€¼æ¬„ä½ (ä¾‹: {const_preview})"
            if const_count > 0
            else "ï¼›æ•¸æ“šçš†å…·å‚™è®ŠåŒ–æ€§"
        )
        if sparse_count > 0:
            quality_info += (
                f"ï¼›å¦æœ‰ {sparse_count} å€‹æ¬„ä½çš„çœŸå€¼æ¯”ä¾‹ä½æ–¼ 80% (ä¾‹: {sparse_preview})"
            )

        prompt = (
            f"ä½ æ˜¯ä¸€å€‹æ©Ÿéˆä¸”åš´è¬¹çš„å·¥æ¥­æ•¸æ“šåˆ†æå°ˆå®¶ã€‚ç›®å‰æ˜¯è¨ºæ–·çš„ç¬¬ {ev.step_count} æ­¥ã€‚\n"
            f"åŸºç¤æ•¸æ“šè³‡è¨Š: ç•¶å‰æª”æ¡ˆå…±æœ‰ {total_rows} è¡Œæ•¸æ“šï¼Œ{total_cols} å€‹æ¬„ä½ã€‚\n"
            f"æ•¸æ“šå“è³ªè­¦è¨Š (çµ•å°äº‹å¯¦): {quality_info}\n"
            f"æ‰€æœ‰å¯ç”¨æ¬„ä½: {all_columns}\n"
            "å¯ç”¨å·¥å…·ç®±: " + json.dumps(tool_specs, ensure_ascii=False) + "\n"
            f"ç”¨æˆ¶å•é¡Œ: {ev.query}\n"
            f"{history_context}\n\n"
            "## æ±ºç­–æº–å‰‡ ##\n"
            "1. **æ•ˆç‡è‡³ä¸Š**: å¦‚æœç›®å‰çš„åˆ†æçµæœï¼ˆå¦‚æœ‰ï¼‰å·²ç¶“èƒ½å®Œå…¨å›ç­”ç”¨æˆ¶å•é¡Œï¼Œè«‹ç«‹å³é¸æ“‡ 'finish' å‹•ä½œï¼Œåš´ç¦åŸ·è¡Œä¸å¿…è¦çš„å·¥å…·ã€‚\n"
            "2. **é‚è¼¯é€£è²«**: åªæœ‰åœ¨éœ€è¦æ›´å¤šè­‰æ“šï¼ˆå¦‚ç™¼ç¾ç•°å¸¸å¾Œéœ€è¦æ‰¾åŸå› ï¼‰æ™‚æ‰ä½¿ç”¨å·¥å…·ã€‚\n"
            "3. **å…§å¿ƒç¨ç™½**: è«‹ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€‘åœ¨ monologue ä¸­ç°¡è¿°ä½ çš„è¨ºæ–·ç­–ç•¥ï¼Œä¸è¦åˆ—å‡ºæ‰€æœ‰å·¥å…·ã€‚\n"
            f"4. **æ­¥æ•¸é™åˆ¶**: ç›®å‰å‰©é¤˜ {MAX_STEPS - ev.step_count} æ¬¡å·¥å…·èª¿ç”¨æ©Ÿæœƒã€‚\n"
            f"{'ï¼ï¼ï¼æ³¨æ„ï¼šé€™æ˜¯æœ€å¾Œä¸€æ­¥ï¼Œå¿…é ˆçµè«–å°å‘ï¼Œé¸æ“‡ finish ä¸¦å½™æ•´æ‰€æœ‰ç™¼ç¾ï¼ï¼ï¼' if is_last_step else ''}\n"
            'è¼¸å‡ºå”¯ä¸€ JSON: {"action": "call_tool"|"finish", "tool_name": "...", "params": {...}, "monologue": "..."}'
        )

        response = await self.llm.acomplete(prompt)
        try:
            text = response.text.strip()
            if "```" in text:
                text = text.split("```")[1].replace("json", "").strip()
            decision = json.loads(text)

            action = decision.get("action", "call_tool")
            monologue = decision.get("monologue", "è¨ºæ–·ä¸­...")

            # å‘å‰ç«¯ç™¼é€ AI çš„æ€è€ƒéç¨‹
            ctx.write_event_to_stream(
                ProgressEvent(msg=f"(Step {ev.step_count}) {monologue}")
            )

            if action == "finish" or is_last_step:
                # çµæŸå‰æª¢æŸ¥æ˜¯å¦æœ‰å¯ç¹ªåœ–æ•¸æ“š (get_time_series_data)
                chart_data = None
                for step_res in ev.prev_results:
                    if step_res.get("tool") == "get_time_series_data":
                        res_data = step_res.get("result", {})
                        if (
                            isinstance(res_data, dict)
                            and "data" in res_data
                            and res_data["data"]
                        ):
                            chart_data = res_data
                            break

                total_rows = summary.get("total_rows", 0) if summary else 0
                total_cols = len(params_list) if params_list else 0

                if chart_data:
                    # å„ªå…ˆè·³è½‰åˆ°è¦–è¦ºåŒ–æ­¥é©Ÿï¼Œé€™æœƒç¢ºä¿ UI æ¸²æŸ“åœ–è¡¨
                    return VisualizingEvent(
                        data=chart_data,
                        query=ev.query,
                        file_id=ev.file_id,
                        session_id=ev.session_id,
                        history=ev.history,
                        mode=ev.mode,
                        row_count=chart_data.get("total_points", total_rows),
                        col_count=len(chart_data.get("data", {}).keys()),
                        mappings=mappings,
                    )

                # å¦å‰‡é€²å…¥ç¸½çµå ±å‘Šéšæ®µ
                aggregated_data = {
                    "final_decision": monologue,
                    "all_steps_results": ev.prev_results,
                }
                return SummarizeEvent(
                    data=aggregated_data,
                    query=ev.query,
                    file_id=ev.file_id,
                    session_id=ev.session_id,
                    history=ev.history,
                    mode=ev.mode,
                    row_count=total_rows,
                    col_count=total_cols,
                )

            # å¦å‰‡ï¼ŒåŸ·è¡Œå·¥å…·ä¸¦é€²å…¥ä¸‹ä¸€æ­¥å¾ªç’°
            tool_name = decision.get("tool_name")
            params = decision.get("params", {})
            params["file_id"] = ev.file_id

            # æ ¹æ“šå·¥å…·åæä¾›å‹•æ…‹çš„é€²åº¦æç¤º
            tool_display_names = {
                "get_time_series_data": "æ­£åœ¨è®€å–æ•¸æ“šè¶¨å‹¢...",
                "detect_outliers": "æ­£åœ¨åµæ¸¬ç•°å¸¸é»...",
                "get_top_correlations": "æ­£åœ¨åˆ†æå› ç´ ç›¸é—œæ€§...",
                "analyze_distribution": "æ­£åœ¨åˆ†ææ•¸æ“šåˆ†ä½ˆ...",
            }
            display_msg = tool_display_names.get(tool_name, f"åŸ·è¡Œå·¥å…· {tool_name}...")
            ctx.write_event_to_stream(ProgressEvent(msg=f"(Executing) {display_msg}"))

            ctx.write_event_to_stream(ToolCallEvent(tool=tool_name, params=params))
            result = self.tool_executor.execute_tool(tool_name, params, ev.session_id)
            ctx.write_event_to_stream(ToolResultEvent(tool=tool_name, result=result))

            # å°‡çµæœå­˜å…¥æ­·å²ï¼Œä¸¦éå¢æ­¥æ•¸ç™¼é€ä¸‹ä¸€å€‹ AnalysisEvent (Loop)
            new_results = ev.prev_results + [
                {"step": ev.step_count, "tool": tool_name, "result": result}
            ]

            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                step_count=ev.step_count + 1,
                prev_results=new_results,
            )

        except Exception as e:
            logger.error(f"Analysis loop failed at step {ev.step_count}: {e}")
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            total_rows = summary.get("total_rows", 0) if summary else 0
            total_cols = summary.get("total_columns", 0) if summary else 0

            return SummarizeEvent(
                data=f"åˆ†æéç¨‹é‡åˆ°æŒ‘æˆ°: {str(e)}",
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                row_count=total_rows,
                col_count=total_cols,
            )

    @step
    async def execute_translation(
        self, ctx: Context, ev: TranslationEvent
    ) -> SummarizeEvent:
        """
        [Local Step] åŸ·è¡Œå°è©±æˆ–ç°¡å–®ç¿»è­¯ï¼Œä¸¦æ³¨å…¥åƒæ•¸èƒŒæ™¯è³‡è¨Š
        """
        # å³ä½¿æ˜¯ç°¡å–®å°è©±ï¼Œä¹ŸæŠ“å–åƒæ•¸æ¸…å–®ä½œç‚º AI çš„èƒŒæ™¯çŸ¥è­˜
        summary = self.tool_executor.analysis_service.load_summary(
            ev.session_id, ev.file_id
        )
        params_list = summary.get("parameters", []) if summary else []
        total_rows = summary.get("total_rows", 0) if summary else 0
        total_cols = summary.get("total_columns", 0) if summary else 0

        # å°‡åƒæ•¸æ¸…å–®æ”¾å…¥ dataï¼Œè®“ humanizer è£¡çš„ AI çœ‹å¾—åˆ°
        context_data = {"available_parameters": params_list}

        return SummarizeEvent(
            data=context_data,
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            mode=ev.mode,
            row_count=total_rows,
            col_count=total_cols,
        )

    def _build_programmatic_chart(self, ev: VisualizingEvent) -> Optional[str]:
        """ç©©å®šåœ–è¡¨ç”Ÿæˆé‚è¼¯"""
        try:
            if (
                not isinstance(ev.data, dict)
                or "data" not in ev.data
                or not ev.data["data"]
            ):
                return None
            actual_data = ev.data["data"]
            query_lower = ev.query.lower()

            # --- åˆ¤æ–·åœ–è¡¨é¡å‹ ---
            is_histogram = any(
                kw in query_lower for kw in ["ç›´æ–¹åœ–", "histogram", "åˆ†ä½ˆ", "åˆ†å¸ƒ"]
            )
            is_scatter = any(kw in query_lower for kw in ["æ•£ä½ˆ", "scatter", "ç›¸é—œæ€§"])

            if is_histogram:
                target_col = next(
                    (
                        c
                        for c in actual_data
                        if c not in ["TIME", "Timestamp"]
                        and any(
                            isinstance(v, (int, float)) for v in actual_data[c][:20]
                        )
                    ),
                    None,
                )
                if not target_col:
                    return None
                vals = [
                    v for v in actual_data[target_col] if isinstance(v, (int, float))
                ]
                v_min, v_max = min(vals), max(vals)
                bins = [0] * 15
                step = (v_max - v_min) / 15 or 1
                for v in vals:
                    bins[min(int((v - v_min) / step), 14)] += 1
                chart_obj = {
                    "type": "chart",
                    "chart_type": "bar",
                    "title": f"åˆ†ä½ˆ: {target_col}",
                    "labels": [f"{v_min + i * step:.1f}" for i in range(15)],
                    "datasets": [{"label": "é »æ¬¡", "data": bins}],
                }
            else:
                label_col = next(
                    (c for c in ["TIME", "Timestamp", "Date"] if c in actual_data), None
                )
                labels = (
                    actual_data[label_col]
                    if label_col
                    else list(range(len(next(iter(actual_data.values())))))
                )
                datasets = []
                for col, vals in actual_data.items():
                    if col == label_col:
                        continue
                    datasets.append(
                        {"label": ev.mappings.get(col, col), "data": vals[:100]}
                    )
                chart_obj = {
                    "type": "chart",
                    "chart_type": "line",
                    "labels": labels[:100],
                    "datasets": datasets,
                }

            return json.dumps(chart_obj, ensure_ascii=False)
        except:
            return None

    @step
    async def visualize_data(
        self, ctx: Context, ev: VisualizingEvent
    ) -> SummarizeEvent:
        ctx.write_event_to_stream(
            ProgressEvent(msg="(Visualizing...) æ­£åœ¨ç¹ªè£½åˆ†æåœ–è¡¨...")
        )
        chart_json = self._build_programmatic_chart(ev)
        return SummarizeEvent(
            data=ev.data,
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            chart_json=chart_json,
            row_count=ev.row_count,
            col_count=ev.col_count,
            mode=ev.mode,
        )

    @step
    async def humanizer(self, ctx: Context, ev: SummarizeEvent) -> StopEvent:
        ctx.write_event_to_stream(
            ProgressEvent(msg="(Humanizing...) æ­£åœ¨ç”Ÿæˆæœ€çµ‚åˆ†æå ±å‘Š...")
        )

        # æœ€çµ‚é˜²ç·šï¼šæŠ“å–ç‰©ç†å…¨é‡çµ±è¨ˆèˆ‡æ¬„ä½æ¸…å–®ä½œç‚ºèƒŒæ™¯
        row_count = ev.row_count
        col_count = ev.col_count
        params_list = []
        try:
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            if summary:
                params_list = summary.get("parameters", [])
                if row_count <= 0:
                    row_count = summary.get("total_rows", 0)
                if col_count <= 0:
                    col_count = summary.get("total_columns", 0)
        except Exception:
            pass

        if ev.mode == "fast":
            system_instruction = (
                "ä½œç‚ºæ•¸æ“šå°ˆå®¶ï¼Œè«‹ç²¾ç°¡å›ç­”ã€‚è‹¥æ•¸æ“šä¸­åŒ…å«å…·é«”åˆ†æçµæœï¼ˆå¦‚ç›¸é—œæ€§æ•¸å€¼ã€ç•°å¸¸é»ï¼‰ï¼Œ"
                "è«‹ä¸»å‹•æ‘˜è¦æœ€é‡è¦çš„ç™¼ç¾ï¼Œé¿å…ç©ºæ´çš„å›è¦†ã€‚ä¸å¿…éåº¦å—é™æ–¼ 150 å­—ï¼Œé‡é»æ˜¯ã€ç²¾ç°¡ä¸”æœ‰æ–™ã€ã€‚"
            )
        else:
            system_instruction = (
                "å°ˆæ³¨æ–¼æ·±åº¦æŠ€è¡“åˆ†æå ±å‘Šã€‚è«‹çµåˆæä¾›çš„æ•¸æ“šï¼Œé€²è¡Œå¤šç¶­åº¦çš„çµæœè§£è®€ã€"
                "å˜—è©¦åˆ†æåƒæ•¸é–“å¯èƒ½çš„ç‰©ç†æˆ–é‚è¼¯å› æœé—œä¿‚ï¼Œä¸¦çµ¦å‡ºå…·é«”çš„æ“ä½œæˆ–æ”¹å–„å»ºè­°ã€‚"
            )

        prompt = (
            f"ç³»çµ±æŒ‡ä»¤: {system_instruction}\n"
            f"ç”¨æˆ¶æå•: {ev.query}\n"
            f"æ•¸æ“šæ¦‚æ³ (èƒŒæ™¯): åŒ…å« {row_count} è¡Œèˆ‡ {col_count} å€‹æ¬„ä½ã€‚æ¬„ä½æ¸…å–®é è¦½: {', '.join(params_list[:100])}...\n"
            f"åˆ†ææ•¸æ“š (å…·é«”å…§å®¹): {json.dumps(ev.data, ensure_ascii=False)[:3500]}\n"
            "é‡è¦è¦å‰‡:\n"
            "1. è‹¥æ•¸æ“šä¸­å·²æœ‰åˆ†æå‡ºçš„å…·é«”æŒ‡æ¨™ï¼ˆå¦‚ç›¸é—œä¿‚æ•¸ã€ç•°å¸¸é»ï¼‰ï¼Œå¿…é ˆåœ¨å›è¦†ä¸­å…·é«”å‘ˆç¾ï¼Œä¸è¦åªçµ¦ç± çµ±æè¿°ã€‚\n"
            "2. è‹¥ç”¨æˆ¶è¦æ±‚ã€æ›´å¤šè³‡è¨Šã€ï¼Œè«‹æª¢æŸ¥æ•¸æ“šé è¦½ä¸­æ˜¯å¦é‚„æœ‰æœªæåˆ°çš„ç´°ç¯€ä¸¦é‡‹å‡ºï¼Œè€Œéåå•ç”¨æˆ¶ã€‚\n"
            "3. è«‹ç”¨ç¹é«”ä¸­æ–‡è‡ªç„¶åœ°å›ç­”ã€‚"
        )

        full_text = ""
        suffix = f"\n\n```json\n{ev.chart_json}\n```\n" if ev.chart_json else ""

        # --- çœŸä¸²æµé–‹å§‹ ---
        async for chunk in self.llm.astream_complete(prompt):
            if chunk.delta:
                full_text += chunk.delta
                ctx.write_event_to_stream(TextChunkEvent(content=chunk.delta))

        if suffix:
            ctx.write_event_to_stream(TextChunkEvent(content=suffix))
            full_text += suffix

        return StopEvent(result={"response": full_text, "data": ev.data})


class LLMAnalysisAgent:
    """ç‚ºäº†å…¼å®¹å¤–éƒ¨èª¿ç”¨çš„å°è£é¡"""

    def __init__(
        self, tool_executor: ToolExecutor, analysis_service: AnalysisService, **kwargs
    ):
        self.workflow = SigmaAnalysisWorkflow(tool_executor, analysis_service)
        self.memories = {}

    def _get_memory(self, session_id: str):
        if session_id not in self.memories:
            self.memories[session_id] = ChatMemoryBuffer.from_defaults(
                token_limit=16000
            )
        return self.memories[session_id]

    async def stream_analyze(
        self, session_id: str, file_id: str, user_question: str, analysis_service=None
    ):
        memory = self._get_memory(session_id)
        history_str = "\n".join([f"{m.role}: {m.content}" for m in memory.get_all()])

        handler = self.workflow.run(
            query=user_question,
            file_id=file_id,
            session_id=session_id,
            history=history_str,
        )

        async for event in handler.stream_events():
            event_type = type(event).__name__
            if event_type == "TextChunkEvent":
                yield json.dumps(
                    {"type": "text_chunk", "content": event.content}, ensure_ascii=False
                )
            elif event_type == "ProgressEvent":
                yield json.dumps(
                    {"type": "thought", "content": event.msg}, ensure_ascii=False
                )
            elif event_type == "MonologueEvent":
                yield json.dumps(
                    {"type": "thought", "content": f"æ€è€ƒ: {event.monologue}"},
                    ensure_ascii=False,
                )
            elif event_type == "ToolCallEvent":
                yield json.dumps(
                    {"type": "tool_call", "tool": event.tool, "params": event.params},
                    ensure_ascii=False,
                )
            elif event_type == "ToolResultEvent":
                yield json.dumps(
                    {"type": "tool_result", "tool": event.tool, "result": event.result},
                    ensure_ascii=False,
                )

        final_result = await handler
        memory.put(ChatMessage(role="user", content=user_question))
        memory.put(
            ChatMessage(role="assistant", content=final_result.get("response", ""))
        )

        yield json.dumps(
            {
                "type": "response",
                "content": final_result.get("response"),
                "tool_result": final_result.get("data"),
            },
            ensure_ascii=False,
        )

    async def clear_session(self, session_id: str = "default"):
        if session_id in self.memories:
            self.memories[session_id].reset()
