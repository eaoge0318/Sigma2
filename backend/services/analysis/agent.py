import json
import logging
import asyncio
import httpx
import requests
import re
from typing import List, Dict, Any, Optional, Union
from llama_index.core.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
    ChatMessage,
)
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.llms.callbacks import llm_completion_callback
from llama_index.core.workflow import (
    Event,
    StartEvent,
    StopEvent,
    Workflow,
    step,
    Context,
)
import config
from .analysis_types import (
    IntentEvent,
    AnalysisEvent,
    TranslationEvent,
    VisualizingEvent,
    SummarizeEvent,
    ProgressEvent,
    TextChunkEvent,
    ToolCallEvent,
    ToolResultEvent,
    ErrorEvent,
)
from .tools.executor import ToolExecutor
from .analysis_service import AnalysisService

logger = logging.getLogger(__name__)


class CustomOllamaLLM(CustomLLM):
    """
    è‡ªå®šç¾©é«˜æ•ˆ Ollama å°è£ï¼Œæ”¯æŒ httpx ç•°æ­¥è«‹æ±‚
    """

    model_name: str
    api_url: str
    timeout: float = 120.0

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=32768,
            num_output=4096,
            model_name=self.model_name,
        )

    @llm_completion_callback()
    async def acomplete(
        self, prompt: str, json_mode: bool = False, **kwargs: Any
    ) -> CompletionResponse:
        """æ ¸å¿ƒéä¸²æµå›å‚³ï¼Œæ”¯æŒ JSON æ¨¡å¼"""
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        if json_mode:
            payload["format"] = "json"

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(self.api_url, json=payload)
                response.raise_for_status()
                result = response.json()
                content = result.get("message", {}).get("content", "")
                return CompletionResponse(text=content)
        except Exception as e:
            logger.error(f"Ollama Async é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(f"ç„¡æ³•éåŒæ­¥é€£ç·šè‡³ Ollama: {str(e)}")

    async def astream_complete(
        self, prompt: str, **kwargs: Any
    ) -> CompletionResponseGen:
        """æ ¸å¿ƒä¸²æµå›å‚³ï¼Œç”¨æ–¼å³æ™‚æ‰“å­—æ©Ÿæ•ˆæœ"""
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": True,
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST", self.api_url, json=payload
                ) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if not line:
                            continue
                        chunk = json.loads(line)
                        if "message" in chunk:
                            content = chunk["message"].get("content", "")
                            yield CompletionResponse(text=content, delta=content)
                        if chunk.get("done"):
                            break
        except Exception as e:
            logger.error(f"Ollama Stream é€£ç·šéŒ¯èª¤: {str(e)}")
            raise ConnectionError(f"ç„¡æ³•ä¸²æµé€£ç·šè‡³ Ollama: {str(e)}")

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        response = requests.post(self.api_url, json=payload, timeout=self.timeout)
        result = response.json()
        return CompletionResponse(text=result.get("message", {}).get("content", ""))

    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        yield self.complete(prompt, **kwargs)


class SigmaAnalysisWorkflow(Workflow):
    """
    Sigma2 æ™ºèƒ½åˆ†æå·¥ä½œæµ (é«˜æ€§èƒ½ä¿®æ­£ç‰ˆ)
    """

    def __init__(
        self,
        tool_executor: ToolExecutor,
        analysis_service: AnalysisService,
        model_name: str = config.LLM_MODEL,
        ollama_api_url: str = config.LLM_API_URL,
        timeout: int = 180,
    ):
        super().__init__(timeout=timeout, verbose=True)
        self.tool_executor = tool_executor
        self.analysis_service = analysis_service
        # ä½¿ç”¨è‡ªå®šç¾©æ¥µé€Ÿå¼•æ“ï¼Œä¸¦å…±äº«å¯¦ä¾‹ä»¥é™ä½é–‹éŠ·
        self.llm = CustomOllamaLLM(model_name=model_name, api_url=ollama_api_url)
        self.llm_json = self.llm  # æ•…æ„ä¸é–‹ JSON æ¨¡å¼ä»¥æå‡ä¸²æµéˆæ´»æ€§

    @step
    async def route_intent(
        self, ctx: Context, ev: StartEvent
    ) -> IntentEvent | ErrorEvent:
        # å‘å‰ç«¯ç™¼é€åˆæ­¥åé¥‹
        ctx.write_event_to_stream(ProgressEvent(msg="â”€ æ­£åœ¨å¿«é€ŸåŒ¹é…æŒ‡ä»¤è·¯å¾‘..."))
        query = getattr(ev, "query", "").strip()
        file_id = getattr(ev, "file_id", None)
        session_id = getattr(ev, "session_id", None)
        history = getattr(ev, "history", "")

        if not query:
            return ErrorEvent(error="æœªæä¾›å•é¡Œ", session_id=session_id)

        # --- æ¥µé€Ÿç¡¬é«”æ±ºç­– (Heuristic Logic) ---
        # 1. å¦‚æœæœ‰ File_ID ä¸”å­—æ•¸ä¸å¤šï¼Œçµ•å¤§å¤šæ•¸éƒ½æ˜¯åˆ†æéœ€æ±‚ï¼Œç›´æ¥é€šé—œ
        if file_id and len(query) < 20:
            intent = "analysis"
        else:
            # 2. é—œéµå­—æ“´å±•éæ¿¾
            analysis_keywords = [
                "åˆ†æ",
                "ç›¸é—œæ€§",
                "ç•°å¸¸",
                "è¶¨å‹¢",
                "æ¬„ä½",
                "æ•¸æ“š",
                "æ‰¾å‡º",
                "é›¢ç¾¤",
                "åˆ†ä½ˆ",
                "å¹¾ç­†",
                "å¤šå°‘",
                "è¡Œæ•¸",
                "ç•«",
                "åœ–",
            ]
            query_lower = query.lower()
            if any(kw in query_lower for kw in analysis_keywords):
                intent = "analysis"
            else:
                # 3. åªæœ‰é•·é›£å¥ä¸”ä¸æ˜ç¢ºæ™‚ï¼Œæ‰å‹•ç”¨ LLM (ä¸”ä½¿ç”¨æ¥µç°¡æŒ‡ä»¤)
                try:
                    prompt = f"Categorize as 'analysis' or 'chat': {query}\nReply only 1 word."
                    response = await self.llm.acomplete(prompt)
                    intent = str(response.text).strip().lower()
                except Exception:
                    intent = "analysis"

        return IntentEvent(
            query=query,
            intent=intent,
            file_id=file_id,
            session_id=session_id,
            history=history,
            mode=ev.mode,
        )

    @step
    async def handle_error(self, ctx: Context, ev: ErrorEvent) -> StopEvent:
        """
        [Local Step] éŒ¯èª¤è™•ç†ç«™
        """
        logger.error(f"[Error Station] Workflow Error: {ev.error}")
        return StopEvent(
            result={
                "response": "æŠ±æ­‰ï¼Œç³»çµ±é‹ä½œå‡ºç¾éŒ¯èª¤ï¼š" + str(ev.error),
                "data": None,
            }
        )

    @step
    async def dispatch_work(
        self, ctx: Context, ev: IntentEvent
    ) -> Union[AnalysisEvent, TranslationEvent, SummarizeEvent]:
        intent = (ev.intent or "").strip().lower()
        query_lower = ev.query.lower()

        # --- é›¶å»¶é²å¿«è»Šé“ (Metadata Fast-Track) ---
        # å¦‚æœåªæ˜¯æƒ³çŸ¥é“æ¬„ä½æ¸…å–®ã€è¡Œæ•¸æˆ–æª”æ¡ˆæ‘˜è¦ï¼Œæ²’å¿…è¦å‹•ç”¨ AI å¤§è…¦
        summary_keywords = [
            "æœ‰å“ªäº›æ¬„ä½",
            "æ¬„ä½æ¸…å–®",
            "æ‰€æœ‰åƒæ•¸",
            "å¹¾ç­†è³‡æ–™",
            "ç¸½è¡Œæ•¸",
            "å¹¾è¡Œ",
            "æ‘˜è¦",
            "æ¦‚æ³",
            "é€™ä»½æª”æ¡ˆ",
            "ç°¡ä»‹",
        ]
        if "analysis" in intent and (any(kw in query_lower for kw in summary_keywords)):
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            if summary:
                ctx.write_event_to_stream(
                    ProgressEvent(
                        msg="â”€ æª¢æ¸¬åˆ°åŸºç¤è³‡è¨Š/æ‘˜è¦æŸ¥è©¢ï¼Œæ­£åœ¨å¾å¿«å–ç›´æ¥æå–ç­”æ¡ˆ..."
                    )
                )
                params_list = summary.get("parameters", [])
                total_rows = summary.get("total_rows", 0)
                categories = summary.get("categories", {})

                # æ§‹å»ºçµæ§‹åŒ–æ‘˜è¦
                cat_info = ", ".join(
                    [f"{k} ({len(v)}å€‹)" for k, v in categories.items()]
                )
                quality_stats = summary.get("quality_stats", {})

                content = (
                    f"### æª”æ¡ˆæ¦‚æ³æ‘˜è¦\n\n"
                    f"æ­¤æª”æ¡ˆå…±æœ‰ **{len(params_list)}** å€‹æ¬„ä½ï¼Œç¸½è¨ˆ **{total_rows}** ç­†æ•¸æ“šã€‚\n"
                    f"**æ•¸æ“šåˆ†é¡**: {cat_info}\n\n"
                )

                # æ–°å¢ï¼šå“è³ªæè¿° (æ›´è©³ç´°ç‰ˆ)
                quality_msg = []
                null_cols = quality_stats.get("null_columns_preview", [])
                const_cols = quality_stats.get("constant_columns_preview", [])
                sparse_cols = quality_stats.get("sparse_columns_preview", [])

                if quality_stats.get("null_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {len(null_cols)} å€‹é«˜ç¼ºå¤±ç‡æ¬„ä½ ({', '.join(null_cols[:3])}...)"
                    )

                if quality_stats.get("sparse_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {quality_stats['sparse_column_count']} å€‹ç¨€ç–æ¬„ä½ (çœŸå€¼æ¯”ä¾‹ < 80%ï¼Œå¦‚ {', '.join(sparse_cols[:3])})"
                    )

                if quality_stats.get("constant_column_count", 0) > 0:
                    quality_msg.append(
                        f"æœ‰ {quality_stats['constant_column_count']} å€‹å®šå€¼/å…¨é›¶æ¬„ä½ (å¦‚ {', '.join(const_cols[:3])})"
                    )

                if quality_msg:
                    content += (
                        f"**æ•¸æ“šå“è³ªè­¦è¨Š**: \n- " + "\n- ".join(quality_msg) + "\n\n"
                    )
                else:
                    content += f"**æ•¸æ“šå“è³ª**: æ•¸æ“šå®Œæ•´ï¼Œç„¡æ˜é¡¯ç¼ºå¤±æˆ–ç¨€ç–æ¬„ä½ã€‚\n\n"

                content += f"æ‚¨å¯ä»¥å•æˆ‘é—œæ–¼é€™äº›åƒæ•¸çš„è¶¨å‹¢ã€ç•°å¸¸åµæ¸¬æˆ–ç›¸é—œæ€§åˆ†æã€‚"

                if any(
                    kw in query_lower
                    for kw in [
                        "æ¬„ä½",
                        "åƒæ•¸",
                        "æ¸…å–®",
                        "å“ªäº›",
                        "å“ªå…©å€‹",
                        "å“ªå¹¾å€‹",
                        "é‚£å…©å€‹",
                        "é‚£å¹¾å€‹",
                    ]
                ):
                    content += (
                        f"\n\nå…¨éƒ¨æ¬„ä½æ¸…å–®å¦‚ä¸‹ (å…± {len(params_list)} å€‹):\n"
                        f"{', '.join(params_list)}"
                    )
                    # å¼·åˆ¶æ””æˆªï¼šç”¨æˆ¶è¿½å•ç©ºå€¼æ¬„ä½
                    if (
                        "ç©ºå€¼" in query_lower
                        or "ç¼ºå¤±" in query_lower
                        or "å“ªå…©å€‹" in query_lower
                    ):
                        null_cols = quality_stats.get("null_columns_preview", [])
                        if null_cols:
                            content += f"\n\n### ğŸ”´ é«˜ç¼ºå¤±ç‡æ¬„ä½è©³ç´°æ¸…å–®:\n**{', '.join(null_cols)}**\n(é€™äº›æ¬„ä½å¹¾ä¹ç‚ºç©ºï¼Œå»ºè­°å¿½ç•¥æˆ–æª¢æŸ¥ä¾†æº)"
                return SummarizeEvent(
                    data={"final_decision": content, "all_steps_results": []},
                    query=ev.query,
                    file_id=ev.file_id,
                    session_id=ev.session_id,
                    history=ev.history,
                    mode=ev.mode,
                    row_count=total_rows,
                    col_count=len(params_list),
                    mappings=summary.get("mappings", {}),
                )

        if "analysis" in intent:
            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
            )
        return TranslationEvent(
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            mode=ev.mode,
        )

    @step
    async def execute_analysis(
        self, ctx: Context, ev: AnalysisEvent
    ) -> Union[AnalysisEvent, VisualizingEvent, SummarizeEvent]:
        """
        [Local Step] åŸ·è¡Œæ™ºæ…§åˆ†ææ±ºç­– (æ”¯æŒæœ€å¤š 3 æ­¥çš„å¾ªç’°è¨ºæ–·)
        """
        summary = self.tool_executor.analysis_service.load_summary(
            ev.session_id, ev.file_id
        )
        params_list = summary.get("parameters", []) if summary else []
        total_cols = len(params_list)
        total_rows = summary.get("total_rows", 0) if summary else 0

        # åªæœ‰åœ¨ç¬¬ä¸€æ­¥é¡¯ç¤ºè©³ç´°æª¢ç´¢è¨Šæ¯ï¼Œå¾ŒçºŒæ­¥æ•¸é¡¯ç¤ºç°¡æ½”é€²åº¦
        if ev.step_count == 1:
            ctx.write_event_to_stream(
                ProgressEvent(
                    msg=f"â”€ æ­£åœ¨åˆå§‹åŒ–åˆ†æç’°å¢ƒï¼Œé–å®š {total_cols} å€‹åŸå§‹æ¬„ä½..."
                )
            )
        else:
            ctx.write_event_to_stream(ProgressEvent(msg=f"â”€ æ­£åœ¨æº–å‚™å»¶ä¼¸åˆ†æé‚è¼¯..."))
        mappings = summary.get("mappings", {}) if summary else {}

        # --- å®‰å…¨é–¥ï¼šè§£é–æ·±åº¦è¨ºæ–·åˆ†æ ---
        MAX_STEPS = 30
        is_last_step = ev.step_count >= MAX_STEPS

        tool_specs = self.tool_executor.list_tools()

        # --- æ¬„ä½æ¸…å–®æ™ºæ…§å£“ç¸® ---
        categories = summary.get("categories", {})
        if total_cols > 50:
            cat_summary = "; ".join(
                [f"{k} ({len(v)}å€‹)" for k, v in categories.items()]
            )
            all_columns_display = f"ç”±æ–¼æ¬„ä½çœ¾å¤šï¼Œåƒ…ä¾é¡åˆ¥é¡¯ç¤ºæ‘˜è¦ï¼š{cat_summary}ã€‚è«‹åœ¨éœ€è¦æ™‚ä½¿ç”¨ search_parameters_by_concept æœå°‹å…·é«”æ¬„ä½ã€‚"
        else:
            all_columns_display = ", ".join(params_list)

        # æ§‹å»ºéå»æ­¥é©Ÿçš„èƒŒæ™¯è³‡è¨Š
        history_context = ""
        if ev.prev_results:
            # åƒ…ä¿ç•™é—œéµçµæœï¼Œç¸®æ¸› Token
            simplified_history = []
            for r in ev.prev_results:
                # æˆªæ–·éé•·çš„çµæœä»¥ç¯€çœ Context (ä½†ä¿ç•™é—œéµæ•¸æ“š)
                raw_result = str(r.get("result", ""))
                truncated_result = (
                    raw_result[:500] + "...(ç•¥)"
                    if len(raw_result) > 500
                    else raw_result
                )

                simplified_history.append(
                    {
                        "step": r.get("step"),
                        "tool": r.get("tool"),
                        "params": r.get("params"),
                        "result": truncated_result,  # [NEW] è®“ AI çœ‹è¦‹éå»çš„æ•¸æ“š
                        "monologue": r.get("monologue"),
                    }
                )
            history_context = "\n### å‰åºåˆ†æçµæœæ‘˜è¦ (å«æ•¸æ“šè¨˜æ†¶) ###\n" + json.dumps(
                simplified_history, ensure_ascii=False
            )

        quality_stats = summary.get("quality_stats", {})
        null_count = quality_stats.get("null_column_count", 0)
        const_count = quality_stats.get("constant_column_count", 0)
        sparse_count = quality_stats.get("sparse_column_count", 0)

        # å°‡åƒåœ¾æ•¸æ“šæ¬„ä½æ¨™è¨˜ç‚ºã€Œé»‘åå–®ã€ï¼Œä¸å†æä¾›å…·é«”åç¨±ä»¥å… AI åˆ†å¿ƒ
        quality_info = f"ã€é»‘åå–®è­¦å ±ã€‘åµæ¸¬åˆ° {null_count} å€‹å…¨ç©ºæ¬„ä½èˆ‡ {const_count} å€‹å®šå€¼æ¬„ä½ã€‚é€™äº›æ¬„ä½å·²è¢«ç³»çµ±è‡ªå‹•å‰”é™¤ï¼Œ**çµ•å°ç¦æ­¢æåŠæˆ–åˆ†æå®ƒå€‘**ã€‚"
        if sparse_count > 0:
            quality_info += (
                f" å¦æœ‰ {sparse_count} å€‹æ¬„ä½æ•¸æ“šæ¥µåº¦ç¨€ç–ï¼Œè«‹å„ªå…ˆé¸æ“‡æ•¸æ“šå®Œæ•´çš„åƒæ•¸ã€‚"
            )

        # æ ¹æ“šæ¨¡å¼åˆ‡æ›æŒ‡ä»¤é›†
        mode_instruction = ""
        if ev.mode == "deep":
            mode_instruction = (
                "## ç•¶å‰æ¨¡å¼ï¼šæ·±åº¦è¨ºæ–· (Deep Analysis) ##\n"
                "ä½ çš„ç›®æ¨™æ˜¯é€²è¡Œå…¨æ–¹ä½çš„æ ¹å› åˆ†æã€‚é™¤äº†åŸºç¤çµ±è¨ˆï¼Œè«‹ä¸»å‹•å–„ç”¨ä»¥ä¸‹é«˜éšå·¥å…·ä¾†å¢å¼·èªªæœåŠ›ï¼š\n"
                "1. **åˆ†ä½ˆæª¢å®š (`distribution_shift_test`)**: é€™æ˜¯ä½ çš„æ ¸æ­¦å™¨ã€‚ç•¶ç™¼ç¾æŸåƒæ•¸ç•°å¸¸æ™‚ï¼Œç”¨å®ƒä¾†è­‰æ˜ã€Œåˆ†ä½ˆå½¢ç‹€è®Šäº†ã€ï¼Œè€Œä¸åªæ˜¯æ•¸å€¼è®Šå¤§ã€‚\n"
                "2. **å› æœåˆ†æ (`causal_relationship_analysis`)**: ç”¨å®ƒä¾†æ‰¾ã€Œé ˜é ­ç¾Šã€ã€‚èª°å…ˆè®Šçš„ï¼Ÿ\n"
                "3. **å¤šç¶­åˆ†æ (`hotelling_t2_analysis`)**: ç”¨å®ƒä¾†é‡åŒ–ã€Œæ•´é«”åç§»ã€ã€‚\n"
                "\n"
                "**ã€çµ•å°ç¦æ­¢æ­»å¾ªç’°èˆ‡å›é ­è‰ã€‘**\n"
                "1. **ç¦æ­¢é‡è¤‡**: æª¢æŸ¥ `history`ï¼å¦‚æœä½ å·²ç¶“ç”¨éæŸå€‹å·¥å…·ä¸”åƒæ•¸ç›¸åŒï¼Œ**çµ•å°ç¦æ­¢å†ç”¨ä¸€æ¬¡**ã€‚\n"
                "2. **ç¦æ­¢å€’é€€**: åœ¨ Step 3 ä¹‹å¾Œï¼Œ**åš´ç¦**å‘¼å« `get_data_overview` æˆ– `get_column_info`ã€‚ä½ æ‰‹ä¸Šçš„è­‰æ“šå·²ç¶“å¤ äº†ï¼Œä¸è¦æµªè²»æ­¥æ•¸ã€‚\n"
                "3. **æœæ–·çµæ¡ˆ**: è‹¥å·²åŸ·è¡Œé `compare_data_segments` æˆ– `hotelling_t2`ï¼Œä¸”æ­¥æ•¸ > 4ï¼Œè«‹ç›´æ¥é€²å…¥ `humanizer` çµæ¡ˆã€‚"
            )
        else:
            mode_instruction = (
                "## ç•¶å‰æ¨¡å¼ï¼šå¿«é€Ÿå›æ‡‰ (Quick Response) ##\n"
                "ä½ çš„ç›®æ¨™æ˜¯åœ¨ **2 æ­¥å…§** çµ¦å‡ºç²¾ç¢ºçµè«–ï¼š\n"
                "1. å„ªå…ˆé¸æ“‡æœ€å¼·åŠ›çš„å–®ä¸€è¨ºæ–·å·¥å…· (å¦‚ `hotelling_t2_analysis` æˆ– `compare_data_segments`)ã€‚\n"
                "2. ç²å¾— Top 3 è²¢ç»åº¦å¾Œç«‹å³çµæ¡ˆï¼Œè§£é‡‹æ ¸å¿ƒåŸå› å³å¯ã€‚\n"
            )

        tools_json = json.dumps(tool_specs, ensure_ascii=False)
        prompt_parts = [
            f"ä½ æ˜¯ä¸€å€‹æ©Ÿéˆä¸”åš´è¬¹çš„å·¥æ¥­æ•¸æ“šåˆ†æå°ˆå®¶ã€‚ç›®å‰æ˜¯è¨ºæ–·çš„ç¬¬ {ev.step_count} æ­¥ã€‚",
            f"åŸºç¤æ•¸æ“šè³‡è¨Š: ç•¶å‰æª”æ¡ˆå…±æœ‰ {total_rows} è¡Œæ•¸æ“šï¼Œ{total_cols} å€‹æ¬„ä½ã€‚",
            f"æ•¸æ“šå“è³ªè­¦è¨Š (çµ•å°äº‹å¯¦): {quality_info}",
            f"æ‰€æœ‰å¯ç”¨æ¬„ä½ (éƒ¨åˆ†å±•ç¤º): {all_columns_display}",
            f"å¯ç”¨å·¥å…·ç®±: {tools_json}",
            f"åˆ†æç›®æ¨™ (Query): {ev.query}",
            f"{history_context}",
            "",
            f"{mode_instruction}",
            "## æ ¸å¿ƒåŸå‰‡ (åš´æ ¼åŸ·è¡Œ) ##",
            "1. **åƒæ•¸åç¨±ç²¾ç¢ºæ€§**: çµ•å°ç¦æ­¢ä½¿ç”¨é¡åˆ¥åç¨± (å¦‚ 'PRESSDRY', 'SHAP') ä½œç‚ºåƒæ•¸ã€‚ä½ å¿…é ˆå¾å¯ç”¨æ¬„ä½æ¸…å–®ä¸­é¸æ“‡å…·é«”çš„æ„Ÿæ¸¬å™¨ä»£ç¢¼ (å¦‚ 'PRESSDRY-DCS_A423')ã€‚",
            "2. **æ•¸æ“šèªªè©±**: ä»»ä½•çµè«–éƒ½å¿…é ˆæœ‰æ•¸æ“šæ”¯æŒ (Z-Score, p-value, T2)ã€‚",
            "3. **å°æ¯”åˆ†æ**: ç•°å¸¸æª¢æ¸¬çš„æ ¸å¿ƒåœ¨æ–¼ã€Œç•°å¸¸ vs æ­£å¸¸ã€ã€‚è«‹æ™‚åˆ»ä¿æŒå°æ¯”æ„è­˜ã€‚",
            "4. **é€æ˜ç¨ç™½**: åœ¨ `monologue` ä¸­ç”¨ç¹é«”ä¸­æ–‡è§£é‡‹ä½ çš„æ€è€ƒè·¯å¾‘ã€‚",
            "5. **è¨˜æ†¶é‹ç”¨**: è«‹åƒè€ƒ `å‰åºåˆ†æçµæœæ‘˜è¦` ä¸­çš„ `result` æ•¸æ“šï¼Œä¸è¦é‡è¤‡åŸ·è¡Œå·²çŸ¥çš„åˆ†æã€‚",
            f"6. **ç‹€æ…‹æé†’**: ç›®å‰æ˜¯ç¬¬ {ev.step_count} æ­¥ã€‚",
            'è¼¸å‡ºå”¯ä¸€å€‹ JSON ç‰©ä»¶ï¼Œå¿…é ˆåŒ…å« "action", "tool_name", "params", "monologue" æ¬„ä½ã€‚',
        ]
        prompt = "\n".join(prompt_parts)

        # 1. å‘Šè¨´ç”¨æˆ¶ AI æ­£åœ¨æ ¹æ“šä¸Šä¸€æ­¥çš„çµæœé€²è¡Œæ¨ç†
        ctx.write_event_to_stream(
            ProgressEvent(msg=f"(Step {ev.step_count}) æ­£åœ¨åˆ†æä¸Šä¸‹æ–‡ä¸¦è¦åŠƒä¸‹ä¸€æ­¥...")
        )

        # å¼·åˆ¶é–‹å•Ÿ JSON æ¨¡å¼
        response = await self.llm.acomplete(prompt, json_mode=True)

        try:
            text = response.text.strip()
            # å„ªå…ˆè™•ç† Markdown ä»£ç¢¼å¡Š
            if "```" in text:
                text = text.split("```")[1].replace("json", "").strip()

            # å„ªå…ˆä½¿ç”¨ Regex æå– JSONï¼Œé˜²æ­¢ Ollama è¼¸å‡ºå¤šé¤˜æ–‡å­—æˆ–é‡è¤‡ JSON
            json_match = re.search(r"\{.*\}", response.text, re.DOTALL)
            if json_match:
                decision = json.loads(json_match.group(0))
            else:
                decision = json.loads(response.text)

            # --- ç¡¬æ ¸é˜²æ­»å¾ªç’°é‚è¼¯ ---
            tool_history = [
                (
                    r.get("tool"),
                    str(
                        r.get("params", {}).get("target")
                        or r.get("params", {}).get("parameter")
                    ),
                )
                for r in ev.prev_results
            ]
            current_tool = decision.get("tool_name")
            current_target = str(
                decision.get("params", {}).get("target")
                or decision.get("params", {}).get("parameter")
            )

            # å¦‚æœåŒä¸€å€‹å·¥å…·å°åŒä¸€å€‹ç›®æ¨™é€£çºŒåŸ·è¡Œè¶…é 2 æ¬¡ï¼Œå¼·åˆ¶ä¿®æ­£ç‚º finish
            repeat_count = tool_history.count((current_tool, current_target))
            if repeat_count >= 2:
                logger.warning(
                    f"Detected repeated tool call: {current_tool} on {current_target}. Forcing finish."
                )
                decision = {
                    "action": "finish",
                    "monologue": f"æª¢æ¸¬åˆ°é‡è¤‡åˆ†æè¡Œç‚º (å·²åŸ·è¡Œ {repeat_count} æ¬¡)ï¼Œç³»çµ±å¼·åˆ¶é€²å…¥æœ€çµ‚å½™æ•´éšæ®µä»¥æ‰“ç ´æ­»å¾ªç’°ã€‚",
                }

            action = decision.get("action", "call_tool")
            monologue = decision.get("monologue", "è¨ºæ–·ä¸­...")

            # 2. å‘Šè¨´ç”¨æˆ¶ AI æ±ºå®šè¦åšä»€éº¼ (å…§å¿ƒç¨ç™½)
            ctx.write_event_to_stream(ProgressEvent(msg=f"ğŸ’¡ ç­–ç•¥: {monologue}"))

            if action == "call_tool":
                tool_name = decision.get("tool_name")
                # 3. å‘Šè¨´ç”¨æˆ¶æ­£åœ¨åŸ·è¡Œä»€éº¼è€—æ™‚æ“ä½œ
                ctx.write_event_to_stream(
                    ProgressEvent(msg=f"ğŸ› ï¸ åŸ·è¡Œå·¥å…·: {tool_name} (æ­£åœ¨é‹ç®—æ•¸æ“š...)")
                )

        except Exception as e:
            # ç™¼ç”Ÿè§£æéŒ¯èª¤æ™‚çš„å¼·åˆ¶ä¿®å¾©é‚è¼¯
            logger.error(f"Error parsing LLM decision: {e}. Raw: {response.text}")
            # å¦‚æœæ˜¯ç¬¬ä¸€æ­¥å°±å¤±æ•—ï¼Œå¼·åˆ¶é€²è¡Œæ•¸æ“šæ¦‚è¦½åˆ†æï¼Œä¸å‡†ç›´æ¥ finish
            if ev.step_count == 1:
                action = "call_tool"
                tool_name = "get_data_overview"
                params = {"file_id": ev.file_id}
                monologue = "åŸå®šè¨ˆç•«è§£æå¤±æ•—ï¼Œå¼·åˆ¶å•Ÿå‹•æ•¸æ“šæ¦‚è¦½ä»¥æ‰“ç ´åƒµå±€ã€‚"
                decision = {
                    "action": action,
                    "tool_name": tool_name,
                    "params": params,
                    "monologue": monologue,
                }
            else:
                action = "finish"
                monologue = "é€£çºŒåˆ†æå‡ºç¾è§£æå›°é›£ï¼Œæº–å‚™é€²è¡Œæœ€çµ‚å½™æ•´ã€‚"
                decision = {"action": action, "monologue": monologue}

        if action == "finish" or is_last_step:
            # çµæŸå‰æª¢æŸ¥æ˜¯å¦æœ‰å¯ç¹ªåœ–æ•¸æ“š (get_time_series_data)
            chart_data = None
            for step_res in ev.prev_results:
                if step_res.get("tool") == "get_time_series_data":
                    res_data = step_res.get("result", {})
                    if (
                        isinstance(res_data, dict)
                        and "data" in res_data
                        and res_data["data"]
                    ):
                        chart_data = res_data
                        break

            total_rows = summary.get("total_rows", 0) if summary else 0
            total_cols = len(params_list) if params_list else 0

            if chart_data:
                # å„ªå…ˆè·³è½‰åˆ°è¦–è¦ºåŒ–æ­¥é©Ÿï¼Œé€™æœƒç¢ºä¿ UI æ¸²æŸ“åœ–è¡¨
                return VisualizingEvent(
                    data=chart_data,
                    query=ev.query,
                    file_id=ev.file_id,
                    session_id=ev.session_id,
                    history=ev.history,
                    mode=ev.mode,
                    row_count=chart_data.get("total_points", total_rows),
                    col_count=len(chart_data.get("data", {}).keys()),
                    mappings=mappings,
                )

            # å»ºç«‹é¡¯ç¤ºåç¨±æ˜ å°„
            full_display_mappings = {p: mappings.get(p, p) for p in params_list}

            # å„ªåŒ–ï¼šæå–å…·é«”çš„åˆ†æçµæœæ‘˜è¦ï¼Œé¿å… AI æ··æ·†
            aggregated_data = {
                "monologue_history": monologue,
                "latest_analysis_results": ev.prev_results[-1].get("results")
                if ev.prev_results
                else None,
                "full_tool_history": ev.prev_results,
            }

            return SummarizeEvent(
                data=aggregated_data,
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                row_count=total_rows,
                col_count=total_cols,
                mappings=full_display_mappings,
            )

        # å¦å‰‡ï¼ŒåŸ·è¡Œå·¥å…·ä¸¦é€²å…¥ä¸‹ä¸€æ­¥å¾ªç’°
        tool_name = decision.get("tool_name")
        params = decision.get("params", {})
        if not isinstance(params, dict):
            params = {}
        params["file_id"] = ev.file_id

        try:
            # æ ¹æ“šå·¥å…·åæä¾›å‹•æ…‹çš„é€²åº¦æç¤º
            tool_display_names = {
                "get_time_series_data": "æ­£åœ¨è®€å–æ•¸æ“šè¶¨å‹¢...",
                "detect_outliers": "æ­£åœ¨åµæ¸¬ç•°å¸¸é»...",
                "get_top_correlations": "æ­£åœ¨åˆ†æå› ç´ ç›¸é—œæ€§...",
                "analyze_distribution": "æ­£åœ¨åˆ†ææ•¸æ“šåˆ†ä½ˆ...",
            }
            display_msg = tool_display_names.get(tool_name, f"åŸ·è¡Œå·¥å…· {tool_name}...")

            ctx.write_event_to_stream(
                ProgressEvent(msg=f"(Step {ev.step_count}) {display_msg}")
            )

            # 4. åŸ·è¡Œå·¥å…·
            tool_result = await self.tool_executor.execute_tool(
                tool_name, params, ev.session_id
            )

            # å¼·åˆ¶åŠŸèƒ½ï¼šå°‡è²¢ç»åº¦å‰ä¸‰åå³æ™‚æ¨é€åˆ°èŠå¤©å®¤æ€è€ƒè¦–çª—
            if isinstance(tool_result, dict) and "top_3_summary" in tool_result:
                ctx.write_event_to_stream(
                    ProgressEvent(msg=f"{tool_result['top_3_summary']}")
                )
            # 5. å°‡çµæœå­˜å…¥æ­·å²ï¼Œä¸¦è§¸ç™¼ä¸‹ä¸€æ­¥
            new_step_result = {
                "step": ev.step_count,
                "tool": tool_name,
                "params": params,
                "result": tool_result,
                "monologue": monologue,
            }

            next_history = list(ev.prev_results)
            next_history.append(new_step_result)

            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                step_count=ev.step_count + 1,
                prev_results=next_history,
            )

        except Exception as e:
            logger.error(f"Tool execution failed: {e}")
            # è‹¥å·¥å…·åŸ·è¡Œå¤±æ•—ï¼Œä¸å´©æ½°ï¼Œè€Œæ˜¯å°‡éŒ¯èª¤ä½œç‚ºçµæœå‚³å…¥ä¸‹ä¸€æ­¥
            error_result = {
                "step": ev.step_count,
                "tool": tool_name,
                "params": params,
                "result": {"error": str(e)},
                "monologue": monologue,
            }
            next_history = list(ev.prev_results)
            next_history.append(error_result)

            return AnalysisEvent(
                query=ev.query,
                file_id=ev.file_id,
                session_id=ev.session_id,
                history=ev.history,
                mode=ev.mode,
                step_count=ev.step_count + 1,
                prev_results=next_history,
            )

    @step
    async def execute_translation(
        self, ctx: Context, ev: TranslationEvent
    ) -> SummarizeEvent:
        """
        [Local Step] åŸ·è¡Œå°è©±æˆ–ç°¡å–®ç¿»è­¯ï¼Œä¸¦æ³¨å…¥åƒæ•¸èƒŒæ™¯è³‡è¨Š
        """
        # å³ä½¿æ˜¯ç°¡å–®å°è©±ï¼Œä¹ŸæŠ“å–åƒæ•¸æ¸…å–®ä½œç‚º AI çš„èƒŒæ™¯çŸ¥è­˜
        summary = self.tool_executor.analysis_service.load_summary(
            ev.session_id, ev.file_id
        )
        params_list = summary.get("parameters", []) if summary else []
        total_rows = summary.get("total_rows", 0) if summary else 0
        total_cols = summary.get("total_columns", 0) if summary else 0
        mappings = summary.get("mappings", {}) if summary else {}

        # å»ºç«‹é¡¯ç¤ºåç¨±æ˜ å°„
        full_display_mappings = {p: mappings.get(p, p) for p in params_list}

        # å°‡åƒæ•¸æ¸…å–®æ”¾å…¥ dataï¼Œè®“ humanizer è£¡çš„ AI çœ‹å¾—åˆ°
        context_data = {"available_parameters": params_list}

        return SummarizeEvent(
            data=context_data,
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            mode=ev.mode,
            row_count=total_rows,
            col_count=total_cols,
            mappings=full_display_mappings,
        )

    def _build_programmatic_chart(self, ev: VisualizingEvent) -> Optional[str]:
        """ç©©å®šåœ–è¡¨ç”Ÿæˆé‚è¼¯"""
        try:
            if (
                not isinstance(ev.data, dict)
                or "data" not in ev.data
                or not ev.data["data"]
            ):
                return None
            actual_data = ev.data["data"]
            query_lower = ev.query.lower()

            # --- åˆ¤æ–·åœ–è¡¨é¡å‹ ---
            is_histogram = any(
                kw in query_lower for kw in ["ç›´æ–¹åœ–", "histogram", "åˆ†ä½ˆ", "åˆ†å¸ƒ"]
            )
            is_scatter = any(kw in query_lower for kw in ["æ•£ä½ˆ", "scatter", "ç›¸é—œæ€§"])

            if is_histogram:
                target_col = next(
                    (
                        c
                        for c in actual_data
                        if c not in ["TIME", "Timestamp"]
                        and any(
                            isinstance(v, (int, float)) for v in actual_data[c][:20]
                        )
                    ),
                    None,
                )
                if not target_col:
                    return None
                vals = [
                    v for v in actual_data[target_col] if isinstance(v, (int, float))
                ]
                v_min, v_max = min(vals), max(vals)
                bins = [0] * 15
                step = (v_max - v_min) / 15 or 1
                for v in vals:
                    bins[min(int((v - v_min) / step), 14)] += 1
                chart_obj = {
                    "type": "chart",
                    "chart_type": "bar",
                    "title": f"åˆ†ä½ˆ: {target_col}",
                    "labels": [f"{v_min + i * step:.1f}" for i in range(15)],
                    "datasets": [{"label": "é »æ¬¡", "data": bins}],
                }
            else:
                label_col = next(
                    (c for c in ["TIME", "Timestamp", "Date"] if c in actual_data), None
                )
                labels = (
                    actual_data[label_col]
                    if label_col
                    else list(range(len(next(iter(actual_data.values())))))
                )
                datasets = []
                for col, vals in actual_data.items():
                    if col == label_col:
                        continue
                    datasets.append(
                        {"label": ev.mappings.get(col, col), "data": vals[:100]}
                    )
                chart_obj = {
                    "type": "chart",
                    "chart_type": "line",
                    "labels": labels[:100],
                    "datasets": datasets,
                }

            return json.dumps(chart_obj, ensure_ascii=False)
        except:
            return None

    @step
    async def visualize_data(
        self, ctx: Context, ev: VisualizingEvent
    ) -> SummarizeEvent:
        ctx.write_event_to_stream(
            ProgressEvent(msg="(Visualizing...) æ­£åœ¨ç¹ªè£½åˆ†æåœ–è¡¨...")
        )
        chart_json = self._build_programmatic_chart(ev)
        return SummarizeEvent(
            data=ev.data,
            query=ev.query,
            file_id=ev.file_id,
            session_id=ev.session_id,
            history=ev.history,
            chart_json=chart_json,
            row_count=ev.row_count,
            col_count=ev.col_count,
            mode=ev.mode,
            mappings=ev.mappings,
        )

    @step
    async def humanizer(self, ctx: Context, ev: SummarizeEvent) -> StopEvent:
        ctx.write_event_to_stream(
            ProgressEvent(msg="(Humanizing...) æ­£åœ¨ç”Ÿæˆæœ€çµ‚åˆ†æå ±å‘Š...")
        )

        # æœ€çµ‚é˜²ç·šï¼šæŠ“å–ç‰©ç†å…¨é‡çµ±è¨ˆèˆ‡æ¬„ä½æ¸…å–®ä½œç‚ºèƒŒæ™¯
        row_count = ev.row_count
        col_count = ev.col_count
        params_list = []
        try:
            summary = self.tool_executor.analysis_service.load_summary(
                ev.session_id, ev.file_id
            )
            if summary:
                params_list = summary.get("parameters", [])
                if row_count <= 0:
                    row_count = summary.get("total_rows", 0)
                if col_count <= 0:
                    col_count = summary.get("total_columns", 0)
        except Exception:
            pass

        if ev.mode == "deep":
            mode_instruction = (
                "## ç•¶å‰æ¨¡å¼ï¼šæ·±åº¦è¨ºæ–· (Deep Analysis) ##\n"
                "ä½ çš„ç›®æ¨™æ˜¯é€²è¡Œå…¨æ–¹ä½çš„æ ¹å› åˆ†æã€‚é™¤äº†åŸºç¤çµ±è¨ˆï¼Œè«‹ä¸»å‹•å–„ç”¨ä»¥ä¸‹é«˜éšå·¥å…·ä¾†å¢å¼·èªªæœåŠ›ï¼š\n"
                "1. **åˆ†ä½ˆæª¢å®š (`distribution_shift_test`)**: é€™æ˜¯ä½ çš„æ ¸æ­¦å™¨ã€‚ç•¶ç™¼ç¾æŸåƒæ•¸ç•°å¸¸æ™‚ï¼Œç”¨å®ƒä¾†è­‰æ˜ã€Œåˆ†ä½ˆå½¢ç‹€è®Šäº†ã€ï¼Œè€Œä¸åªæ˜¯æ•¸å€¼è®Šå¤§ã€‚\n"
                "2. **å› æœåˆ†æ (`causal_relationship_analysis`)**: ç”¨å®ƒä¾†æ‰¾ã€Œé ˜é ­ç¾Šã€ã€‚èª°å…ˆè®Šçš„ï¼Ÿ\n"
                "3. **å¤šç¶­åˆ†æ (`hotelling_t2_analysis`)**: ç”¨å®ƒä¾†é‡åŒ–ã€Œæ•´é«”åç§»ã€ã€‚\n"
                "\n"
                "**ã€çµ•å°ç¦æ­¢æ­»å¾ªç’°ã€‘**\n"
                "æª¢æŸ¥ `history`ï¼å¦‚æœä½ å·²ç¶“ç”¨éæŸå€‹å·¥å…· (å¦‚ `get_top_correlations`) ä¸”åƒæ•¸ç›¸åŒï¼Œ**çµ•å°ç¦æ­¢å†ç”¨ä¸€æ¬¡**ã€‚\n"
                "è‹¥åŸºç¤åˆ†æå·²å®Œæˆï¼Œè«‹ç›´æ¥é€²å…¥ `distribution_shift_test` æˆ– `causal_relationship_analysis`ã€‚\n"
                "è‹¥è­‰æ“šå·²å……è¶³ï¼Œè«‹ç›´æ¥ `humanizer` çµæ¡ˆã€‚"
            )
        else:
            mode_instruction = (
                "## ç•¶å‰æ¨¡å¼ï¼šå¿«é€Ÿå›æ‡‰ (Quick Response) ##\n"
                "ä½ çš„ç›®æ¨™æ˜¯åœ¨ **2 æ­¥å…§** çµ¦å‡ºç²¾ç¢ºçµè«–ï¼š\n"
                "1. å„ªå…ˆé¸æ“‡æœ€å¼·åŠ›çš„å–®ä¸€è¨ºæ–·å·¥å…· (å¦‚ `hotelling_t2_analysis` æˆ– `compare_data_segments`)ã€‚\n"
                "2. ç²å¾— Top 3 è²¢ç»åº¦å¾Œç«‹å³çµæ¡ˆï¼Œè§£é‡‹æ ¸å¿ƒåŸå› å³å¯ã€‚\n"
            )

        # å¢åŠ æ•¸æ“šå…§å®¹æ›å…‰é‡ï¼Œæ·±åº¦æ¨¡å¼ä¸‹ä¸æ‡‰éåº¦æˆªæ–·
        data_json = json.dumps(ev.data, ensure_ascii=False)
        data_limit = 20000 if ev.mode == "deep" else 5000

        prompt = (
            f"ç³»çµ±ç‹€æ…‹: {mode_instruction}\n"  # Changed from system_instruction to mode_instruction
            f"ç”¨æˆ¶æå•: {ev.query}\n"
            f"æ•¸æ“šæ¦‚æ³ (èƒŒæ™¯): åŒ…å« {row_count} è¡Œèˆ‡ {col_count} å€‹æ¬„ä½ã€‚\n"
            f"åƒæ•¸é¡¯ç¤ºåç¨±å°æ‡‰ (Mapping): {json.dumps(ev.mappings, ensure_ascii=False)}\n"
            f"åˆ†ææ•¸æ“š (å…¨é‡æ­·å²ç²¾è¯): {data_json[:data_limit]}\n"
            "## ç”Ÿæˆæº–å‰‡ (æ•¸å€¼å…ˆè¡Œ + è§£é‡‹éš¨å¾Œ) ##\n"
            "1. **åš´ç¦ç©ºæ´æè¿°**: å¿…é ˆå…ˆå¼•ç”¨æ•¸æ“š (p-value, T2, Z-Score) ä½œç‚ºé–‹é ­ã€‚\n"
            "2. **ç¿»è­¯ç‰©ç†æ„ç¾©**: è§£é‡‹æ™‚è¦å…·é«”å°æ‡‰åˆ°è¨­å‚™ç‹€æ…‹ (å¦‚ï¼šé¦¬é”è€—æã€é…æ–¹åˆ‡æ›ã€å‚³æ„Ÿå™¨æ¼‚ç§»)ã€‚\n"
            "3. **å°ˆæ¥­å£å»**: ç¹é«”ä¸­æ–‡ï¼Œå°ˆæ¥­å·¥æ¥­è¨ºæ–·å·¥ç¨‹å¸«å£å»ã€‚"
        )

        full_text = ""
        suffix = f"\n\n```json\n{ev.chart_json}\n```\n" if ev.chart_json else ""

        # --- çœŸä¸²æµé–‹å§‹ ---
        async for chunk in self.llm.astream_complete(prompt):
            if chunk.delta:
                full_text += chunk.delta
                ctx.write_event_to_stream(TextChunkEvent(content=chunk.delta))

        if suffix:
            ctx.write_event_to_stream(TextChunkEvent(content=suffix))
            full_text += suffix

        return StopEvent(result={"response": full_text, "data": ev.data})


class LLMAnalysisAgent:
    """ç‚ºäº†å…¼å®¹å¤–éƒ¨èª¿ç”¨çš„å°è£é¡"""

    def __init__(
        self, tool_executor: ToolExecutor, analysis_service: AnalysisService, **kwargs
    ):
        self.workflow = SigmaAnalysisWorkflow(tool_executor, analysis_service)
        self.memories = {}

    def _get_memory(self, session_id: str):
        if session_id not in self.memories:
            self.memories[session_id] = ChatMemoryBuffer.from_defaults(
                token_limit=16000
            )
        return self.memories[session_id]

    async def stream_analyze(
        self, session_id: str, file_id: str, user_question: str, analysis_service=None
    ):
        memory = self._get_memory(session_id)
        history_str = "\n".join([f"{m.role}: {m.content}" for m in memory.get_all()])

        handler = self.workflow.run(
            query=user_question,
            file_id=file_id,
            session_id=session_id,
            history=history_str,
        )

        async for event in handler.stream_events():
            event_type = type(event).__name__
            if event_type == "TextChunkEvent":
                yield json.dumps(
                    {"type": "text_chunk", "content": event.content}, ensure_ascii=False
                )
            elif event_type == "ProgressEvent":
                yield json.dumps(
                    {"type": "thought", "content": event.msg}, ensure_ascii=False
                )
            elif event_type == "MonologueEvent":
                yield json.dumps(
                    {"type": "thought", "content": f"æ€è€ƒ: {event.monologue}"},
                    ensure_ascii=False,
                )
            elif event_type == "ToolCallEvent":
                yield json.dumps(
                    {"type": "tool_call", "tool": event.tool, "params": event.params},
                    ensure_ascii=False,
                )
            elif event_type == "ToolResultEvent":
                yield json.dumps(
                    {"type": "tool_result", "tool": event.tool, "result": event.result},
                    ensure_ascii=False,
                )

        final_result = await handler
        memory.put(ChatMessage(role="user", content=user_question))
        memory.put(
            ChatMessage(role="assistant", content=final_result.get("response", ""))
        )

        yield json.dumps(
            {
                "type": "response",
                "content": final_result.get("response"),
                "tool_result": final_result.get("data"),
            },
            ensure_ascii=False,
        )

    async def clear_session(self, session_id: str = "default"):
        if session_id in self.memories:
            self.memories[session_id].reset()
