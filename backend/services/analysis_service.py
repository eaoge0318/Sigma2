"""
åˆ†ææœå‹™
è² è²¬é€²éšåˆ†æã€æ¨¡å‹è¨“ç·´ç­‰æ¥­å‹™é‚è¼¯
"""

import os
import json
import uuid
import pandas as pd
import numpy as np
from typing import Dict, Any, List
from fastapi import HTTPException
from backend.models.request_models import (
    AdvancedAnalysisRequest,
    SaveFileRequest,
    TrainRequest,
    QuickAnalysisRequest,
)

import config as app_config


class AnalysisService:
    """åˆ†ææœå‹™ï¼Œè™•ç†æ•¸æ“šåˆ†æç›¸é—œçš„æ¥­å‹™é‚è¼¯"""

    def __init__(self, base_upload_dir: str = None):
        self.base_upload_dir = base_upload_dir or app_config.BASE_STORAGE_DIR

    def get_user_upload_dir(self, session_id: str) -> str:
        """å–å¾—ç‰¹å®šä½¿ç”¨è€…çš„ä¸Šå‚³ç›®éŒ„ (Helper)"""
        safe_session_id = "".join(
            [c for c in session_id if c.isalnum() or c in ("-", "_")]
        ).strip()
        if not safe_session_id:
            safe_session_id = "default"
        return os.path.join(self.base_upload_dir, safe_session_id, "uploads")

    async def advanced_analysis(
        self, req: AdvancedAnalysisRequest, session_id: str = "default"
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œé€²éšåˆ†æ

        Args:
            req: åˆ†æè«‹æ±‚
            session_id: ä½¿ç”¨è€… Session ID

        Returns:
            åˆ†æçµæœ
        """
        # print(
        #     f"DEBUG: Advanced Analysis requested for {req.filename}, "
        #     f"target={req.target_column}, algo={req.algorithm}, session={session_id}"
        # )
        try:
            upload_dir = self.get_user_upload_dir(session_id)
            file_path = os.path.join(upload_dir, req.filename)
            if not os.path.exists(file_path):
                # print(f"DEBUG: File not found at {file_path}")
                raise HTTPException(404, detail=f"File not found: {req.filename}")

            df = pd.read_csv(file_path)

            if req.target_column not in df.columns:
                raise HTTPException(400, detail="Target column not in file")

            # åƒ…å°æ•¸å€¼æ¬„ä½é€²è¡Œåˆ†æ
            numeric_df = df.select_dtypes(include=[np.number])
            if req.target_column not in numeric_df.columns:
                # å˜—è©¦è½‰æ›
                try:
                    df[req.target_column] = pd.to_numeric(
                        df[req.target_column], errors="coerce"
                    )
                    numeric_df = df.select_dtypes(include=[np.number])
                except Exception as e:
                    raise HTTPException(
                        400,
                        detail=f"Target '{req.target_column}' is not numeric: {str(e)}",
                    )

            # æº–å‚™ X and y
            y = numeric_df[req.target_column].ffill().bfill()
            X = numeric_df.drop(columns=[req.target_column]).fillna(0)

            results = []

            if req.algorithm == "correlation":
                # è¨ˆç®—çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸
                raw_corrs = X.corrwith(y)
                sorted_cols = raw_corrs.abs().sort_values(ascending=False).index

                for col in sorted_cols:
                    val = raw_corrs[col]
                    if not np.isnan(val):
                        results.append({"col": str(col), "score": float(val)})

            elif req.algorithm == "xgboost":
                import xgboost as xgb
                import shap

                # è¨“ç·´è¼•é‡ç´šæ¨¡å‹
                model = xgb.XGBRegressor(n_estimators=100, max_depth=4, random_state=42)
                model.fit(X, y)

                # è¨ˆç®— SHAP å€¼
                explainer = shap.TreeExplainer(model)
                shap_values = explainer.shap_values(X)

                if isinstance(shap_values, list):
                    shap_values = shap_values[0]

                # å¹³å‡çµ•å° SHAP å€¼
                vals = np.abs(shap_values).mean(axis=0)
                feature_importance = pd.Series(vals, index=X.columns).sort_values(
                    ascending=False
                )

                for col, val in feature_importance.items():
                    results.append({"col": str(col), "score": float(val)})

            else:
                raise HTTPException(400, detail="Unknown algorithm")

            return {"status": "success", "results": results}

        except HTTPException:
            raise
        except Exception as e:
            import traceback

            traceback.print_exc()
            raise HTTPException(500, detail=f"Analysis failed: {str(e)}")

    async def save_filtered_file(
        self, req: SaveFileRequest, session_id: str = "default"
    ) -> Dict[str, str]:
        """
        å„²å­˜éæ¿¾å¾Œçš„æª”æ¡ˆ

        Args:
            req: å„²å­˜è«‹æ±‚
            session_id: ä½¿ç”¨è€… Session ID

        Returns:
            å„²å­˜çµæœ
        """
        try:
            # ç¢ºä¿æª”åå®‰å…¨
            safe_filename = "".join(
                [c for c in req.filename if c.isalnum() or c in (" ", ".", "_", "-")]
            ).strip()
            if not safe_filename.endswith(".csv"):
                safe_filename += ".csv"

            upload_dir = self.get_user_upload_dir(session_id)
            file_path = os.path.join(upload_dir, safe_filename)

            # ä½¿ç”¨ pandas å¯«å…¥ CSV
            df = pd.DataFrame(req.rows, columns=req.headers)
            df.to_csv(file_path, index=False, encoding="utf-8-sig")

            return {
                "status": "success",
                "filename": safe_filename,
                "message": f"æª”æ¡ˆ {safe_filename} å·²å„²å­˜è‡³æª”æ¡ˆç®¡ç†",
            }
        except Exception as e:
            raise HTTPException(500, detail=f"Save failed: {str(e)}")

    async def train_model(
        self, req: TrainRequest, session_id: str = "default"
    ) -> Dict[str, Any]:
        """
        ç•°æ­¥è§¸ç™¼æ¨¡å‹è¨“ç·´ä»»å‹™ï¼Œä¿å­˜é…ç½®ä¸¦èƒŒæ™¯å•Ÿå‹•å¼•æ“ (å¤šç§Ÿæˆ¶éš”é›¢ç‰ˆ)ã€‚
        """
        try:
            import json
            import subprocess
            import sys
            from datetime import datetime
            from backend.dependencies import get_file_service

            file_service = get_file_service()

            # 1. æº–å‚™éš”é›¢ç›®éŒ„
            config_dir = file_service.get_user_path(session_id, "configs")
            log_dir = file_service.get_user_path(session_id, "logs")
            bundles_dir = file_service.get_user_path(session_id, "bundles")

            # 2. ç”Ÿæˆé…ç½®èˆ‡ä½œæ¥­æ¨™è¨˜
            job_id = f"job_{uuid.uuid4().hex[:8]}"
            config_filename = f"{job_id}.json"
            json_path = os.path.join(config_dir, config_filename)

            full_config = dict(req.config)

            # é—œéµä¿®æ­£ï¼šå°‡ç›¸å°æª”åè½‰æ›ç‚ºçµ•å°è·¯å¾‘ï¼Œç¢ºä¿èƒŒæ™¯å¼•æ“èƒ½æ‰¾åˆ°æª”æ¡ˆ
            raw_filename = full_config.get("filename")
            target_abs_path = None
            if raw_filename:
                # å–å¾—ä½¿ç”¨è€…ä¸Šå‚³ç›®éŒ„
                user_dir = file_service.get_user_upload_dir(session_id)
                full_path = os.path.join(user_dir, raw_filename)

                # å¦‚æœè©² session ç›®éŒ„æ‰¾ä¸åˆ°ï¼Œå˜—è©¦ default ç›®éŒ„
                if not os.path.exists(full_path) and session_id != "default":
                    default_dir = file_service.get_user_upload_dir("default")
                    full_path = os.path.join(default_dir, raw_filename)

                if os.path.exists(full_path):
                    target_abs_path = os.path.abspath(full_path)
                    full_config["data_full_path"] = target_abs_path
                else:
                    print(f"Warning: Training data file not found at {full_path}")

            # è‡ªå‹•æ ¡æ­£è³‡æ–™ç­†æ•¸
            if target_abs_path and (
                full_config.get("rows") == "æœªçŸ¥" or not full_config.get("rows")
            ):
                try:
                    with open(
                        target_abs_path, "r", encoding="utf-8", errors="ignore"
                    ) as f:
                        row_count = sum(1 for _ in f) - 1
                    full_config["rows"] = str(max(0, row_count))
                except Exception:
                    full_config["rows"] = "æœªçŸ¥"

            full_config["job_id"] = job_id
            full_config["session_id"] = session_id  # è¨˜éŒ„æ‰€å±¬ session
            full_config["created_at"] = datetime.now().strftime("%Y/%m/%d %H:%M:%S")
            full_config["status"] = "training"
            full_config["bundles_dir"] = bundles_dir  # å‘ŠçŸ¥å¼•æ“æ¨¡å‹å­˜æ”¾åœ°

            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(full_config, f, ensure_ascii=False, indent=4)

            log_file_path = os.path.join(log_dir, f"{job_id}.log")

            # 3. æ ¹æ“šä»»å‹™é¡å‹èˆ‡é…ç½®è‡ªå‹•åˆ¤å®šåŸ·è¡Œå¼•æ“
            mission_type = full_config.get("missionType") or full_config.get(
                "type", "supervised"
            )
            has_rl = len(full_config.get("actions", [])) > 0
            has_ml = len(full_config.get("features", [])) > 0

            if has_rl and has_ml:
                script_name = os.path.join("engines", "joint_training_orchestrator.py")
            elif mission_type == "rl" or has_rl:
                script_name = os.path.join("engines", "engine_strategy.py")
            else:
                script_name = os.path.join("engines", "engine_prediction.py")

            script_path = os.path.abspath(script_name)

            # å•Ÿå‹•å­é€²ç¨‹ä¸¦å°‡è¼¸å‡ºå®šå‘åˆ°éš”é›¢å¾Œçš„ log
            try:
                log_file = open(log_file_path, "ab")
                proc = subprocess.Popen(
                    [sys.executable, script_path, json_path],
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    creationflags=subprocess.CREATE_NO_WINDOW if os.name == "nt" else 0,
                )
                full_config["pid"] = proc.pid
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(full_config, f, ensure_ascii=False, indent=4)
            except Exception as e:
                full_config["status"] = "failed"
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(full_config, f, ensure_ascii=False, indent=4)
                return {"status": "error", "message": f"å•Ÿå‹•è¨“ç·´å¤±æ•—: {str(e)}"}

            display_name = full_config.get("modelName") or full_config.get(
                "model_name", "Unnamed"
            )
            return {
                "status": "success",
                "message": f"Successfully started {mission_type} training for {display_name}",
                "job_id": job_id,
            }
        except Exception as e:
            raise HTTPException(500, detail=f"è¨“ç·´ç·¨æ’å¤±æ•—: {str(e)}")

    async def list_models(self, session_id: str = "default") -> List[Dict[str, Any]]:
        """
        åˆ—è¡¨åŒ–é¡¯ç¤ºç‰¹å®šä½¿ç”¨è€…çš„æ¨¡å‹å·¥ä½œã€‚
        """
        try:
            from backend.dependencies import get_file_service

            file_service = get_file_service()
            config_dir = file_service.get_user_path(session_id, "configs")

            if not os.path.exists(config_dir):
                return []

            models = []
            for fname in os.listdir(config_dir):
                if fname.endswith(".json"):
                    fpath = os.path.join(config_dir, fname)
                    try:
                        with open(fpath, "r", encoding="utf-8") as f:
                            m_data = json.load(f)

                            # é—œéµå¢å¼·ï¼šæª¢æŸ¥ã€Œè¨“ç·´ä¸­ã€çš„æ¨¡å‹é€²ç¨‹æ˜¯å¦é‚„æ´»è‘—
                            if m_data.get("status") == "training" and m_data.get("pid"):
                                pid = m_data.get("pid")
                                is_alive = self._check_process_alive(pid)

                                if not is_alive:
                                    m_data["status"] = "failed"
                                    m_data["error"] = "Process unexpectedly terminated."
                                    with open(fpath, "w", encoding="utf-8") as fw:
                                        json.dump(
                                            m_data, fw, ensure_ascii=False, indent=4
                                        )
                            models.append(m_data)
                    except Exception:
                        continue
            return sorted(models, key=lambda x: x.get("created_at", ""), reverse=True)
        except Exception as e:
            print(f"Error listing models: {str(e)}")
            return []

    def _check_process_alive(self, pid: int) -> bool:
        """æª¢æŸ¥é€²ç¨‹æ˜¯å¦é‚„æ´»è‘— (Helper)"""
        if os.name == "nt":
            try:
                import subprocess as sp

                output = sp.check_output(
                    f'tasklist /FI "PID eq {pid}" /NH', shell=True
                ).decode("gbk", errors="ignore")
                return str(pid) in output
            except Exception:
                return False
        else:
            try:
                os.kill(pid, 0)
                return True
            except OSError:
                return False

    async def get_training_log(self, job_id: str, session_id: str = "default") -> str:
        """ç²å–ç‰¹å®šä»»å‹™çš„è¨“ç·´æ—¥èªŒå…§å®¹ (éš”é›¢ç‰ˆ)"""
        from backend.dependencies import get_file_service

        job_id = "".join(c for c in job_id if c.isalnum() or c == "_")
        log_dir = get_file_service().get_user_path(session_id, "logs")
        log_path = os.path.join(log_dir, f"{job_id}.log")

        if not os.path.exists(log_path):
            return "å°šæœªç”Ÿæˆæ—¥èªŒæˆ–ä»»å‹™ä¸å­˜åœ¨ã€‚"

        try:
            encodings = ["utf-8", "gbk", "big5", "cp950", "utf-16"]
            content = None
            with open(log_path, "rb") as f:
                raw_data = f.read()
                from collections import deque

                lines = deque(raw_data.splitlines(), maxlen=2000)
                raw_data_tail = b"\n".join(lines)

            for enc in encodings:
                try:
                    content = raw_data_tail.decode(enc, errors="replace")
                    break
                except Exception:
                    continue
            return content or "æ­£åœ¨åˆå§‹åŒ–è¨“ç·´ç³»çµ±ä¸¦ç­‰å¾…æ—¥èªŒè¼¸å‡º..."
        except Exception as e:
            return f"è®€å–æ—¥èªŒå‡ºéŒ¯: {str(e)}"

    async def delete_model(
        self, job_id: str, session_id: str = "default"
    ) -> Dict[str, Any]:
        """åˆªé™¤ç‰¹å®šæ¨¡å‹ä»»å‹™ (éš”é›¢ç‰ˆ)"""
        from backend.dependencies import get_file_service
        import signal

        file_service = get_file_service()

        job_id = "".join(c for c in job_id if c.isalnum() or c == "_")
        config_path = os.path.join(
            file_service.get_user_path(session_id, "configs"), f"{job_id}.json"
        )
        log_path = os.path.join(
            file_service.get_user_path(session_id, "logs"), f"{job_id}.log"
        )

        try:
            if os.path.exists(config_path):
                with open(config_path, "r", encoding="utf-8") as f:
                    m_data = json.load(f)
                    pid = m_data.get("pid")
                    if pid and m_data.get("status") == "training":
                        if os.name == "nt":
                            os.system(f"taskkill /F /T /PID {pid}")
                        else:
                            os.kill(pid, signal.SIGTERM)
        except Exception:
            pass

        deleted = False
        try:
            if os.path.exists(config_path):
                os.remove(config_path)
                deleted = True
            if os.path.exists(log_path):
                os.remove(log_path)
                deleted = True
            if deleted:
                return {"status": "success", "message": f"æ¨¡å‹ä»»å‹™ {job_id} å·²åˆªé™¤"}
            return {"status": "error", "message": "æ‰¾ä¸åˆ°æª”æ¡ˆ"}
        except Exception as e:
            return {"status": "error", "message": f"åˆªé™¤å¤±æ•—: {str(e)}"}

    async def stop_model(
        self, job_id: str, session_id: str = "default"
    ) -> Dict[str, Any]:
        """åœæ­¢è¨“ç·´é€²ç¨‹ (éš”é›¢ç‰ˆ)"""
        from backend.dependencies import get_file_service
        import signal

        file_service = get_file_service()

        job_id = "".join(c for c in job_id if c.isalnum() or c == "_")
        config_path = os.path.join(
            file_service.get_user_path(session_id, "configs"), f"{job_id}.json"
        )

        if not os.path.exists(config_path):
            return {"status": "error", "message": "æ‰¾ä¸åˆ°è©²æ¨¡å‹é…ç½®"}

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                m_data = json.load(f)

            pid = m_data.get("pid")
            if pid and m_data.get("status") == "training":
                if os.name == "nt":
                    os.system(f"taskkill /F /T /PID {pid}")
                else:
                    os.kill(pid, signal.SIGTERM)

                m_data["status"] = "failed"
                m_data["error"] = "Manually stopped by user."
                with open(config_path, "w", encoding="utf-8") as fw:
                    json.dump(m_data, fw, ensure_ascii=False, indent=4)
                return {"status": "success", "message": f"ä»»å‹™ {job_id} å·²å¼·åˆ¶åœæ­¢"}
            return {"status": "error", "message": "ä»»å‹™å·²çµæŸæˆ–ç„¡é‹è¡Œä¸­çš„é€²ç¨‹"}
        except Exception as e:
            return {"status": "error", "message": f"åœæ­¢å¤±æ•—: {str(e)}"}

    async def quick_analysis(
        self, req: QuickAnalysisRequest, session_id: str = "default"
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œå¿«é€Ÿåˆ†æï¼Œç”Ÿæˆæ•¸æ“šæ‘˜è¦ã€‚å„ªå…ˆç”±ç£ç¢Ÿè®€å–å…¨é‡æ•¸æ“šä»¥ç²å–æº–ç¢ºçµ±è¨ˆã€‚
        """
        try:
            # ç¢ºä¿æª”æ¡ˆè·¯å¾‘æ­£ç¢º
            filename = os.path.basename(req.filename)
            upload_dir = self.get_user_upload_dir(session_id)
            file_path = os.path.join(upload_dir, filename)

            # Fallback: Check default directory if file not found in current session
            if not os.path.exists(file_path) and session_id != "default":
                default_dir = self.get_user_upload_dir("default")
                default_path = os.path.join(default_dir, filename)
                if os.path.exists(default_path):
                    # print(
                    #     f"DEBUG: [QuickAnalysis] File found in default dir: {default_path}"
                    # )
                    file_path = default_path

            df = None
            data_source = f"å‰ç«¯å‚³é€æ•¸æ“š (ç´„ {len(req.rows)} ç­†)"

            if os.path.exists(file_path):
                try:
                    # è®€å–å…¨é‡æ•¸æ“š
                    df = pd.read_csv(file_path)
                    data_source = f"ä¼ºæœå™¨åŸå§‹æª”æ¡ˆ ({len(df)} ç­†)"
                    # print(f"DEBUG: [QuickAnalysis] Loading full file: {file_path}")

                    # å¥—ç”¨éæ¿¾å™¨
                    if req.filters and len(req.filters) > 0:
                        for f in req.filters:
                            ftype = f.get("type", "text")

                            # é‡å° 'indices' ç‰¹æ®Šè™•ç†ï¼šä½¿ç”¨ index è€Œé ilocï¼Œç¢ºä¿ç²¾æº–åŒ¹é…åŸå§‹è¡Œè™Ÿ
                            if ftype in ["indices", "exclude_indices"]:
                                indices = f.get("indices", [])
                                if ftype == "indices":
                                    df = df[df.index.isin(indices)]
                                else:
                                    df = df[~df.index.isin(indices)]
                                continue

                            col_name = f.get("colName")
                            col_idx = f.get("colIdx")
                            actual_col = col_name
                            if (
                                actual_col not in df.columns
                                and isinstance(col_idx, int)
                                and col_idx < len(df.columns)
                            ):
                                actual_col = df.columns[col_idx]

                            if actual_col in df.columns:
                                if ftype == "range":
                                    f_min = pd.to_numeric(f.get("min"), errors="coerce")
                                    f_max = pd.to_numeric(f.get("max"), errors="coerce")
                                    df[actual_col] = pd.to_numeric(
                                        df[actual_col], errors="coerce"
                                    )
                                    df = df[
                                        (df[actual_col] >= f_min)
                                        & (df[actual_col] <= f_max)
                                    ]
                                elif ftype == "exclude_range":
                                    f_min = pd.to_numeric(f.get("min"), errors="coerce")
                                    f_max = pd.to_numeric(f.get("max"), errors="coerce")
                                    df[actual_col] = pd.to_numeric(
                                        df[actual_col], errors="coerce"
                                    )
                                    df = df[
                                        (df[actual_col] < f_min)
                                        | (df[actual_col] > f_max)
                                    ]
                                elif ftype == "not_empty":
                                    # è™•ç†ç©ºå­—ä¸²èˆ‡ NaN
                                    df = df[
                                        df[actual_col].astype(str).str.strip() != ""
                                    ]
                                    df = df[df[actual_col].notna()]
                                else:  # text search (contains)
                                    val = str(f.get("value", "")).lower()
                                    df = df[
                                        df[actual_col]
                                        .astype(str)
                                        .str.lower()
                                        .str.contains(val, na=False)
                                    ]

                    data_source = f"ä¼ºæœå™¨å…¨é‡éæ¿¾æ•¸æ“š ({len(df)} ç­†)"
                except Exception as e:
                    print(f"ERROR: [QuickAnalysis] Full file load failed: {e}")
                    df = None

            # å¦‚æœç£ç¢Ÿè®€å–å¤±æ•—æˆ–æª”æ¡ˆä¸å­˜åœ¨ï¼Œå›é€€åˆ°ä½¿ç”¨è«‹æ±‚ä¸­å‚³å…¥çš„ rows
            if df is None:
                df = pd.DataFrame(req.rows, columns=req.headers)

            row_count = len(df)
            col_count = len(df.columns)

            # è¨ˆç®—å®Œæ•´åº¦ (åŸºæ–¼ç•¶å‰ df çš„æ‰€æœ‰è¡Œ)
            completeness = df.count() / len(df) if len(df) > 0 else df.count() * 0
            low_completeness_cols = completeness[completeness < 0.9]

            summary_text = (
                f"ğŸ“Š **å…¨é‡æ•¸æ“šç©ºå€¼åˆ†æå ±å‘Š ({filename})**\n"
                f"> [!IMPORTANT]\n"
                f"> æ­¤ä»½å ±å‘Šæ˜¯é‡å°**å¾Œç«¯å…¨é‡æª”æ¡ˆå¯¦é«”**é€²è¡Œçš„ç©ºå€¼èˆ‡å®Œæ•´åº¦è¨ºæ–·ã€‚\n\n"
                f"- **æ•¸æ“šä¾†æº**: {data_source}\n"
                f"- **ç¸½åˆ†æç­†æ•¸**: {row_count} ç­†\n"
                f"- **æ¬„ä½ç¸½æ•¸**: {col_count} å€‹\n"
            )

            if not low_completeness_cols.empty:
                summary_text += "- **ç©ºå€¼è­¦å‘Š (å…¨é‡å®Œæ•´åº¦ < 90%)**:\n"
                for col, comp in low_completeness_cols.items():
                    summary_text += f"  - `{col}`: å®Œæ•´åº¦ {comp * 100:.1f}%\n"
            else:
                summary_text += "- **ç©ºå€¼æª¢æŸ¥**: é€šéã€‚æ‰€æœ‰æ¬„ä½åœ¨å…¨é‡æ•¸æ“šä¸‹å®Œæ•´åº¦å‡ç‚º 100% æˆ– > 90%\n"

            return {
                "status": "success",
                "summary": summary_text,
                "row_count": row_count,
            }
        except Exception as e:
            import traceback

            traceback.print_exc()
            raise HTTPException(500, detail=f"Quick analysis failed: {str(e)}")

    async def get_column_data(
        self, filename: str, column: str, session_id: str = "default"
    ) -> Dict[str, Any]:
        """
        ç²å–ç‰¹å®šæ¬„ä½çš„æ•¸æ“šåˆ†ä½ˆï¼Œç”¨æ–¼åœ–è¡¨é è¦½
        """
        try:
            upload_dir = self.get_user_upload_dir(session_id)
            file_path = os.path.join(upload_dir, filename)

            # Fallback to default
            if not os.path.exists(file_path) and session_id != "default":
                default_path = os.path.join(
                    self.get_user_upload_dir("default"), filename
                )
                if os.path.exists(default_path):
                    file_path = default_path

            if not os.path.exists(file_path):
                raise HTTPException(404, detail=f"File not found: {filename}")

            df = pd.read_csv(file_path)
            if column not in df.columns:
                raise HTTPException(400, detail=f"Column '{column}' not in file")

            # è½‰ç‚ºæ•¸å€¼ï¼Œéæ•¸å€¼è½‰ç‚º 0
            series = pd.to_numeric(df[column], errors="coerce").fillna(0)
            data = series.tolist()

            # æ¡æ¨£è‡³æœ€å¤š 500 é»ä»¥ææ˜‡å‰ç«¯ç¹ªåœ–æ•ˆèƒ½
            if len(data) > 500:
                indices = np.linspace(0, len(data) - 1, 500, dtype=int)
                data = [data[i] for i in indices]

            return {"success": True, "data": data}
        except HTTPException:
            raise
        except Exception as e:
            raise HTTPException(500, detail=f"Get column data failed: {str(e)}")
