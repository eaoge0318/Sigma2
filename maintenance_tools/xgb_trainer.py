# xgb_trainer.py
import pandas as pd
import numpy as np
import xgboost as xgb
import DataPreprocess
import config
import os
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score


def train_xgb_simulator():
    # 1. è¼‰å…¥ä¸¦æ•´ç†è³‡æ–™
    print("æ­£åœ¨å¾è³‡æ–™é›†ä¸­å­¸ç¿’å› æœè¦å¾‹...")
    df, all_cols = DataPreprocess.get_processed_data_and_cols(config.RAW_DATA_PATH)

    bg_features = [
        c for c in all_cols if c not in config.ACTION_FEATURES + [config.MEASURE_COL]
    ]

    # 2. æ§‹å»ºã€Œå› æœã€ç‰¹å¾µçŸ©é™£
    # X: [State_t, Action_delta_t]
    # y: [Measure_t+1]
    X_list, y_list = [], []

    for i in range(len(df) - 1):
        row, row2 = df.iloc[i], df.iloc[i + 1]

        # å–å¾—å‹•ä½œä½ç§»é‡
        delta_a = (
            row2[config.ACTION_FEATURES].values - row[config.ACTION_FEATURES].values
        ).astype(np.float32)

        # ç‰¹å¾µçµ„åˆ: èƒŒæ™¯ + ç•¶å‰åƒæ•¸ + èª¿æ•´é‡
        features = np.concatenate(
            [
                row[bg_features].values.astype(np.float32),
                row[config.ACTION_FEATURES].values.astype(np.float32),
                delta_a,
            ]
        )

        X_list.append(features)
        y_list.append(row2[config.MEASURE_COL])

    X = np.array(X_list)
    y = np.array(y_list)

    # ç‰¹å¾µåç¨± (ç”¨æ–¼å¾ŒçºŒ LLM åˆ†æé‡è¦æ€§)
    feature_names = (
        bg_features
        + config.ACTION_FEATURES
        + [f"delta_{a}" for a in config.ACTION_FEATURES]
    )

    # 3. è¨“ç·´æ¨¡å‹
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42
    )

    print(f"æ­£åœ¨è¨“ç·´ XGBoost æ¨¡æ“¬å™¨... (æ¨£æœ¬æ•¸: {len(X)})")
    model = xgb.XGBRegressor(
        n_estimators=1000,
        max_depth=7,
        learning_rate=0.03,
        objective="reg:squarederror",
        tree_method="hist",
        n_jobs=-1,
    )

    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=200)

    # 4. è©•ä¼°èˆ‡å­˜æª”
    y_pred = model.predict(X_test)
    print(f"\nğŸ“Š æ¨¡æ“¬å™¨æ€§èƒ½å ±å‘Š:")
    print(f"   æº–ç¢ºåº¦ (R2 Score): {r2_score(y_test, y_pred):.4f}")
    print(f"   å¹³å‡èª¤å·® (MAE): {mean_absolute_error(y_test, y_pred):.6e}")

    save_path = os.path.join(config.MODEL_SAVE_DIR, "xgb_simulator.json")
    model.save_model(save_path)
    joblib.dump(feature_names, os.path.join(config.MODEL_SAVE_DIR, "xgb_features.pkl"))
    print(f"ğŸ’¾ æ¨¡æ“¬å™¨å·²å­˜æª”: {save_path}")


if __name__ == "__main__":
    train_xgb_simulator()
